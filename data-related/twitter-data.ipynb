{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Necessary Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.transforms import RandomNodeSplit, RandomLinkSplit\n",
    "from torch_geometric.nn import GCNConv, GATConv,GraphSAGE\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing raw twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Machine Learning\\\\Few-Shot-GNN-LLM\\\\data-related\\\\TwitterDataset.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWITTER_RAW_DATA = os.path.join(os.getcwd(), \"TwitterDataset.csv\")\n",
    "TWITTER_RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text regularization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_regularization(text):\n",
    "    try:\n",
    "        text = str(text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Expand contractions, can't => cannot\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # Remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Normalize accented characters \"café\" → \"cafe\"\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "        # Remove extra white spaces\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('text_regularization', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing raw twitter data to find features and labels</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset_path = TWITTER_RAW_DATA):\n",
    "    try:\n",
    "        twitter_raw_data = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\")\n",
    "        twitter_data = twitter_raw_data[twitter_raw_data[\"gender:confidence\"] >= 0.5]\n",
    "        columns_to_keep = ['gender', 'description']\n",
    "        columns_to_drop = [col for col in twitter_data if col not in columns_to_keep]\n",
    "        twitter_data = twitter_data.drop(columns=columns_to_drop)\n",
    "        twitter_data = twitter_data.dropna()\n",
    "        twitter_data = twitter_data[twitter_data['gender'].isin(['male', 'female'])]\n",
    "        feature = twitter_data['description']\n",
    "        feature = [text_regularization(each) for each in feature]\n",
    "        label = twitter_data['gender'].to_list()\n",
    "        return [feature, label]\n",
    "    except Exception as e:\n",
    "        print('preprocess_data', e)\n",
    "data = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Path for pre processed twitter data</>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_PROCESSED_DATA = os.path.join(os.getcwd(), \"TwitterProcessedDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing pre processed twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[0], columns=['feature'])\n",
    "df['label'] = data[1]\n",
    "df.to_csv(TWITTER_PROCESSED_DATA, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading pre processed twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(TWITTER_PROCESSED_DATA)  \n",
    "twitter_data = twitter_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentence Bert for feature embedding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_embeddings  = sbert_model.encode(twitter_data['feature'].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.0812,  0.0170,  ...,  0.0391,  0.0038, -0.1770],\n",
       "        [ 0.0254, -0.0611,  0.0312,  ..., -0.0036,  0.0272, -0.0790],\n",
       "        [ 0.0242, -0.0003,  0.0522,  ..., -0.0283, -0.0319,  0.0106],\n",
       "        ...,\n",
       "        [-0.0991,  0.0148,  0.0329,  ...,  0.0250,  0.0063, -0.0639],\n",
       "        [-0.0535, -0.0089,  0.0194,  ...,  0.1068, -0.0364,  0.0584],\n",
       "        [-0.0138,  0.0522,  0.0515,  ...,  0.0227, -0.0691, -0.0231]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building edges using cosine similarity to generate graph</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (9, 9),\n",
       " (9, 9),\n",
       " (10, 10),\n",
       " (10, 10),\n",
       " (11, 11),\n",
       " (11, 11),\n",
       " (12, 12),\n",
       " (12, 12),\n",
       " (13, 13),\n",
       " (13, 13),\n",
       " (14, 14),\n",
       " (14, 14),\n",
       " (15, 15),\n",
       " (15, 15),\n",
       " (16, 16),\n",
       " (16, 16),\n",
       " (17, 17),\n",
       " (17, 17),\n",
       " (18, 18),\n",
       " (18, 18),\n",
       " (19, 19),\n",
       " (19, 19),\n",
       " (20, 20),\n",
       " (20, 20),\n",
       " (21, 21),\n",
       " (21, 21),\n",
       " (22, 22),\n",
       " (22, 22),\n",
       " (23, 23),\n",
       " (23, 23),\n",
       " (24, 24),\n",
       " (24, 24),\n",
       " (25, 25),\n",
       " (25, 25),\n",
       " (26, 26),\n",
       " (26, 26),\n",
       " (27, 27),\n",
       " (27, 27),\n",
       " (28, 28),\n",
       " (28, 28),\n",
       " (29, 29),\n",
       " (29, 29),\n",
       " (30, 30),\n",
       " (30, 30),\n",
       " (31, 31),\n",
       " (31, 31),\n",
       " (32, 32),\n",
       " (32, 32),\n",
       " (33, 33),\n",
       " (33, 33),\n",
       " (34, 34),\n",
       " (34, 34),\n",
       " (35, 35),\n",
       " (35, 35),\n",
       " (36, 36),\n",
       " (36, 36),\n",
       " (37, 37),\n",
       " (37, 37),\n",
       " (38, 38),\n",
       " (38, 38),\n",
       " (39, 39),\n",
       " (39, 39),\n",
       " (40, 40),\n",
       " (40, 40),\n",
       " (41, 41),\n",
       " (41, 41),\n",
       " (42, 42),\n",
       " (42, 42),\n",
       " (42, 4540),\n",
       " (4540, 42),\n",
       " (43, 43),\n",
       " (43, 43),\n",
       " (44, 44),\n",
       " (44, 44),\n",
       " (45, 45),\n",
       " (45, 45),\n",
       " (46, 46),\n",
       " (46, 46),\n",
       " (47, 47),\n",
       " (47, 47),\n",
       " (48, 48),\n",
       " (48, 48),\n",
       " (49, 49),\n",
       " (49, 49),\n",
       " (50, 50),\n",
       " (50, 50),\n",
       " (51, 51),\n",
       " (51, 51),\n",
       " (52, 52),\n",
       " (52, 52),\n",
       " (53, 53),\n",
       " (53, 53),\n",
       " (54, 54),\n",
       " (54, 54),\n",
       " (55, 55),\n",
       " (55, 55),\n",
       " (56, 56),\n",
       " (56, 56),\n",
       " (57, 57),\n",
       " (57, 57),\n",
       " (57, 3087),\n",
       " (3087, 57),\n",
       " (58, 58),\n",
       " (58, 58),\n",
       " (59, 59),\n",
       " (59, 59),\n",
       " (59, 10718),\n",
       " (10718, 59),\n",
       " (60, 60),\n",
       " (60, 60),\n",
       " (61, 61),\n",
       " (61, 61),\n",
       " (62, 62),\n",
       " (62, 62),\n",
       " (63, 63),\n",
       " (63, 63),\n",
       " (64, 64),\n",
       " (64, 64),\n",
       " (65, 65),\n",
       " (65, 65),\n",
       " (66, 66),\n",
       " (66, 66),\n",
       " (67, 67),\n",
       " (67, 67),\n",
       " (68, 68),\n",
       " (68, 68),\n",
       " (69, 69),\n",
       " (69, 69),\n",
       " (70, 70),\n",
       " (70, 70),\n",
       " (71, 71),\n",
       " (71, 71),\n",
       " (72, 72),\n",
       " (72, 72),\n",
       " (73, 73),\n",
       " (73, 73),\n",
       " (74, 74),\n",
       " (74, 74),\n",
       " (74, 173),\n",
       " (173, 74),\n",
       " (75, 75),\n",
       " (75, 75),\n",
       " (76, 76),\n",
       " (76, 76),\n",
       " (76, 3494),\n",
       " (3494, 76),\n",
       " (77, 77),\n",
       " (77, 77),\n",
       " (78, 78),\n",
       " (78, 78),\n",
       " (78, 9016),\n",
       " (9016, 78),\n",
       " (79, 79),\n",
       " (79, 79),\n",
       " (80, 80),\n",
       " (80, 80),\n",
       " (81, 81),\n",
       " (81, 81),\n",
       " (82, 82),\n",
       " (82, 82),\n",
       " (83, 83),\n",
       " (83, 83),\n",
       " (84, 84),\n",
       " (84, 84),\n",
       " (85, 85),\n",
       " (85, 85),\n",
       " (85, 1994),\n",
       " (1994, 85),\n",
       " (86, 86),\n",
       " (86, 86),\n",
       " (87, 87),\n",
       " (87, 87),\n",
       " (88, 88),\n",
       " (88, 88),\n",
       " (89, 89),\n",
       " (89, 89),\n",
       " (90, 90),\n",
       " (90, 90),\n",
       " (91, 91),\n",
       " (91, 91),\n",
       " (92, 92),\n",
       " (92, 92),\n",
       " (93, 93),\n",
       " (93, 93),\n",
       " (94, 94),\n",
       " (94, 94),\n",
       " (95, 95),\n",
       " (95, 95),\n",
       " (96, 96),\n",
       " (96, 96),\n",
       " (97, 97),\n",
       " (97, 97),\n",
       " (98, 98),\n",
       " (98, 98),\n",
       " (99, 99),\n",
       " (99, 99),\n",
       " (100, 100),\n",
       " (100, 100),\n",
       " (101, 101),\n",
       " (101, 101),\n",
       " (102, 102),\n",
       " (102, 102),\n",
       " (103, 103),\n",
       " (103, 103),\n",
       " (104, 104),\n",
       " (104, 104),\n",
       " (105, 105),\n",
       " (105, 105),\n",
       " (106, 106),\n",
       " (106, 106),\n",
       " (107, 107),\n",
       " (107, 107),\n",
       " (107, 271),\n",
       " (271, 107),\n",
       " (107, 430),\n",
       " (430, 107),\n",
       " (107, 518),\n",
       " (518, 107),\n",
       " (107, 689),\n",
       " (689, 107),\n",
       " (108, 108),\n",
       " (108, 108),\n",
       " (109, 109),\n",
       " (109, 109),\n",
       " (110, 110),\n",
       " (110, 110),\n",
       " (111, 111),\n",
       " (111, 111),\n",
       " (112, 112),\n",
       " (112, 112),\n",
       " (113, 113),\n",
       " (113, 113),\n",
       " (114, 114),\n",
       " (114, 114),\n",
       " (115, 115),\n",
       " (115, 115),\n",
       " (116, 116),\n",
       " (116, 116),\n",
       " (117, 117),\n",
       " (117, 117),\n",
       " (118, 118),\n",
       " (118, 118),\n",
       " (119, 119),\n",
       " (119, 119),\n",
       " (120, 120),\n",
       " (120, 120),\n",
       " (121, 121),\n",
       " (121, 121),\n",
       " (122, 122),\n",
       " (122, 122),\n",
       " (123, 123),\n",
       " (123, 123),\n",
       " (124, 124),\n",
       " (124, 124),\n",
       " (125, 125),\n",
       " (125, 125),\n",
       " (126, 126),\n",
       " (126, 126),\n",
       " (127, 127),\n",
       " (127, 127),\n",
       " (128, 128),\n",
       " (128, 128),\n",
       " (129, 129),\n",
       " (129, 129),\n",
       " (130, 130),\n",
       " (130, 130),\n",
       " (131, 131),\n",
       " (131, 131),\n",
       " (131, 742),\n",
       " (742, 131),\n",
       " (131, 4277),\n",
       " (4277, 131),\n",
       " (132, 132),\n",
       " (132, 132),\n",
       " (133, 133),\n",
       " (133, 133),\n",
       " (134, 134),\n",
       " (134, 134),\n",
       " (135, 135),\n",
       " (135, 135),\n",
       " (136, 136),\n",
       " (136, 136),\n",
       " (137, 137),\n",
       " (137, 137),\n",
       " (138, 138),\n",
       " (138, 138),\n",
       " (139, 139),\n",
       " (139, 139),\n",
       " (140, 140),\n",
       " (140, 140),\n",
       " (141, 141),\n",
       " (141, 141),\n",
       " (142, 142),\n",
       " (142, 142),\n",
       " (143, 143),\n",
       " (143, 143),\n",
       " (144, 144),\n",
       " (144, 144),\n",
       " (145, 145),\n",
       " (145, 145),\n",
       " (146, 146),\n",
       " (146, 146),\n",
       " (147, 147),\n",
       " (147, 147),\n",
       " (148, 148),\n",
       " (148, 148),\n",
       " (149, 149),\n",
       " (149, 149),\n",
       " (150, 150),\n",
       " (150, 150),\n",
       " (151, 151),\n",
       " (151, 151),\n",
       " (152, 152),\n",
       " (152, 152),\n",
       " (153, 153),\n",
       " (153, 153),\n",
       " (154, 154),\n",
       " (154, 154),\n",
       " (155, 155),\n",
       " (155, 155),\n",
       " (156, 156),\n",
       " (156, 156),\n",
       " (157, 157),\n",
       " (157, 157),\n",
       " (158, 158),\n",
       " (158, 158),\n",
       " (159, 159),\n",
       " (159, 159),\n",
       " (160, 160),\n",
       " (160, 160),\n",
       " (161, 161),\n",
       " (161, 161),\n",
       " (162, 162),\n",
       " (162, 162),\n",
       " (162, 207),\n",
       " (207, 162),\n",
       " (163, 163),\n",
       " (163, 163),\n",
       " (164, 164),\n",
       " (164, 164),\n",
       " (165, 165),\n",
       " (165, 165),\n",
       " (166, 166),\n",
       " (166, 166),\n",
       " (167, 167),\n",
       " (167, 167),\n",
       " (168, 168),\n",
       " (168, 168),\n",
       " (169, 169),\n",
       " (169, 169),\n",
       " (170, 170),\n",
       " (170, 170),\n",
       " (170, 2934),\n",
       " (2934, 170),\n",
       " (170, 7865),\n",
       " (7865, 170),\n",
       " (171, 171),\n",
       " (171, 171),\n",
       " (172, 172),\n",
       " (172, 172),\n",
       " (173, 173),\n",
       " (173, 173),\n",
       " (174, 174),\n",
       " (174, 174),\n",
       " (175, 175),\n",
       " (175, 175),\n",
       " (176, 176),\n",
       " (176, 176),\n",
       " (177, 177),\n",
       " (177, 177),\n",
       " (178, 178),\n",
       " (178, 178),\n",
       " (179, 179),\n",
       " (179, 179),\n",
       " (180, 180),\n",
       " (180, 180),\n",
       " (181, 181),\n",
       " (181, 181),\n",
       " (182, 182),\n",
       " (182, 182),\n",
       " (183, 183),\n",
       " (183, 183),\n",
       " (184, 184),\n",
       " (184, 184),\n",
       " (185, 185),\n",
       " (185, 185),\n",
       " (186, 186),\n",
       " (186, 186),\n",
       " (187, 187),\n",
       " (187, 187),\n",
       " (188, 188),\n",
       " (188, 188),\n",
       " (189, 189),\n",
       " (189, 189),\n",
       " (190, 190),\n",
       " (190, 190),\n",
       " (190, 838),\n",
       " (838, 190),\n",
       " (190, 1423),\n",
       " (1423, 190),\n",
       " (190, 4102),\n",
       " (4102, 190),\n",
       " (191, 191),\n",
       " (191, 191),\n",
       " (192, 192),\n",
       " (192, 192),\n",
       " (193, 193),\n",
       " (193, 193),\n",
       " (194, 194),\n",
       " (194, 194),\n",
       " (195, 195),\n",
       " (195, 195),\n",
       " (196, 196),\n",
       " (196, 196),\n",
       " (197, 197),\n",
       " (197, 197),\n",
       " (198, 198),\n",
       " (198, 198),\n",
       " (199, 199),\n",
       " (199, 199),\n",
       " (200, 200),\n",
       " (200, 200),\n",
       " (201, 201),\n",
       " (201, 201),\n",
       " (202, 202),\n",
       " (202, 202),\n",
       " (203, 203),\n",
       " (203, 203),\n",
       " (204, 204),\n",
       " (204, 204),\n",
       " (205, 205),\n",
       " (205, 205),\n",
       " (206, 206),\n",
       " (206, 206),\n",
       " (207, 207),\n",
       " (207, 207),\n",
       " (208, 208),\n",
       " (208, 208),\n",
       " (209, 209),\n",
       " (209, 209),\n",
       " (210, 210),\n",
       " (210, 210),\n",
       " (211, 211),\n",
       " (211, 211),\n",
       " (211, 368),\n",
       " (368, 211),\n",
       " (212, 212),\n",
       " (212, 212),\n",
       " (213, 213),\n",
       " (213, 213),\n",
       " (214, 214),\n",
       " (214, 214),\n",
       " (215, 215),\n",
       " (215, 215),\n",
       " (216, 216),\n",
       " (216, 216),\n",
       " (217, 217),\n",
       " (217, 217),\n",
       " (218, 218),\n",
       " (218, 218),\n",
       " (218, 3410),\n",
       " (3410, 218),\n",
       " (219, 219),\n",
       " (219, 219),\n",
       " (220, 220),\n",
       " (220, 220),\n",
       " (221, 221),\n",
       " (221, 221),\n",
       " (222, 222),\n",
       " (222, 222),\n",
       " (222, 5418),\n",
       " (5418, 222),\n",
       " (223, 223),\n",
       " (223, 223),\n",
       " (224, 224),\n",
       " (224, 224),\n",
       " (225, 225),\n",
       " (225, 225),\n",
       " (226, 226),\n",
       " (226, 226),\n",
       " (226, 7595),\n",
       " (7595, 226),\n",
       " (227, 227),\n",
       " (227, 227),\n",
       " (228, 228),\n",
       " (228, 228),\n",
       " (229, 229),\n",
       " (229, 229),\n",
       " (230, 230),\n",
       " (230, 230),\n",
       " (231, 231),\n",
       " (231, 231),\n",
       " (232, 232),\n",
       " (232, 232),\n",
       " (233, 233),\n",
       " (233, 233),\n",
       " (234, 234),\n",
       " (234, 234),\n",
       " (235, 235),\n",
       " (235, 235),\n",
       " (236, 236),\n",
       " (236, 236),\n",
       " (237, 237),\n",
       " (237, 237),\n",
       " (238, 238),\n",
       " (238, 238),\n",
       " (239, 239),\n",
       " (239, 239),\n",
       " (240, 240),\n",
       " (240, 240),\n",
       " (241, 241),\n",
       " (241, 241),\n",
       " (242, 242),\n",
       " (242, 242),\n",
       " (242, 5520),\n",
       " (5520, 242),\n",
       " (242, 6916),\n",
       " (6916, 242),\n",
       " (242, 7468),\n",
       " (7468, 242),\n",
       " (242, 8775),\n",
       " (8775, 242),\n",
       " (243, 243),\n",
       " (243, 243),\n",
       " (244, 244),\n",
       " (244, 244),\n",
       " (245, 245),\n",
       " (245, 245),\n",
       " (246, 246),\n",
       " (246, 246),\n",
       " (247, 247),\n",
       " (247, 247),\n",
       " (248, 248),\n",
       " (248, 248),\n",
       " (248, 7589),\n",
       " (7589, 248),\n",
       " (249, 249),\n",
       " (249, 249),\n",
       " (250, 250),\n",
       " (250, 250),\n",
       " (251, 251),\n",
       " (251, 251),\n",
       " (252, 252),\n",
       " (252, 252),\n",
       " (253, 253),\n",
       " (253, 253),\n",
       " (254, 254),\n",
       " (254, 254),\n",
       " (255, 255),\n",
       " (255, 255),\n",
       " (256, 256),\n",
       " (256, 256),\n",
       " (257, 257),\n",
       " (257, 257),\n",
       " (258, 258),\n",
       " (258, 258),\n",
       " (259, 259),\n",
       " (259, 259),\n",
       " (260, 260),\n",
       " (260, 260),\n",
       " (261, 261),\n",
       " (261, 261),\n",
       " (262, 262),\n",
       " (262, 262),\n",
       " (263, 263),\n",
       " (263, 263),\n",
       " (264, 264),\n",
       " (264, 264),\n",
       " (265, 265),\n",
       " (265, 265),\n",
       " (266, 266),\n",
       " (266, 266),\n",
       " (267, 267),\n",
       " (267, 267),\n",
       " (268, 268),\n",
       " (268, 268),\n",
       " (269, 269),\n",
       " (269, 269),\n",
       " (270, 270),\n",
       " (270, 270),\n",
       " (271, 271),\n",
       " (271, 271),\n",
       " (271, 430),\n",
       " (430, 271),\n",
       " (271, 518),\n",
       " (518, 271),\n",
       " (271, 689),\n",
       " (689, 271),\n",
       " (272, 272),\n",
       " (272, 272),\n",
       " (273, 273),\n",
       " (273, 273),\n",
       " (274, 274),\n",
       " (274, 274),\n",
       " (275, 275),\n",
       " (275, 275),\n",
       " (276, 276),\n",
       " (276, 276),\n",
       " (277, 277),\n",
       " (277, 277),\n",
       " (278, 278),\n",
       " (278, 278),\n",
       " (279, 279),\n",
       " (279, 279),\n",
       " (280, 280),\n",
       " (280, 280),\n",
       " (281, 281),\n",
       " (281, 281),\n",
       " (282, 282),\n",
       " (282, 282),\n",
       " (283, 283),\n",
       " (283, 283),\n",
       " (284, 284),\n",
       " (284, 284),\n",
       " (285, 285),\n",
       " (285, 285),\n",
       " (286, 286),\n",
       " (286, 286),\n",
       " (286, 2110),\n",
       " (2110, 286),\n",
       " (286, 2179),\n",
       " (2179, 286),\n",
       " (286, 3847),\n",
       " (3847, 286),\n",
       " (287, 287),\n",
       " (287, 287),\n",
       " (288, 288),\n",
       " (288, 288),\n",
       " (289, 289),\n",
       " (289, 289),\n",
       " (290, 290),\n",
       " (290, 290),\n",
       " (291, 291),\n",
       " (291, 291),\n",
       " (292, 292),\n",
       " (292, 292),\n",
       " (293, 293),\n",
       " (293, 293),\n",
       " (294, 294),\n",
       " (294, 294),\n",
       " (295, 295),\n",
       " (295, 295),\n",
       " (296, 296),\n",
       " (296, 296),\n",
       " (297, 297),\n",
       " (297, 297),\n",
       " (297, 3417),\n",
       " (3417, 297),\n",
       " (298, 298),\n",
       " (298, 298),\n",
       " (299, 299),\n",
       " (299, 299),\n",
       " (299, 7596),\n",
       " (7596, 299),\n",
       " (300, 300),\n",
       " (300, 300),\n",
       " (301, 301),\n",
       " (301, 301),\n",
       " (302, 302),\n",
       " (302, 302),\n",
       " (303, 303),\n",
       " (303, 303),\n",
       " (304, 304),\n",
       " (304, 304),\n",
       " (305, 305),\n",
       " (305, 305),\n",
       " (306, 306),\n",
       " (306, 306),\n",
       " (306, 10427),\n",
       " (10427, 306),\n",
       " (307, 307),\n",
       " (307, 307),\n",
       " (308, 308),\n",
       " (308, 308),\n",
       " (308, 3517),\n",
       " (3517, 308),\n",
       " (308, 4840),\n",
       " (4840, 308),\n",
       " (308, 6307),\n",
       " (6307, 308),\n",
       " (308, 9936),\n",
       " (9936, 308),\n",
       " (309, 309),\n",
       " (309, 309),\n",
       " (310, 310),\n",
       " (310, 310),\n",
       " (311, 311),\n",
       " (311, 311),\n",
       " (312, 312),\n",
       " (312, 312),\n",
       " (313, 313),\n",
       " (313, 313),\n",
       " (314, 314),\n",
       " (314, 314),\n",
       " (315, 315),\n",
       " (315, 315),\n",
       " (316, 316),\n",
       " (316, 316),\n",
       " (317, 317),\n",
       " (317, 317),\n",
       " (317, 322),\n",
       " (322, 317),\n",
       " (318, 318),\n",
       " (318, 318),\n",
       " (319, 319),\n",
       " (319, 319),\n",
       " (319, 331),\n",
       " (331, 319),\n",
       " (320, 320),\n",
       " (320, 320),\n",
       " (321, 321),\n",
       " (321, 321),\n",
       " (322, 322),\n",
       " (322, 322),\n",
       " (323, 323),\n",
       " (323, 323),\n",
       " (324, 324),\n",
       " (324, 324),\n",
       " (325, 325),\n",
       " (325, 325),\n",
       " (326, 326),\n",
       " (326, 326),\n",
       " (327, 327),\n",
       " (327, 327),\n",
       " (328, 328),\n",
       " (328, 328),\n",
       " (329, 329),\n",
       " (329, 329),\n",
       " (329, 2094),\n",
       " (2094, 329),\n",
       " (330, 330),\n",
       " (330, 330),\n",
       " (331, 331),\n",
       " (331, 331),\n",
       " (332, 332),\n",
       " (332, 332),\n",
       " (333, 333),\n",
       " (333, 333),\n",
       " (334, 334),\n",
       " (334, 334),\n",
       " (334, 10620),\n",
       " (10620, 334),\n",
       " (335, 335),\n",
       " (335, 335),\n",
       " (336, 336),\n",
       " (336, 336),\n",
       " (337, 337),\n",
       " (337, 337),\n",
       " (338, 338),\n",
       " (338, 338),\n",
       " (339, 339),\n",
       " (339, 339),\n",
       " (340, 340),\n",
       " (340, 340),\n",
       " (341, 341),\n",
       " (341, 341),\n",
       " (342, 342),\n",
       " (342, 342),\n",
       " (343, 343),\n",
       " (343, 343),\n",
       " (344, 344),\n",
       " (344, 344),\n",
       " (345, 345),\n",
       " (345, 345),\n",
       " (346, 346),\n",
       " (346, 346),\n",
       " (347, 347),\n",
       " (347, 347),\n",
       " (348, 348),\n",
       " (348, 348),\n",
       " (349, 349),\n",
       " (349, 349),\n",
       " (350, 350),\n",
       " (350, 350),\n",
       " (350, 1020),\n",
       " (1020, 350),\n",
       " (350, 9045),\n",
       " (9045, 350),\n",
       " (351, 351),\n",
       " (351, 351),\n",
       " (352, 352),\n",
       " (352, 352),\n",
       " (353, 353),\n",
       " (353, 353),\n",
       " (354, 354),\n",
       " (354, 354),\n",
       " (355, 355),\n",
       " (355, 355),\n",
       " (356, 356),\n",
       " (356, 356),\n",
       " (357, 357),\n",
       " (357, 357),\n",
       " (358, 358),\n",
       " (358, 358),\n",
       " (359, 359),\n",
       " (359, 359),\n",
       " (360, 360),\n",
       " (360, 360),\n",
       " (361, 361),\n",
       " (361, 361),\n",
       " (362, 362),\n",
       " (362, 362),\n",
       " (362, 7416),\n",
       " (7416, 362),\n",
       " (363, 363),\n",
       " (363, 363),\n",
       " (363, 3236),\n",
       " (3236, 363),\n",
       " (364, 364),\n",
       " (364, 364),\n",
       " (365, 365),\n",
       " (365, 365),\n",
       " (366, 366),\n",
       " (366, 366),\n",
       " (367, 367),\n",
       " (367, 367),\n",
       " (368, 368),\n",
       " (368, 368),\n",
       " (369, 369),\n",
       " (369, 369),\n",
       " (370, 370),\n",
       " (370, 370),\n",
       " (371, 371),\n",
       " (371, 371),\n",
       " (372, 372),\n",
       " (372, 372),\n",
       " (373, 373),\n",
       " (373, 373),\n",
       " (374, 374),\n",
       " (374, 374),\n",
       " (375, 375),\n",
       " (375, 375),\n",
       " (376, 376),\n",
       " (376, 376),\n",
       " (377, 377),\n",
       " (377, 377),\n",
       " (378, 378),\n",
       " (378, 378),\n",
       " (379, 379),\n",
       " (379, 379),\n",
       " (380, 380),\n",
       " (380, 380),\n",
       " (381, 381),\n",
       " (381, 381),\n",
       " (382, 382),\n",
       " (382, 382),\n",
       " (383, 383),\n",
       " (383, 383),\n",
       " (384, 384),\n",
       " (384, 384),\n",
       " (384, 966),\n",
       " (966, 384),\n",
       " (384, 1723),\n",
       " (1723, 384),\n",
       " (385, 385),\n",
       " (385, 385),\n",
       " (386, 386),\n",
       " (386, 386),\n",
       " (387, 387),\n",
       " (387, 387),\n",
       " (388, 388),\n",
       " (388, 388),\n",
       " (389, 389),\n",
       " (389, 389),\n",
       " (390, 390),\n",
       " (390, 390),\n",
       " (391, 391),\n",
       " (391, 391),\n",
       " (392, 392),\n",
       " (392, 392),\n",
       " (393, 393),\n",
       " (393, 393),\n",
       " (393, 4180),\n",
       " (4180, 393),\n",
       " (394, 394),\n",
       " (394, 394),\n",
       " (395, 395),\n",
       " (395, 395),\n",
       " (396, 396),\n",
       " (396, 396),\n",
       " (397, 397),\n",
       " (397, 397),\n",
       " (398, 398),\n",
       " (398, 398),\n",
       " (399, 399),\n",
       " (399, 399),\n",
       " (400, 400),\n",
       " (400, 400),\n",
       " (401, 401),\n",
       " (401, 401),\n",
       " (402, 402),\n",
       " (402, 402),\n",
       " (403, 403),\n",
       " (403, 403),\n",
       " (404, 404),\n",
       " (404, 404),\n",
       " (405, 405),\n",
       " (405, 405),\n",
       " (406, 406),\n",
       " (406, 406),\n",
       " (407, 407),\n",
       " (407, 407),\n",
       " (408, 408),\n",
       " (408, 408),\n",
       " (409, 409),\n",
       " (409, 409),\n",
       " (410, 410),\n",
       " (410, 410),\n",
       " (411, 411),\n",
       " (411, 411),\n",
       " (412, 412),\n",
       " (412, 412),\n",
       " (413, 413),\n",
       " (413, 413),\n",
       " (414, 414),\n",
       " (414, 414),\n",
       " (415, 415),\n",
       " (415, 415),\n",
       " (416, 416),\n",
       " (416, 416),\n",
       " (417, 417),\n",
       " (417, 417),\n",
       " (418, 418),\n",
       " (418, 418),\n",
       " (419, 419),\n",
       " (419, 419),\n",
       " (419, 1758),\n",
       " (1758, 419),\n",
       " (419, 9457),\n",
       " (9457, 419),\n",
       " (420, 420),\n",
       " (420, 420),\n",
       " (421, 421),\n",
       " (421, 421),\n",
       " (422, 422),\n",
       " (422, 422),\n",
       " (423, 423),\n",
       " (423, 423),\n",
       " (423, 4308),\n",
       " (4308, 423),\n",
       " (424, 424),\n",
       " (424, 424),\n",
       " (425, 425),\n",
       " (425, 425),\n",
       " (426, 426),\n",
       " (426, 426),\n",
       " (427, 427),\n",
       " (427, 427),\n",
       " (428, 428),\n",
       " (428, 428),\n",
       " (429, 429),\n",
       " (429, 429),\n",
       " (430, 430),\n",
       " (430, 430),\n",
       " (430, 518),\n",
       " (518, 430),\n",
       " (430, 689),\n",
       " (689, 430),\n",
       " (431, 431),\n",
       " (431, 431),\n",
       " (432, 432),\n",
       " (432, 432),\n",
       " (433, 433),\n",
       " (433, 433),\n",
       " (434, 434),\n",
       " (434, 434),\n",
       " (435, 435),\n",
       " (435, 435),\n",
       " (436, 436),\n",
       " (436, 436),\n",
       " (437, 437),\n",
       " (437, 437),\n",
       " (438, 438),\n",
       " (438, 438),\n",
       " (439, 439),\n",
       " (439, 439),\n",
       " (440, 440),\n",
       " (440, 440),\n",
       " (441, 441),\n",
       " (441, 441),\n",
       " (442, 442),\n",
       " (442, 442),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_edges(features, threshold=0.5):\n",
    "    similarity_matrix = cosine_similarity(features)\n",
    "    edges = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i, len(features)):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                edges.append((i, j))\n",
    "                edges.append((j, i))\n",
    "    return edges\n",
    "\n",
    "edges = build_edges(feature_embeddings.to(\"cpu\"), threshold=0.8)\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting labels to numeric value</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(twitter_data['label'].tolist())\n",
    "label_tensor = torch.tensor(label_encoded, dtype=torch.long).unsqueeze(1)\n",
    "label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.0812,  0.0170,  ...,  0.0391,  0.0038, -0.1770],\n",
       "        [ 0.0254, -0.0611,  0.0312,  ..., -0.0036,  0.0272, -0.0790],\n",
       "        [ 0.0242, -0.0003,  0.0522,  ..., -0.0283, -0.0319,  0.0106],\n",
       "        ...,\n",
       "        [-0.0991,  0.0148,  0.0329,  ...,  0.0250,  0.0063, -0.0639],\n",
       "        [-0.0535, -0.0089,  0.0194,  ...,  0.1068, -0.0364,  0.0584],\n",
       "        [-0.0138,  0.0522,  0.0515,  ...,  0.0227, -0.0691, -0.0231]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = feature_embeddings\n",
    "node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting edges to edge index to make graph data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0],\n",
       "        [    0,     0],\n",
       "        [    1,     1],\n",
       "        ...,\n",
       "        [10748, 10748],\n",
       "        [10749, 10749],\n",
       "        [10749, 10749]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label = label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = edge_index.t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Making graph data to train and evaluate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[10750, 384], edge_index=[2, 23808], y=[10750, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data = Data(x=node_features, edge_index=edge_index, y=node_label)\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting data in to train, validation and test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "splitter = RandomNodeSplit(split=\"train_rest\", num_val=0.2, num_test=0.2)\n",
    "graph_data = splitter(graph_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access masks for each set\n",
    "train_mask = graph_data.train_mask\n",
    "val_mask = graph_data.val_mask\n",
    "test_mask = graph_data.test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     1,  ..., 10748, 10749, 10749],\n",
       "        [    0,     0,     1,  ..., 10748, 10749, 10749]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6450]) torch.Size([2150]) torch.Size([2150])\n"
     ]
    }
   ],
   "source": [
    "train_nodes = train_mask.nonzero().flatten()\n",
    "val_nodes = val_mask.nonzero().flatten()\n",
    "test_nodes = test_mask.nonzero().flatten()\n",
    "print(train_nodes.shape, val_nodes.shape, test_nodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting edges to train, validation and test accourding to train, validation and test nodes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_splitter = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
    "train_data, val_data, test_data = edge_splitter(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[10750, 384], edge_index=[2, 14286], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[28572], edge_label_index=[2, 28572])\n",
      "Data(x=[10750, 384], edge_index=[2, 14286], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[9522], edge_label_index=[2, 9522])\n",
      "Data(x=[10750, 384], edge_index=[2, 19047], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[9522], edge_label_index=[2, 9522])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for GCNConv model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = graph_data.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>GCNConv model to evaluate twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Graph Neural Network model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17540\\3335159088.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGCN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "model = GCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary classification loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training function, calculating loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data.x, train_data.edge_index)\n",
    "    loss = loss_fn(out[train_data.train_mask], train_data.y[train_data.train_mask].squeeze().float())\n",
    "    # loss = F.nll_loss(out[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluating model and calculating Accuracy, precision, recall and f1 score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Convert probabilities to binary predictions (0 or 1)\n",
    "        preds = (probs > 0.5).cpu().numpy()\n",
    "        \n",
    "        preds = preds[data.test_mask.cpu().numpy()]\n",
    "\n",
    "        y_true = data.y[data.test_mask].squeeze().cpu().numpy()        \n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        precision = precision_score(y_true, preds, average=\"binary\")\n",
    "        recall = recall_score(y_true, preds, average=\"binary\")\n",
    "        f1 = f1_score(y_true, preds, average=\"binary\")\n",
    "        return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training for multiple epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6954, Test Accuracy: 0.5758\n",
      "Epoch 20, Loss: 0.5910, Test Accuracy: 0.6637\n",
      "Epoch 40, Loss: 0.5683, Test Accuracy: 0.6693\n",
      "Epoch 60, Loss: 0.5610, Test Accuracy: 0.6716\n",
      "Early stopping at epoch 71. Best validation accuracy: 0.6726\n",
      "Final Test Accuracy: 0.6693\n",
      "Final Test Precision: 0.6757\n",
      "Final Test Recall: 0.6170\n",
      "Final Test F1: 0.6450\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_val_acc = 0\n",
    "patience = 10  # Stop if validation accuracy does not improve for 10 epochs\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, _, _, _ = evaluate(train_data)\n",
    "    val_acc, _, _, _ = evaluate(val_data)\n",
    "    test_acc, _, _, _ = evaluate(test_data)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate(test_data)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate(test_data)\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class for GAT model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAT Model for Binary Classification\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        # First GAT layer (multi-head attention)\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        # Second GAT layer (single-head for binary output)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, 1, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)  # Use ELU activation function\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.view(-1)  # Output raw logits (no sigmoid here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for GAT model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_data.num_node_features\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GAT model\n",
    "model = GAT(in_channels=input_dim, hidden_channels=hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training for multiple epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6954, Test Accuracy: 0.5767\n",
      "Epoch 20, Loss: 0.6508, Test Accuracy: 0.6693\n",
      "Early stopping at epoch 37. Best validation accuracy: 0.6749\n",
      "Final Test Accuracy: 0.6651\n",
      "Final Test Precision: 0.7051\n",
      "Final Test Recall: 0.5368\n",
      "Final Test F1: 0.6095\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_val_acc = 0\n",
    "patience = 10  # Stop if validation accuracy does not improve for 10 epochs\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, _, _, _ = evaluate(train_data)\n",
    "    val_acc, _, _, _ = evaluate(val_data)\n",
    "    test_acc, _, _, _ = evaluate(test_data)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate(test_data)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate(test_data)\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6450\n",
      "Validation data size: 2150\n",
      "Test data size: 2150\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and temp datasets with a ratio of 6:4\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    twitter_data['feature'], label_encoded, test_size=0.4, random_state=seed, stratify=label_encoded\n",
    ")\n",
    "\n",
    "# Split the temp dataset into validation and test datasets with a ratio of 2:2\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data, temp_labels, test_size=0.5, random_state=seed, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Validation data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length = 128,\n",
    "        return_tensors='pt'  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer_function(train_data.tolist())\n",
    "val_encodings = tokenizer_function(val_data.tolist())\n",
    "test_encodings = tokenizer_function(test_data.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GenderDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = GenderDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = GenderDataset(test_encodings, test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)  # Binary classification\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits Loss\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.float().unsqueeze(1)  # Ensure labels are of shape (batch_size, 1)\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "model = CustomBertModel(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = (torch.sigmoid(torch.tensor(logits)) > 0.5).numpy().astype(int)  # Convert logits to binary predictions\n",
    "    labels = labels.astype(int)\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertResults\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2020' max='2020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2020/2020 13:23, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.586744</td>\n",
       "      <td>0.685116</td>\n",
       "      <td>0.725395</td>\n",
       "      <td>0.569656</td>\n",
       "      <td>0.638161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.648965</td>\n",
       "      <td>0.671628</td>\n",
       "      <td>0.636800</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.692776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.316300</td>\n",
       "      <td>0.770591</td>\n",
       "      <td>0.699070</td>\n",
       "      <td>0.716289</td>\n",
       "      <td>0.633588</td>\n",
       "      <td>0.672405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>1.039450</td>\n",
       "      <td>0.694419</td>\n",
       "      <td>0.679522</td>\n",
       "      <td>0.706107</td>\n",
       "      <td>0.692560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>1.242801</td>\n",
       "      <td>0.695814</td>\n",
       "      <td>0.694280</td>\n",
       "      <td>0.671756</td>\n",
       "      <td>0.682832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2020, training_loss=0.357210906543354, metrics={'train_runtime': 803.9622, 'train_samples_per_second': 40.114, 'train_steps_per_second': 2.513, 'total_flos': 0.0, 'train_loss': 0.357210906543354, 'epoch': 5.0})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.7702117562294006, 'eval_accuracy': 0.6911627906976744, 'eval_precision': 0.7042553191489361, 'eval_recall': 0.6316793893129771, 'eval_f1': 0.6659959758551308, 'eval_runtime': 17.9661, 'eval_samples_per_second': 119.67, 'eval_steps_per_second': 7.514, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f'Test Results: {test_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)['logits']\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return {\"Male\": 1 - probs.item(), \"Female\": probs.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5931]], device='cuda:0')\n",
      "Predicted Gender Probabilities: {'Male': 0.4068903923034668, 'Female': 0.5931096076965332}\n"
     ]
    }
   ],
   "source": [
    "example_text = \"I love watching football and playing video games.\"\n",
    "prediction = predict(example_text)\n",
    "print(f\"Predicted Gender Probabilities: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.0",
   "language": "python",
   "name": "v3.11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
