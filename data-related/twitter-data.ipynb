{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Necessary Packages</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch_geometric.transforms import RandomNodeSplit, RandomLinkSplit\n",
    "from torch_geometric.nn import GCNConv, GATConv,GraphSAGE\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing raw twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Projects\\\\Machine Learning\\\\Few-Shot-GNN-LLM\\\\data-related\\\\TwitterDataset.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWITTER_RAW_DATA = os.path.join(os.getcwd(), \"TwitterDataset.csv\")\n",
    "TWITTER_RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text regularization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_regularization(text):\n",
    "    try:\n",
    "        text = str(text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Expand contractions, can't => cannot\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # Remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Normalize accented characters \"café\" → \"cafe\"\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "        # Remove extra white spaces\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('text_regularization', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocessing raw twitter data to find features and labels</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset_path = TWITTER_RAW_DATA):\n",
    "    try:\n",
    "        twitter_raw_data = pd.read_csv(dataset_path, encoding=\"ISO-8859-1\")\n",
    "        twitter_data = twitter_raw_data[twitter_raw_data[\"gender:confidence\"] >= 0.5]\n",
    "        columns_to_keep = ['gender', 'description']\n",
    "        columns_to_drop = [col for col in twitter_data if col not in columns_to_keep]\n",
    "        twitter_data = twitter_data.drop(columns=columns_to_drop)\n",
    "        twitter_data = twitter_data.dropna()\n",
    "        twitter_data = twitter_data[twitter_data['gender'].isin(['male', 'female'])]\n",
    "        feature = twitter_data['description']\n",
    "        feature = [text_regularization(each) for each in feature]\n",
    "        label = twitter_data['gender'].to_list()\n",
    "        return [feature, label]\n",
    "    except Exception as e:\n",
    "        print('preprocess_data', e)\n",
    "data = preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Path for pre processed twitter data</>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_PROCESSED_DATA = os.path.join(os.getcwd(), \"TwitterProcessedDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Storing pre processed twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data[0], columns=['feature'])\n",
    "df['label'] = data[1]\n",
    "df.to_csv(TWITTER_PROCESSED_DATA, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reading pre processed twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv(TWITTER_PROCESSED_DATA)  \n",
    "twitter_data = twitter_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sentence Bert for feature embedding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\susan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_embeddings  = sbert_model.encode(twitter_data['feature'].tolist(), convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.0812,  0.0170,  ...,  0.0391,  0.0038, -0.1770],\n",
       "        [ 0.0254, -0.0611,  0.0312,  ..., -0.0036,  0.0272, -0.0790],\n",
       "        [ 0.0242, -0.0003,  0.0522,  ..., -0.0283, -0.0319,  0.0106],\n",
       "        ...,\n",
       "        [-0.0991,  0.0148,  0.0329,  ...,  0.0250,  0.0063, -0.0639],\n",
       "        [-0.0535, -0.0089,  0.0194,  ...,  0.1068, -0.0364,  0.0584],\n",
       "        [-0.0138,  0.0522,  0.0515,  ...,  0.0227, -0.0691, -0.0231]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building edges using cosine similarity to generate graph</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (9, 9),\n",
       " (9, 9),\n",
       " (10, 10),\n",
       " (10, 10),\n",
       " (11, 11),\n",
       " (11, 11),\n",
       " (12, 12),\n",
       " (12, 12),\n",
       " (13, 13),\n",
       " (13, 13),\n",
       " (14, 14),\n",
       " (14, 14),\n",
       " (15, 15),\n",
       " (15, 15),\n",
       " (16, 16),\n",
       " (16, 16),\n",
       " (17, 17),\n",
       " (17, 17),\n",
       " (18, 18),\n",
       " (18, 18),\n",
       " (19, 19),\n",
       " (19, 19),\n",
       " (20, 20),\n",
       " (20, 20),\n",
       " (21, 21),\n",
       " (21, 21),\n",
       " (22, 22),\n",
       " (22, 22),\n",
       " (23, 23),\n",
       " (23, 23),\n",
       " (24, 24),\n",
       " (24, 24),\n",
       " (25, 25),\n",
       " (25, 25),\n",
       " (26, 26),\n",
       " (26, 26),\n",
       " (27, 27),\n",
       " (27, 27),\n",
       " (28, 28),\n",
       " (28, 28),\n",
       " (29, 29),\n",
       " (29, 29),\n",
       " (30, 30),\n",
       " (30, 30),\n",
       " (31, 31),\n",
       " (31, 31),\n",
       " (32, 32),\n",
       " (32, 32),\n",
       " (33, 33),\n",
       " (33, 33),\n",
       " (34, 34),\n",
       " (34, 34),\n",
       " (35, 35),\n",
       " (35, 35),\n",
       " (36, 36),\n",
       " (36, 36),\n",
       " (37, 37),\n",
       " (37, 37),\n",
       " (38, 38),\n",
       " (38, 38),\n",
       " (39, 39),\n",
       " (39, 39),\n",
       " (40, 40),\n",
       " (40, 40),\n",
       " (41, 41),\n",
       " (41, 41),\n",
       " (42, 42),\n",
       " (42, 42),\n",
       " (42, 4540),\n",
       " (4540, 42),\n",
       " (43, 43),\n",
       " (43, 43),\n",
       " (44, 44),\n",
       " (44, 44),\n",
       " (45, 45),\n",
       " (45, 45),\n",
       " (46, 46),\n",
       " (46, 46),\n",
       " (47, 47),\n",
       " (47, 47),\n",
       " (48, 48),\n",
       " (48, 48),\n",
       " (49, 49),\n",
       " (49, 49),\n",
       " (50, 50),\n",
       " (50, 50),\n",
       " (51, 51),\n",
       " (51, 51),\n",
       " (52, 52),\n",
       " (52, 52),\n",
       " (53, 53),\n",
       " (53, 53),\n",
       " (54, 54),\n",
       " (54, 54),\n",
       " (55, 55),\n",
       " (55, 55),\n",
       " (56, 56),\n",
       " (56, 56),\n",
       " (57, 57),\n",
       " (57, 57),\n",
       " (57, 3087),\n",
       " (3087, 57),\n",
       " (58, 58),\n",
       " (58, 58),\n",
       " (59, 59),\n",
       " (59, 59),\n",
       " (59, 10718),\n",
       " (10718, 59),\n",
       " (60, 60),\n",
       " (60, 60),\n",
       " (61, 61),\n",
       " (61, 61),\n",
       " (62, 62),\n",
       " (62, 62),\n",
       " (63, 63),\n",
       " (63, 63),\n",
       " (64, 64),\n",
       " (64, 64),\n",
       " (65, 65),\n",
       " (65, 65),\n",
       " (66, 66),\n",
       " (66, 66),\n",
       " (67, 67),\n",
       " (67, 67),\n",
       " (68, 68),\n",
       " (68, 68),\n",
       " (69, 69),\n",
       " (69, 69),\n",
       " (70, 70),\n",
       " (70, 70),\n",
       " (71, 71),\n",
       " (71, 71),\n",
       " (72, 72),\n",
       " (72, 72),\n",
       " (73, 73),\n",
       " (73, 73),\n",
       " (74, 74),\n",
       " (74, 74),\n",
       " (74, 173),\n",
       " (173, 74),\n",
       " (75, 75),\n",
       " (75, 75),\n",
       " (76, 76),\n",
       " (76, 76),\n",
       " (76, 3494),\n",
       " (3494, 76),\n",
       " (77, 77),\n",
       " (77, 77),\n",
       " (78, 78),\n",
       " (78, 78),\n",
       " (78, 9016),\n",
       " (9016, 78),\n",
       " (79, 79),\n",
       " (79, 79),\n",
       " (80, 80),\n",
       " (80, 80),\n",
       " (81, 81),\n",
       " (81, 81),\n",
       " (82, 82),\n",
       " (82, 82),\n",
       " (83, 83),\n",
       " (83, 83),\n",
       " (84, 84),\n",
       " (84, 84),\n",
       " (85, 85),\n",
       " (85, 85),\n",
       " (85, 1994),\n",
       " (1994, 85),\n",
       " (86, 86),\n",
       " (86, 86),\n",
       " (87, 87),\n",
       " (87, 87),\n",
       " (88, 88),\n",
       " (88, 88),\n",
       " (89, 89),\n",
       " (89, 89),\n",
       " (90, 90),\n",
       " (90, 90),\n",
       " (91, 91),\n",
       " (91, 91),\n",
       " (92, 92),\n",
       " (92, 92),\n",
       " (93, 93),\n",
       " (93, 93),\n",
       " (94, 94),\n",
       " (94, 94),\n",
       " (95, 95),\n",
       " (95, 95),\n",
       " (96, 96),\n",
       " (96, 96),\n",
       " (97, 97),\n",
       " (97, 97),\n",
       " (98, 98),\n",
       " (98, 98),\n",
       " (99, 99),\n",
       " (99, 99),\n",
       " (100, 100),\n",
       " (100, 100),\n",
       " (101, 101),\n",
       " (101, 101),\n",
       " (102, 102),\n",
       " (102, 102),\n",
       " (103, 103),\n",
       " (103, 103),\n",
       " (104, 104),\n",
       " (104, 104),\n",
       " (105, 105),\n",
       " (105, 105),\n",
       " (106, 106),\n",
       " (106, 106),\n",
       " (107, 107),\n",
       " (107, 107),\n",
       " (107, 271),\n",
       " (271, 107),\n",
       " (107, 430),\n",
       " (430, 107),\n",
       " (107, 518),\n",
       " (518, 107),\n",
       " (107, 689),\n",
       " (689, 107),\n",
       " (108, 108),\n",
       " (108, 108),\n",
       " (109, 109),\n",
       " (109, 109),\n",
       " (110, 110),\n",
       " (110, 110),\n",
       " (111, 111),\n",
       " (111, 111),\n",
       " (112, 112),\n",
       " (112, 112),\n",
       " (113, 113),\n",
       " (113, 113),\n",
       " (114, 114),\n",
       " (114, 114),\n",
       " (115, 115),\n",
       " (115, 115),\n",
       " (116, 116),\n",
       " (116, 116),\n",
       " (117, 117),\n",
       " (117, 117),\n",
       " (118, 118),\n",
       " (118, 118),\n",
       " (119, 119),\n",
       " (119, 119),\n",
       " (120, 120),\n",
       " (120, 120),\n",
       " (121, 121),\n",
       " (121, 121),\n",
       " (122, 122),\n",
       " (122, 122),\n",
       " (123, 123),\n",
       " (123, 123),\n",
       " (124, 124),\n",
       " (124, 124),\n",
       " (125, 125),\n",
       " (125, 125),\n",
       " (126, 126),\n",
       " (126, 126),\n",
       " (127, 127),\n",
       " (127, 127),\n",
       " (128, 128),\n",
       " (128, 128),\n",
       " (129, 129),\n",
       " (129, 129),\n",
       " (130, 130),\n",
       " (130, 130),\n",
       " (131, 131),\n",
       " (131, 131),\n",
       " (131, 742),\n",
       " (742, 131),\n",
       " (131, 4277),\n",
       " (4277, 131),\n",
       " (132, 132),\n",
       " (132, 132),\n",
       " (133, 133),\n",
       " (133, 133),\n",
       " (134, 134),\n",
       " (134, 134),\n",
       " (135, 135),\n",
       " (135, 135),\n",
       " (136, 136),\n",
       " (136, 136),\n",
       " (137, 137),\n",
       " (137, 137),\n",
       " (138, 138),\n",
       " (138, 138),\n",
       " (139, 139),\n",
       " (139, 139),\n",
       " (140, 140),\n",
       " (140, 140),\n",
       " (141, 141),\n",
       " (141, 141),\n",
       " (142, 142),\n",
       " (142, 142),\n",
       " (143, 143),\n",
       " (143, 143),\n",
       " (144, 144),\n",
       " (144, 144),\n",
       " (145, 145),\n",
       " (145, 145),\n",
       " (146, 146),\n",
       " (146, 146),\n",
       " (147, 147),\n",
       " (147, 147),\n",
       " (148, 148),\n",
       " (148, 148),\n",
       " (149, 149),\n",
       " (149, 149),\n",
       " (150, 150),\n",
       " (150, 150),\n",
       " (151, 151),\n",
       " (151, 151),\n",
       " (152, 152),\n",
       " (152, 152),\n",
       " (153, 153),\n",
       " (153, 153),\n",
       " (154, 154),\n",
       " (154, 154),\n",
       " (155, 155),\n",
       " (155, 155),\n",
       " (156, 156),\n",
       " (156, 156),\n",
       " (157, 157),\n",
       " (157, 157),\n",
       " (158, 158),\n",
       " (158, 158),\n",
       " (159, 159),\n",
       " (159, 159),\n",
       " (160, 160),\n",
       " (160, 160),\n",
       " (161, 161),\n",
       " (161, 161),\n",
       " (162, 162),\n",
       " (162, 162),\n",
       " (162, 207),\n",
       " (207, 162),\n",
       " (163, 163),\n",
       " (163, 163),\n",
       " (164, 164),\n",
       " (164, 164),\n",
       " (165, 165),\n",
       " (165, 165),\n",
       " (166, 166),\n",
       " (166, 166),\n",
       " (167, 167),\n",
       " (167, 167),\n",
       " (168, 168),\n",
       " (168, 168),\n",
       " (169, 169),\n",
       " (169, 169),\n",
       " (170, 170),\n",
       " (170, 170),\n",
       " (170, 2934),\n",
       " (2934, 170),\n",
       " (170, 7865),\n",
       " (7865, 170),\n",
       " (171, 171),\n",
       " (171, 171),\n",
       " (172, 172),\n",
       " (172, 172),\n",
       " (173, 173),\n",
       " (173, 173),\n",
       " (174, 174),\n",
       " (174, 174),\n",
       " (175, 175),\n",
       " (175, 175),\n",
       " (176, 176),\n",
       " (176, 176),\n",
       " (177, 177),\n",
       " (177, 177),\n",
       " (178, 178),\n",
       " (178, 178),\n",
       " (179, 179),\n",
       " (179, 179),\n",
       " (180, 180),\n",
       " (180, 180),\n",
       " (181, 181),\n",
       " (181, 181),\n",
       " (182, 182),\n",
       " (182, 182),\n",
       " (183, 183),\n",
       " (183, 183),\n",
       " (184, 184),\n",
       " (184, 184),\n",
       " (185, 185),\n",
       " (185, 185),\n",
       " (186, 186),\n",
       " (186, 186),\n",
       " (187, 187),\n",
       " (187, 187),\n",
       " (188, 188),\n",
       " (188, 188),\n",
       " (189, 189),\n",
       " (189, 189),\n",
       " (190, 190),\n",
       " (190, 190),\n",
       " (190, 838),\n",
       " (838, 190),\n",
       " (190, 1423),\n",
       " (1423, 190),\n",
       " (190, 4102),\n",
       " (4102, 190),\n",
       " (191, 191),\n",
       " (191, 191),\n",
       " (192, 192),\n",
       " (192, 192),\n",
       " (193, 193),\n",
       " (193, 193),\n",
       " (194, 194),\n",
       " (194, 194),\n",
       " (195, 195),\n",
       " (195, 195),\n",
       " (196, 196),\n",
       " (196, 196),\n",
       " (197, 197),\n",
       " (197, 197),\n",
       " (198, 198),\n",
       " (198, 198),\n",
       " (199, 199),\n",
       " (199, 199),\n",
       " (200, 200),\n",
       " (200, 200),\n",
       " (201, 201),\n",
       " (201, 201),\n",
       " (202, 202),\n",
       " (202, 202),\n",
       " (203, 203),\n",
       " (203, 203),\n",
       " (204, 204),\n",
       " (204, 204),\n",
       " (205, 205),\n",
       " (205, 205),\n",
       " (206, 206),\n",
       " (206, 206),\n",
       " (207, 207),\n",
       " (207, 207),\n",
       " (208, 208),\n",
       " (208, 208),\n",
       " (209, 209),\n",
       " (209, 209),\n",
       " (210, 210),\n",
       " (210, 210),\n",
       " (211, 211),\n",
       " (211, 211),\n",
       " (211, 368),\n",
       " (368, 211),\n",
       " (212, 212),\n",
       " (212, 212),\n",
       " (213, 213),\n",
       " (213, 213),\n",
       " (214, 214),\n",
       " (214, 214),\n",
       " (215, 215),\n",
       " (215, 215),\n",
       " (216, 216),\n",
       " (216, 216),\n",
       " (217, 217),\n",
       " (217, 217),\n",
       " (218, 218),\n",
       " (218, 218),\n",
       " (218, 3410),\n",
       " (3410, 218),\n",
       " (219, 219),\n",
       " (219, 219),\n",
       " (220, 220),\n",
       " (220, 220),\n",
       " (221, 221),\n",
       " (221, 221),\n",
       " (222, 222),\n",
       " (222, 222),\n",
       " (222, 5418),\n",
       " (5418, 222),\n",
       " (223, 223),\n",
       " (223, 223),\n",
       " (224, 224),\n",
       " (224, 224),\n",
       " (225, 225),\n",
       " (225, 225),\n",
       " (226, 226),\n",
       " (226, 226),\n",
       " (226, 7595),\n",
       " (7595, 226),\n",
       " (227, 227),\n",
       " (227, 227),\n",
       " (228, 228),\n",
       " (228, 228),\n",
       " (229, 229),\n",
       " (229, 229),\n",
       " (230, 230),\n",
       " (230, 230),\n",
       " (231, 231),\n",
       " (231, 231),\n",
       " (232, 232),\n",
       " (232, 232),\n",
       " (233, 233),\n",
       " (233, 233),\n",
       " (234, 234),\n",
       " (234, 234),\n",
       " (235, 235),\n",
       " (235, 235),\n",
       " (236, 236),\n",
       " (236, 236),\n",
       " (237, 237),\n",
       " (237, 237),\n",
       " (238, 238),\n",
       " (238, 238),\n",
       " (239, 239),\n",
       " (239, 239),\n",
       " (240, 240),\n",
       " (240, 240),\n",
       " (241, 241),\n",
       " (241, 241),\n",
       " (242, 242),\n",
       " (242, 242),\n",
       " (242, 5520),\n",
       " (5520, 242),\n",
       " (242, 6916),\n",
       " (6916, 242),\n",
       " (242, 7468),\n",
       " (7468, 242),\n",
       " (242, 8775),\n",
       " (8775, 242),\n",
       " (243, 243),\n",
       " (243, 243),\n",
       " (244, 244),\n",
       " (244, 244),\n",
       " (245, 245),\n",
       " (245, 245),\n",
       " (246, 246),\n",
       " (246, 246),\n",
       " (247, 247),\n",
       " (247, 247),\n",
       " (248, 248),\n",
       " (248, 248),\n",
       " (248, 7589),\n",
       " (7589, 248),\n",
       " (249, 249),\n",
       " (249, 249),\n",
       " (250, 250),\n",
       " (250, 250),\n",
       " (251, 251),\n",
       " (251, 251),\n",
       " (252, 252),\n",
       " (252, 252),\n",
       " (253, 253),\n",
       " (253, 253),\n",
       " (254, 254),\n",
       " (254, 254),\n",
       " (255, 255),\n",
       " (255, 255),\n",
       " (256, 256),\n",
       " (256, 256),\n",
       " (257, 257),\n",
       " (257, 257),\n",
       " (258, 258),\n",
       " (258, 258),\n",
       " (259, 259),\n",
       " (259, 259),\n",
       " (260, 260),\n",
       " (260, 260),\n",
       " (261, 261),\n",
       " (261, 261),\n",
       " (262, 262),\n",
       " (262, 262),\n",
       " (263, 263),\n",
       " (263, 263),\n",
       " (264, 264),\n",
       " (264, 264),\n",
       " (265, 265),\n",
       " (265, 265),\n",
       " (266, 266),\n",
       " (266, 266),\n",
       " (267, 267),\n",
       " (267, 267),\n",
       " (268, 268),\n",
       " (268, 268),\n",
       " (269, 269),\n",
       " (269, 269),\n",
       " (270, 270),\n",
       " (270, 270),\n",
       " (271, 271),\n",
       " (271, 271),\n",
       " (271, 430),\n",
       " (430, 271),\n",
       " (271, 518),\n",
       " (518, 271),\n",
       " (271, 689),\n",
       " (689, 271),\n",
       " (272, 272),\n",
       " (272, 272),\n",
       " (273, 273),\n",
       " (273, 273),\n",
       " (274, 274),\n",
       " (274, 274),\n",
       " (275, 275),\n",
       " (275, 275),\n",
       " (276, 276),\n",
       " (276, 276),\n",
       " (277, 277),\n",
       " (277, 277),\n",
       " (278, 278),\n",
       " (278, 278),\n",
       " (279, 279),\n",
       " (279, 279),\n",
       " (280, 280),\n",
       " (280, 280),\n",
       " (281, 281),\n",
       " (281, 281),\n",
       " (282, 282),\n",
       " (282, 282),\n",
       " (283, 283),\n",
       " (283, 283),\n",
       " (284, 284),\n",
       " (284, 284),\n",
       " (285, 285),\n",
       " (285, 285),\n",
       " (286, 286),\n",
       " (286, 286),\n",
       " (286, 2110),\n",
       " (2110, 286),\n",
       " (286, 2179),\n",
       " (2179, 286),\n",
       " (286, 3847),\n",
       " (3847, 286),\n",
       " (287, 287),\n",
       " (287, 287),\n",
       " (288, 288),\n",
       " (288, 288),\n",
       " (289, 289),\n",
       " (289, 289),\n",
       " (290, 290),\n",
       " (290, 290),\n",
       " (291, 291),\n",
       " (291, 291),\n",
       " (292, 292),\n",
       " (292, 292),\n",
       " (293, 293),\n",
       " (293, 293),\n",
       " (294, 294),\n",
       " (294, 294),\n",
       " (295, 295),\n",
       " (295, 295),\n",
       " (296, 296),\n",
       " (296, 296),\n",
       " (297, 297),\n",
       " (297, 297),\n",
       " (297, 3417),\n",
       " (3417, 297),\n",
       " (298, 298),\n",
       " (298, 298),\n",
       " (299, 299),\n",
       " (299, 299),\n",
       " (299, 7596),\n",
       " (7596, 299),\n",
       " (300, 300),\n",
       " (300, 300),\n",
       " (301, 301),\n",
       " (301, 301),\n",
       " (302, 302),\n",
       " (302, 302),\n",
       " (303, 303),\n",
       " (303, 303),\n",
       " (304, 304),\n",
       " (304, 304),\n",
       " (305, 305),\n",
       " (305, 305),\n",
       " (306, 306),\n",
       " (306, 306),\n",
       " (306, 10427),\n",
       " (10427, 306),\n",
       " (307, 307),\n",
       " (307, 307),\n",
       " (308, 308),\n",
       " (308, 308),\n",
       " (308, 3517),\n",
       " (3517, 308),\n",
       " (308, 4840),\n",
       " (4840, 308),\n",
       " (308, 6307),\n",
       " (6307, 308),\n",
       " (308, 9936),\n",
       " (9936, 308),\n",
       " (309, 309),\n",
       " (309, 309),\n",
       " (310, 310),\n",
       " (310, 310),\n",
       " (311, 311),\n",
       " (311, 311),\n",
       " (312, 312),\n",
       " (312, 312),\n",
       " (313, 313),\n",
       " (313, 313),\n",
       " (314, 314),\n",
       " (314, 314),\n",
       " (315, 315),\n",
       " (315, 315),\n",
       " (316, 316),\n",
       " (316, 316),\n",
       " (317, 317),\n",
       " (317, 317),\n",
       " (317, 322),\n",
       " (322, 317),\n",
       " (318, 318),\n",
       " (318, 318),\n",
       " (319, 319),\n",
       " (319, 319),\n",
       " (319, 331),\n",
       " (331, 319),\n",
       " (320, 320),\n",
       " (320, 320),\n",
       " (321, 321),\n",
       " (321, 321),\n",
       " (322, 322),\n",
       " (322, 322),\n",
       " (323, 323),\n",
       " (323, 323),\n",
       " (324, 324),\n",
       " (324, 324),\n",
       " (325, 325),\n",
       " (325, 325),\n",
       " (326, 326),\n",
       " (326, 326),\n",
       " (327, 327),\n",
       " (327, 327),\n",
       " (328, 328),\n",
       " (328, 328),\n",
       " (329, 329),\n",
       " (329, 329),\n",
       " (329, 2094),\n",
       " (2094, 329),\n",
       " (330, 330),\n",
       " (330, 330),\n",
       " (331, 331),\n",
       " (331, 331),\n",
       " (332, 332),\n",
       " (332, 332),\n",
       " (333, 333),\n",
       " (333, 333),\n",
       " (334, 334),\n",
       " (334, 334),\n",
       " (334, 10620),\n",
       " (10620, 334),\n",
       " (335, 335),\n",
       " (335, 335),\n",
       " (336, 336),\n",
       " (336, 336),\n",
       " (337, 337),\n",
       " (337, 337),\n",
       " (338, 338),\n",
       " (338, 338),\n",
       " (339, 339),\n",
       " (339, 339),\n",
       " (340, 340),\n",
       " (340, 340),\n",
       " (341, 341),\n",
       " (341, 341),\n",
       " (342, 342),\n",
       " (342, 342),\n",
       " (343, 343),\n",
       " (343, 343),\n",
       " (344, 344),\n",
       " (344, 344),\n",
       " (345, 345),\n",
       " (345, 345),\n",
       " (346, 346),\n",
       " (346, 346),\n",
       " (347, 347),\n",
       " (347, 347),\n",
       " (348, 348),\n",
       " (348, 348),\n",
       " (349, 349),\n",
       " (349, 349),\n",
       " (350, 350),\n",
       " (350, 350),\n",
       " (350, 1020),\n",
       " (1020, 350),\n",
       " (350, 9045),\n",
       " (9045, 350),\n",
       " (351, 351),\n",
       " (351, 351),\n",
       " (352, 352),\n",
       " (352, 352),\n",
       " (353, 353),\n",
       " (353, 353),\n",
       " (354, 354),\n",
       " (354, 354),\n",
       " (355, 355),\n",
       " (355, 355),\n",
       " (356, 356),\n",
       " (356, 356),\n",
       " (357, 357),\n",
       " (357, 357),\n",
       " (358, 358),\n",
       " (358, 358),\n",
       " (359, 359),\n",
       " (359, 359),\n",
       " (360, 360),\n",
       " (360, 360),\n",
       " (361, 361),\n",
       " (361, 361),\n",
       " (362, 362),\n",
       " (362, 362),\n",
       " (362, 7416),\n",
       " (7416, 362),\n",
       " (363, 363),\n",
       " (363, 363),\n",
       " (363, 3236),\n",
       " (3236, 363),\n",
       " (364, 364),\n",
       " (364, 364),\n",
       " (365, 365),\n",
       " (365, 365),\n",
       " (366, 366),\n",
       " (366, 366),\n",
       " (367, 367),\n",
       " (367, 367),\n",
       " (368, 368),\n",
       " (368, 368),\n",
       " (369, 369),\n",
       " (369, 369),\n",
       " (370, 370),\n",
       " (370, 370),\n",
       " (371, 371),\n",
       " (371, 371),\n",
       " (372, 372),\n",
       " (372, 372),\n",
       " (373, 373),\n",
       " (373, 373),\n",
       " (374, 374),\n",
       " (374, 374),\n",
       " (375, 375),\n",
       " (375, 375),\n",
       " (376, 376),\n",
       " (376, 376),\n",
       " (377, 377),\n",
       " (377, 377),\n",
       " (378, 378),\n",
       " (378, 378),\n",
       " (379, 379),\n",
       " (379, 379),\n",
       " (380, 380),\n",
       " (380, 380),\n",
       " (381, 381),\n",
       " (381, 381),\n",
       " (382, 382),\n",
       " (382, 382),\n",
       " (383, 383),\n",
       " (383, 383),\n",
       " (384, 384),\n",
       " (384, 384),\n",
       " (384, 966),\n",
       " (966, 384),\n",
       " (384, 1723),\n",
       " (1723, 384),\n",
       " (385, 385),\n",
       " (385, 385),\n",
       " (386, 386),\n",
       " (386, 386),\n",
       " (387, 387),\n",
       " (387, 387),\n",
       " (388, 388),\n",
       " (388, 388),\n",
       " (389, 389),\n",
       " (389, 389),\n",
       " (390, 390),\n",
       " (390, 390),\n",
       " (391, 391),\n",
       " (391, 391),\n",
       " (392, 392),\n",
       " (392, 392),\n",
       " (393, 393),\n",
       " (393, 393),\n",
       " (393, 4180),\n",
       " (4180, 393),\n",
       " (394, 394),\n",
       " (394, 394),\n",
       " (395, 395),\n",
       " (395, 395),\n",
       " (396, 396),\n",
       " (396, 396),\n",
       " (397, 397),\n",
       " (397, 397),\n",
       " (398, 398),\n",
       " (398, 398),\n",
       " (399, 399),\n",
       " (399, 399),\n",
       " (400, 400),\n",
       " (400, 400),\n",
       " (401, 401),\n",
       " (401, 401),\n",
       " (402, 402),\n",
       " (402, 402),\n",
       " (403, 403),\n",
       " (403, 403),\n",
       " (404, 404),\n",
       " (404, 404),\n",
       " (405, 405),\n",
       " (405, 405),\n",
       " (406, 406),\n",
       " (406, 406),\n",
       " (407, 407),\n",
       " (407, 407),\n",
       " (408, 408),\n",
       " (408, 408),\n",
       " (409, 409),\n",
       " (409, 409),\n",
       " (410, 410),\n",
       " (410, 410),\n",
       " (411, 411),\n",
       " (411, 411),\n",
       " (412, 412),\n",
       " (412, 412),\n",
       " (413, 413),\n",
       " (413, 413),\n",
       " (414, 414),\n",
       " (414, 414),\n",
       " (415, 415),\n",
       " (415, 415),\n",
       " (416, 416),\n",
       " (416, 416),\n",
       " (417, 417),\n",
       " (417, 417),\n",
       " (418, 418),\n",
       " (418, 418),\n",
       " (419, 419),\n",
       " (419, 419),\n",
       " (419, 1758),\n",
       " (1758, 419),\n",
       " (419, 9457),\n",
       " (9457, 419),\n",
       " (420, 420),\n",
       " (420, 420),\n",
       " (421, 421),\n",
       " (421, 421),\n",
       " (422, 422),\n",
       " (422, 422),\n",
       " (423, 423),\n",
       " (423, 423),\n",
       " (423, 4308),\n",
       " (4308, 423),\n",
       " (424, 424),\n",
       " (424, 424),\n",
       " (425, 425),\n",
       " (425, 425),\n",
       " (426, 426),\n",
       " (426, 426),\n",
       " (427, 427),\n",
       " (427, 427),\n",
       " (428, 428),\n",
       " (428, 428),\n",
       " (429, 429),\n",
       " (429, 429),\n",
       " (430, 430),\n",
       " (430, 430),\n",
       " (430, 518),\n",
       " (518, 430),\n",
       " (430, 689),\n",
       " (689, 430),\n",
       " (431, 431),\n",
       " (431, 431),\n",
       " (432, 432),\n",
       " (432, 432),\n",
       " (433, 433),\n",
       " (433, 433),\n",
       " (434, 434),\n",
       " (434, 434),\n",
       " (435, 435),\n",
       " (435, 435),\n",
       " (436, 436),\n",
       " (436, 436),\n",
       " (437, 437),\n",
       " (437, 437),\n",
       " (438, 438),\n",
       " (438, 438),\n",
       " (439, 439),\n",
       " (439, 439),\n",
       " (440, 440),\n",
       " (440, 440),\n",
       " (441, 441),\n",
       " (441, 441),\n",
       " (442, 442),\n",
       " (442, 442),\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_edges(features, threshold=0.5):\n",
    "    similarity_matrix = cosine_similarity(features)\n",
    "    edges = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i, len(features)):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                edges.append((i, j))\n",
    "                edges.append((j, i))\n",
    "    return edges\n",
    "\n",
    "edges = build_edges(feature_embeddings, threshold=0.8)\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting labels to numeric value</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(twitter_data['label'].tolist())\n",
    "label_tensor = torch.tensor(label_encoded, dtype=torch.long).unsqueeze(1)\n",
    "label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.0812,  0.0170,  ...,  0.0391,  0.0038, -0.1770],\n",
       "        [ 0.0254, -0.0611,  0.0312,  ..., -0.0036,  0.0272, -0.0790],\n",
       "        [ 0.0242, -0.0003,  0.0522,  ..., -0.0283, -0.0319,  0.0106],\n",
       "        ...,\n",
       "        [-0.0991,  0.0148,  0.0329,  ...,  0.0250,  0.0063, -0.0639],\n",
       "        [-0.0535, -0.0089,  0.0194,  ...,  0.1068, -0.0364,  0.0584],\n",
       "        [-0.0138,  0.0522,  0.0515,  ...,  0.0227, -0.0691, -0.0231]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = feature_embeddings\n",
    "node_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Converting edges to edge index to make graph data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0],\n",
       "        [    0,     0],\n",
       "        [    1,     1],\n",
       "        ...,\n",
       "        [10748, 10748],\n",
       "        [10749, 10749],\n",
       "        [10749, 10749]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label = label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = edge_index.t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Making graph data to train and evaluate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[10750, 384], edge_index=[2, 23808], y=[10750, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data = Data(x=node_features, edge_index=edge_index, y=node_label)\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting data in to train, validation and test</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "splitter = RandomNodeSplit(split=\"train_rest\", num_val=0.2, num_test=0.2)\n",
    "graph_data = splitter(graph_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access masks for each set\n",
    "train_mask = graph_data.train_mask\n",
    "val_mask = graph_data.val_mask\n",
    "test_mask = graph_data.test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     1,  ..., 10748, 10749, 10749],\n",
       "        [    0,     0,     1,  ..., 10748, 10749, 10749]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6450]) torch.Size([2150]) torch.Size([2150])\n"
     ]
    }
   ],
   "source": [
    "train_nodes = train_mask.nonzero().flatten()\n",
    "val_nodes = val_mask.nonzero().flatten()\n",
    "test_nodes = test_mask.nonzero().flatten()\n",
    "print(train_nodes.shape, val_nodes.shape, test_nodes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Splitting edges to train, validation and test accourding to train, validation and test nodes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_splitter = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
    "train_data, val_data, test_data = edge_splitter(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[10750, 384], edge_index=[2, 14286], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[28572], edge_label_index=[2, 28572])\n",
      "Data(x=[10750, 384], edge_index=[2, 14286], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[9522], edge_label_index=[2, 9522])\n",
      "Data(x=[10750, 384], edge_index=[2, 19047], y=[10750, 1], train_mask=[10750], val_mask=[10750], test_mask=[10750], edge_label=[9522], edge_label_index=[2, 9522])\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for GCNConv model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = graph_data.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>GCNConv model to evaluate twitter data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Graph Neural Network model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "model = GCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()  # Binary classification loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training function, calculating loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data.x, train_data.edge_index)\n",
    "    loss = loss_fn(out[train_data.train_mask], train_data.y[train_data.train_mask].squeeze().float())\n",
    "    # loss = F.nll_loss(out[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluating model and calculating Accuracy, precision, recall and f1 score</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Convert probabilities to binary predictions (0 or 1)\n",
    "        preds = (probs > 0.5).cpu().numpy()\n",
    "        \n",
    "        preds = preds[data.test_mask.cpu().numpy()]\n",
    "\n",
    "        y_true = data.y[data.test_mask].squeeze().cpu().numpy()        \n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        precision = precision_score(y_true, preds, average=\"binary\")\n",
    "        recall = recall_score(y_true, preds, average=\"binary\")\n",
    "        f1 = f1_score(y_true, preds, average=\"binary\")\n",
    "        return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training for multiple epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6948, Test Accuracy: 0.5837\n",
      "Epoch 20, Loss: 0.5922, Test Accuracy: 0.6642\n",
      "Epoch 40, Loss: 0.5719, Test Accuracy: 0.6684\n",
      "Epoch 60, Loss: 0.5614, Test Accuracy: 0.6679\n",
      "Epoch 80, Loss: 0.5516, Test Accuracy: 0.6726\n",
      "Epoch 100, Loss: 0.5405, Test Accuracy: 0.6772\n",
      "Epoch 120, Loss: 0.5292, Test Accuracy: 0.6786\n",
      "Epoch 140, Loss: 0.5243, Test Accuracy: 0.6800\n",
      "Epoch 160, Loss: 0.5182, Test Accuracy: 0.6800\n",
      "Epoch 180, Loss: 0.5080, Test Accuracy: 0.6735\n",
      "Final Test Accuracy: 0.6753\n",
      "Final Test Precision: 0.6862\n",
      "Final Test Recall: 0.6141\n",
      "Final Test F1: 0.6482\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_val_acc = 0\n",
    "patience = 10  # Stop if validation accuracy does not improve for 10 epochs\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    train_acc, _, _, _ = evaluate(train_data)\n",
    "    val_acc, _, _, _ = evaluate(val_data)\n",
    "    test_acc, _, _, _ = evaluate(test_data)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate()\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate()\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Class for GAT model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAT Model for Binary Classification\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        # First GAT layer (multi-head attention)\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        # Second GAT layer (single-head for binary output)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, 1, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)  # Use ELU activation function\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.view(-1)  # Output raw logits (no sigmoid here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Parameters for GAT model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_data.num_node_features\n",
    "hidden_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GAT model\n",
    "model = GAT(in_channels=input_dim, hidden_channels=hidden_dim).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training for multiple epochs</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6947, Test Accuracy: 0.5953\n",
      "Epoch 20, Loss: 0.6500, Test Accuracy: 0.6665\n",
      "Epoch 40, Loss: 0.6492, Test Accuracy: 0.6619\n",
      "Epoch 60, Loss: 0.6489, Test Accuracy: 0.6628\n",
      "Epoch 80, Loss: 0.6482, Test Accuracy: 0.6670\n",
      "Epoch 100, Loss: 0.6477, Test Accuracy: 0.6647\n",
      "Epoch 120, Loss: 0.6523, Test Accuracy: 0.6660\n",
      "Epoch 140, Loss: 0.6485, Test Accuracy: 0.6647\n",
      "Epoch 160, Loss: 0.6475, Test Accuracy: 0.6674\n",
      "Epoch 180, Loss: 0.6439, Test Accuracy: 0.6670\n",
      "Final Test Accuracy: 0.6712\n",
      "Final Test Precision: 0.7088\n",
      "Final Test Recall: 0.5511\n",
      "Final Test F1: 0.6201\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate()\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate()\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(twitter_data['feature'].tolist(), padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(tokens[\"input_ids\"], tokens[\"attention_mask\"], label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.6, 0.2, 0.2], generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertGenderClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertGenderClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)  # Regularization\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, 1)  # Binary classification (male/female)\n",
    "        self.activation = nn.Sigmoid()  # Sigmoid activation\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = BertGenderClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed: 8 in total data: 807\n",
      "Data processed: 16 in total data: 807\n",
      "Data processed: 24 in total data: 807\n",
      "Data processed: 32 in total data: 807\n",
      "Data processed: 40 in total data: 807\n",
      "Data processed: 48 in total data: 807\n",
      "Data processed: 56 in total data: 807\n",
      "Data processed: 64 in total data: 807\n",
      "Data processed: 72 in total data: 807\n",
      "Data processed: 80 in total data: 807\n",
      "Data processed: 88 in total data: 807\n",
      "Data processed: 96 in total data: 807\n",
      "Data processed: 104 in total data: 807\n",
      "Data processed: 112 in total data: 807\n",
      "Data processed: 120 in total data: 807\n",
      "Data processed: 128 in total data: 807\n",
      "Data processed: 136 in total data: 807\n",
      "Data processed: 144 in total data: 807\n",
      "Data processed: 152 in total data: 807\n",
      "Data processed: 160 in total data: 807\n",
      "Data processed: 168 in total data: 807\n",
      "Data processed: 176 in total data: 807\n",
      "Data processed: 184 in total data: 807\n",
      "Data processed: 192 in total data: 807\n",
      "Data processed: 200 in total data: 807\n",
      "Data processed: 208 in total data: 807\n",
      "Data processed: 216 in total data: 807\n",
      "Data processed: 224 in total data: 807\n",
      "Data processed: 232 in total data: 807\n",
      "Data processed: 240 in total data: 807\n",
      "Data processed: 248 in total data: 807\n",
      "Data processed: 256 in total data: 807\n",
      "Data processed: 264 in total data: 807\n",
      "Data processed: 272 in total data: 807\n",
      "Data processed: 280 in total data: 807\n",
      "Data processed: 288 in total data: 807\n",
      "Data processed: 296 in total data: 807\n",
      "Data processed: 304 in total data: 807\n",
      "Data processed: 312 in total data: 807\n",
      "Data processed: 320 in total data: 807\n",
      "Data processed: 328 in total data: 807\n",
      "Data processed: 336 in total data: 807\n",
      "Data processed: 344 in total data: 807\n",
      "Data processed: 352 in total data: 807\n",
      "Data processed: 360 in total data: 807\n",
      "Data processed: 368 in total data: 807\n",
      "Data processed: 376 in total data: 807\n",
      "Data processed: 384 in total data: 807\n",
      "Data processed: 392 in total data: 807\n",
      "Data processed: 400 in total data: 807\n",
      "Data processed: 408 in total data: 807\n",
      "Data processed: 416 in total data: 807\n",
      "Data processed: 424 in total data: 807\n",
      "Data processed: 432 in total data: 807\n",
      "Data processed: 440 in total data: 807\n",
      "Data processed: 448 in total data: 807\n",
      "Data processed: 456 in total data: 807\n",
      "Data processed: 464 in total data: 807\n",
      "Data processed: 472 in total data: 807\n",
      "Data processed: 480 in total data: 807\n",
      "Data processed: 488 in total data: 807\n",
      "Data processed: 496 in total data: 807\n",
      "Data processed: 504 in total data: 807\n",
      "Data processed: 512 in total data: 807\n",
      "Data processed: 520 in total data: 807\n",
      "Data processed: 528 in total data: 807\n",
      "Data processed: 536 in total data: 807\n",
      "Data processed: 544 in total data: 807\n",
      "Data processed: 552 in total data: 807\n",
      "Data processed: 560 in total data: 807\n",
      "Data processed: 568 in total data: 807\n",
      "Data processed: 576 in total data: 807\n",
      "Data processed: 584 in total data: 807\n",
      "Data processed: 592 in total data: 807\n",
      "Data processed: 600 in total data: 807\n",
      "Data processed: 608 in total data: 807\n",
      "Data processed: 616 in total data: 807\n",
      "Data processed: 624 in total data: 807\n",
      "Data processed: 632 in total data: 807\n",
      "Data processed: 640 in total data: 807\n",
      "Data processed: 648 in total data: 807\n",
      "Data processed: 656 in total data: 807\n",
      "Data processed: 664 in total data: 807\n",
      "Data processed: 672 in total data: 807\n",
      "Data processed: 680 in total data: 807\n",
      "Data processed: 688 in total data: 807\n",
      "Data processed: 696 in total data: 807\n",
      "Data processed: 704 in total data: 807\n",
      "Data processed: 712 in total data: 807\n",
      "Data processed: 720 in total data: 807\n",
      "Data processed: 728 in total data: 807\n",
      "Data processed: 736 in total data: 807\n",
      "Data processed: 744 in total data: 807\n",
      "Data processed: 752 in total data: 807\n",
      "Data processed: 760 in total data: 807\n",
      "Data processed: 768 in total data: 807\n",
      "Data processed: 776 in total data: 807\n",
      "Data processed: 784 in total data: 807\n",
      "Data processed: 792 in total data: 807\n",
      "Data processed: 800 in total data: 807\n",
      "Data processed: 808 in total data: 807\n",
      "Data processed: 816 in total data: 807\n",
      "Data processed: 824 in total data: 807\n",
      "Data processed: 832 in total data: 807\n",
      "Data processed: 840 in total data: 807\n",
      "Data processed: 848 in total data: 807\n",
      "Data processed: 856 in total data: 807\n",
      "Data processed: 864 in total data: 807\n",
      "Data processed: 872 in total data: 807\n",
      "Data processed: 880 in total data: 807\n",
      "Data processed: 888 in total data: 807\n",
      "Data processed: 896 in total data: 807\n",
      "Data processed: 904 in total data: 807\n",
      "Data processed: 912 in total data: 807\n",
      "Data processed: 920 in total data: 807\n",
      "Data processed: 928 in total data: 807\n",
      "Data processed: 936 in total data: 807\n",
      "Data processed: 944 in total data: 807\n",
      "Data processed: 952 in total data: 807\n",
      "Data processed: 960 in total data: 807\n",
      "Data processed: 968 in total data: 807\n",
      "Data processed: 976 in total data: 807\n",
      "Data processed: 984 in total data: 807\n",
      "Data processed: 992 in total data: 807\n",
      "Data processed: 1000 in total data: 807\n",
      "Data processed: 1008 in total data: 807\n",
      "Data processed: 1016 in total data: 807\n",
      "Data processed: 1024 in total data: 807\n",
      "Data processed: 1032 in total data: 807\n",
      "Data processed: 1040 in total data: 807\n",
      "Data processed: 1048 in total data: 807\n",
      "Data processed: 1056 in total data: 807\n",
      "Data processed: 1064 in total data: 807\n",
      "Data processed: 1072 in total data: 807\n",
      "Data processed: 1080 in total data: 807\n",
      "Data processed: 1088 in total data: 807\n",
      "Data processed: 1096 in total data: 807\n",
      "Data processed: 1104 in total data: 807\n",
      "Data processed: 1112 in total data: 807\n",
      "Data processed: 1120 in total data: 807\n",
      "Data processed: 1128 in total data: 807\n",
      "Data processed: 1136 in total data: 807\n",
      "Data processed: 1144 in total data: 807\n",
      "Data processed: 1152 in total data: 807\n",
      "Data processed: 1160 in total data: 807\n",
      "Data processed: 1168 in total data: 807\n",
      "Data processed: 1176 in total data: 807\n",
      "Data processed: 1184 in total data: 807\n",
      "Data processed: 1192 in total data: 807\n",
      "Data processed: 1200 in total data: 807\n",
      "Data processed: 1208 in total data: 807\n",
      "Data processed: 1216 in total data: 807\n",
      "Data processed: 1224 in total data: 807\n",
      "Data processed: 1232 in total data: 807\n",
      "Data processed: 1240 in total data: 807\n",
      "Data processed: 1248 in total data: 807\n",
      "Data processed: 1256 in total data: 807\n",
      "Data processed: 1264 in total data: 807\n",
      "Data processed: 1272 in total data: 807\n",
      "Data processed: 1280 in total data: 807\n",
      "Data processed: 1288 in total data: 807\n",
      "Data processed: 1296 in total data: 807\n",
      "Data processed: 1304 in total data: 807\n",
      "Data processed: 1312 in total data: 807\n",
      "Data processed: 1320 in total data: 807\n",
      "Data processed: 1328 in total data: 807\n",
      "Data processed: 1336 in total data: 807\n",
      "Data processed: 1344 in total data: 807\n",
      "Data processed: 1352 in total data: 807\n",
      "Data processed: 1360 in total data: 807\n",
      "Data processed: 1368 in total data: 807\n",
      "Data processed: 1376 in total data: 807\n",
      "Data processed: 1384 in total data: 807\n",
      "Data processed: 1392 in total data: 807\n",
      "Data processed: 1400 in total data: 807\n",
      "Data processed: 1408 in total data: 807\n",
      "Data processed: 1416 in total data: 807\n",
      "Data processed: 1424 in total data: 807\n",
      "Data processed: 1432 in total data: 807\n",
      "Data processed: 1440 in total data: 807\n",
      "Data processed: 1448 in total data: 807\n",
      "Data processed: 1456 in total data: 807\n",
      "Data processed: 1464 in total data: 807\n",
      "Data processed: 1472 in total data: 807\n",
      "Data processed: 1480 in total data: 807\n",
      "Data processed: 1488 in total data: 807\n",
      "Data processed: 1496 in total data: 807\n",
      "Data processed: 1504 in total data: 807\n",
      "Data processed: 1512 in total data: 807\n",
      "Data processed: 1520 in total data: 807\n",
      "Data processed: 1528 in total data: 807\n",
      "Data processed: 1536 in total data: 807\n",
      "Data processed: 1544 in total data: 807\n",
      "Data processed: 1552 in total data: 807\n",
      "Data processed: 1560 in total data: 807\n",
      "Data processed: 1568 in total data: 807\n",
      "Data processed: 1576 in total data: 807\n",
      "Data processed: 1584 in total data: 807\n",
      "Data processed: 1592 in total data: 807\n",
      "Data processed: 1600 in total data: 807\n",
      "Data processed: 1608 in total data: 807\n",
      "Data processed: 1616 in total data: 807\n",
      "Data processed: 1624 in total data: 807\n",
      "Data processed: 1632 in total data: 807\n",
      "Data processed: 1640 in total data: 807\n",
      "Data processed: 1648 in total data: 807\n",
      "Data processed: 1656 in total data: 807\n",
      "Data processed: 1664 in total data: 807\n",
      "Data processed: 1672 in total data: 807\n",
      "Data processed: 1680 in total data: 807\n",
      "Data processed: 1688 in total data: 807\n",
      "Data processed: 1696 in total data: 807\n",
      "Data processed: 1704 in total data: 807\n",
      "Data processed: 1712 in total data: 807\n",
      "Data processed: 1720 in total data: 807\n",
      "Data processed: 1728 in total data: 807\n",
      "Data processed: 1736 in total data: 807\n",
      "Data processed: 1744 in total data: 807\n",
      "Data processed: 1752 in total data: 807\n",
      "Data processed: 1760 in total data: 807\n",
      "Data processed: 1768 in total data: 807\n",
      "Data processed: 1776 in total data: 807\n",
      "Data processed: 1784 in total data: 807\n",
      "Data processed: 1792 in total data: 807\n",
      "Data processed: 1800 in total data: 807\n",
      "Data processed: 1808 in total data: 807\n",
      "Data processed: 1816 in total data: 807\n",
      "Data processed: 1824 in total data: 807\n",
      "Data processed: 1832 in total data: 807\n",
      "Data processed: 1840 in total data: 807\n",
      "Data processed: 1848 in total data: 807\n",
      "Data processed: 1856 in total data: 807\n",
      "Data processed: 1864 in total data: 807\n",
      "Data processed: 1872 in total data: 807\n",
      "Data processed: 1880 in total data: 807\n",
      "Data processed: 1888 in total data: 807\n",
      "Data processed: 1896 in total data: 807\n",
      "Data processed: 1904 in total data: 807\n",
      "Data processed: 1912 in total data: 807\n",
      "Data processed: 1920 in total data: 807\n",
      "Data processed: 1928 in total data: 807\n",
      "Data processed: 1936 in total data: 807\n",
      "Data processed: 1944 in total data: 807\n",
      "Data processed: 1952 in total data: 807\n",
      "Data processed: 1960 in total data: 807\n",
      "Data processed: 1968 in total data: 807\n",
      "Data processed: 1976 in total data: 807\n",
      "Data processed: 1984 in total data: 807\n",
      "Data processed: 1992 in total data: 807\n",
      "Data processed: 2000 in total data: 807\n",
      "Data processed: 2008 in total data: 807\n",
      "Data processed: 2016 in total data: 807\n",
      "Data processed: 2024 in total data: 807\n",
      "Data processed: 2032 in total data: 807\n",
      "Data processed: 2040 in total data: 807\n",
      "Data processed: 2048 in total data: 807\n",
      "Data processed: 2056 in total data: 807\n",
      "Data processed: 2064 in total data: 807\n",
      "Data processed: 2072 in total data: 807\n",
      "Data processed: 2080 in total data: 807\n",
      "Data processed: 2088 in total data: 807\n",
      "Data processed: 2096 in total data: 807\n",
      "Data processed: 2104 in total data: 807\n",
      "Data processed: 2112 in total data: 807\n",
      "Data processed: 2120 in total data: 807\n",
      "Data processed: 2128 in total data: 807\n",
      "Data processed: 2136 in total data: 807\n",
      "Data processed: 2144 in total data: 807\n",
      "Data processed: 2152 in total data: 807\n",
      "Data processed: 2160 in total data: 807\n",
      "Data processed: 2168 in total data: 807\n",
      "Data processed: 2176 in total data: 807\n",
      "Data processed: 2184 in total data: 807\n",
      "Data processed: 2192 in total data: 807\n",
      "Data processed: 2200 in total data: 807\n",
      "Data processed: 2208 in total data: 807\n",
      "Data processed: 2216 in total data: 807\n",
      "Data processed: 2224 in total data: 807\n",
      "Data processed: 2232 in total data: 807\n",
      "Data processed: 2240 in total data: 807\n",
      "Data processed: 2248 in total data: 807\n",
      "Data processed: 2256 in total data: 807\n",
      "Data processed: 2264 in total data: 807\n",
      "Data processed: 2272 in total data: 807\n",
      "Data processed: 2280 in total data: 807\n",
      "Data processed: 2288 in total data: 807\n",
      "Data processed: 2296 in total data: 807\n",
      "Data processed: 2304 in total data: 807\n",
      "Data processed: 2312 in total data: 807\n",
      "Data processed: 2320 in total data: 807\n",
      "Data processed: 2328 in total data: 807\n",
      "Data processed: 2336 in total data: 807\n",
      "Data processed: 2344 in total data: 807\n",
      "Data processed: 2352 in total data: 807\n",
      "Data processed: 2360 in total data: 807\n",
      "Data processed: 2368 in total data: 807\n",
      "Data processed: 2376 in total data: 807\n",
      "Data processed: 2384 in total data: 807\n",
      "Data processed: 2392 in total data: 807\n",
      "Data processed: 2400 in total data: 807\n",
      "Data processed: 2408 in total data: 807\n",
      "Data processed: 2416 in total data: 807\n",
      "Data processed: 2424 in total data: 807\n",
      "Data processed: 2432 in total data: 807\n",
      "Data processed: 2440 in total data: 807\n",
      "Data processed: 2448 in total data: 807\n",
      "Data processed: 2456 in total data: 807\n",
      "Data processed: 2464 in total data: 807\n",
      "Data processed: 2472 in total data: 807\n",
      "Data processed: 2480 in total data: 807\n",
      "Data processed: 2488 in total data: 807\n",
      "Data processed: 2496 in total data: 807\n",
      "Data processed: 2504 in total data: 807\n",
      "Data processed: 2512 in total data: 807\n",
      "Data processed: 2520 in total data: 807\n",
      "Data processed: 2528 in total data: 807\n",
      "Data processed: 2536 in total data: 807\n",
      "Data processed: 2544 in total data: 807\n",
      "Data processed: 2552 in total data: 807\n",
      "Data processed: 2560 in total data: 807\n",
      "Data processed: 2568 in total data: 807\n",
      "Data processed: 2576 in total data: 807\n",
      "Data processed: 2584 in total data: 807\n",
      "Data processed: 2592 in total data: 807\n",
      "Data processed: 2600 in total data: 807\n",
      "Data processed: 2608 in total data: 807\n",
      "Data processed: 2616 in total data: 807\n",
      "Data processed: 2624 in total data: 807\n",
      "Data processed: 2632 in total data: 807\n",
      "Data processed: 2640 in total data: 807\n",
      "Data processed: 2648 in total data: 807\n",
      "Data processed: 2656 in total data: 807\n",
      "Data processed: 2664 in total data: 807\n",
      "Data processed: 2672 in total data: 807\n",
      "Data processed: 2680 in total data: 807\n",
      "Data processed: 2688 in total data: 807\n",
      "Data processed: 2696 in total data: 807\n",
      "Data processed: 2704 in total data: 807\n",
      "Data processed: 2712 in total data: 807\n",
      "Data processed: 2720 in total data: 807\n",
      "Data processed: 2728 in total data: 807\n",
      "Data processed: 2736 in total data: 807\n",
      "Data processed: 2744 in total data: 807\n",
      "Data processed: 2752 in total data: 807\n",
      "Data processed: 2760 in total data: 807\n",
      "Data processed: 2768 in total data: 807\n",
      "Data processed: 2776 in total data: 807\n",
      "Data processed: 2784 in total data: 807\n",
      "Data processed: 2792 in total data: 807\n",
      "Data processed: 2800 in total data: 807\n",
      "Data processed: 2808 in total data: 807\n",
      "Data processed: 2816 in total data: 807\n",
      "Data processed: 2824 in total data: 807\n",
      "Data processed: 2832 in total data: 807\n",
      "Data processed: 2840 in total data: 807\n",
      "Data processed: 2848 in total data: 807\n",
      "Data processed: 2856 in total data: 807\n",
      "Data processed: 2864 in total data: 807\n",
      "Data processed: 2872 in total data: 807\n",
      "Data processed: 2880 in total data: 807\n",
      "Data processed: 2888 in total data: 807\n",
      "Data processed: 2896 in total data: 807\n",
      "Data processed: 2904 in total data: 807\n",
      "Data processed: 2912 in total data: 807\n",
      "Data processed: 2920 in total data: 807\n",
      "Data processed: 2928 in total data: 807\n",
      "Data processed: 2936 in total data: 807\n",
      "Data processed: 2944 in total data: 807\n",
      "Data processed: 2952 in total data: 807\n",
      "Data processed: 2960 in total data: 807\n",
      "Data processed: 2968 in total data: 807\n",
      "Data processed: 2976 in total data: 807\n",
      "Data processed: 2984 in total data: 807\n",
      "Data processed: 2992 in total data: 807\n",
      "Data processed: 3000 in total data: 807\n",
      "Data processed: 3008 in total data: 807\n",
      "Data processed: 3016 in total data: 807\n",
      "Data processed: 3024 in total data: 807\n",
      "Data processed: 3032 in total data: 807\n",
      "Data processed: 3040 in total data: 807\n",
      "Data processed: 3048 in total data: 807\n",
      "Data processed: 3056 in total data: 807\n",
      "Data processed: 3064 in total data: 807\n",
      "Data processed: 3072 in total data: 807\n",
      "Data processed: 3080 in total data: 807\n",
      "Data processed: 3088 in total data: 807\n",
      "Data processed: 3096 in total data: 807\n",
      "Data processed: 3104 in total data: 807\n",
      "Data processed: 3112 in total data: 807\n",
      "Data processed: 3120 in total data: 807\n",
      "Data processed: 3128 in total data: 807\n",
      "Data processed: 3136 in total data: 807\n",
      "Data processed: 3144 in total data: 807\n",
      "Data processed: 3152 in total data: 807\n",
      "Data processed: 3160 in total data: 807\n",
      "Data processed: 3168 in total data: 807\n",
      "Data processed: 3176 in total data: 807\n",
      "Data processed: 3184 in total data: 807\n",
      "Data processed: 3192 in total data: 807\n",
      "Data processed: 3200 in total data: 807\n",
      "Data processed: 3208 in total data: 807\n",
      "Data processed: 3216 in total data: 807\n",
      "Data processed: 3224 in total data: 807\n",
      "Data processed: 3232 in total data: 807\n",
      "Data processed: 3240 in total data: 807\n",
      "Data processed: 3248 in total data: 807\n",
      "Data processed: 3256 in total data: 807\n",
      "Data processed: 3264 in total data: 807\n",
      "Data processed: 3272 in total data: 807\n",
      "Data processed: 3280 in total data: 807\n",
      "Data processed: 3288 in total data: 807\n",
      "Data processed: 3296 in total data: 807\n",
      "Data processed: 3304 in total data: 807\n",
      "Data processed: 3312 in total data: 807\n",
      "Data processed: 3320 in total data: 807\n",
      "Data processed: 3328 in total data: 807\n",
      "Data processed: 3336 in total data: 807\n",
      "Data processed: 3344 in total data: 807\n",
      "Data processed: 3352 in total data: 807\n",
      "Data processed: 3360 in total data: 807\n",
      "Data processed: 3368 in total data: 807\n",
      "Data processed: 3376 in total data: 807\n",
      "Data processed: 3384 in total data: 807\n",
      "Data processed: 3392 in total data: 807\n",
      "Data processed: 3400 in total data: 807\n",
      "Data processed: 3408 in total data: 807\n",
      "Data processed: 3416 in total data: 807\n",
      "Data processed: 3424 in total data: 807\n",
      "Data processed: 3432 in total data: 807\n",
      "Data processed: 3440 in total data: 807\n",
      "Data processed: 3448 in total data: 807\n",
      "Data processed: 3456 in total data: 807\n",
      "Data processed: 3464 in total data: 807\n",
      "Data processed: 3472 in total data: 807\n",
      "Data processed: 3480 in total data: 807\n",
      "Data processed: 3488 in total data: 807\n",
      "Data processed: 3496 in total data: 807\n",
      "Data processed: 3504 in total data: 807\n",
      "Data processed: 3512 in total data: 807\n",
      "Data processed: 3520 in total data: 807\n",
      "Data processed: 3528 in total data: 807\n",
      "Data processed: 3536 in total data: 807\n",
      "Data processed: 3544 in total data: 807\n",
      "Data processed: 3552 in total data: 807\n",
      "Data processed: 3560 in total data: 807\n",
      "Data processed: 3568 in total data: 807\n",
      "Data processed: 3576 in total data: 807\n",
      "Data processed: 3584 in total data: 807\n",
      "Data processed: 3592 in total data: 807\n",
      "Data processed: 3600 in total data: 807\n",
      "Data processed: 3608 in total data: 807\n",
      "Data processed: 3616 in total data: 807\n",
      "Data processed: 3624 in total data: 807\n",
      "Data processed: 3632 in total data: 807\n",
      "Data processed: 3640 in total data: 807\n",
      "Data processed: 3648 in total data: 807\n",
      "Data processed: 3656 in total data: 807\n",
      "Data processed: 3664 in total data: 807\n",
      "Data processed: 3672 in total data: 807\n",
      "Data processed: 3680 in total data: 807\n",
      "Data processed: 3688 in total data: 807\n",
      "Data processed: 3696 in total data: 807\n",
      "Data processed: 3704 in total data: 807\n",
      "Data processed: 3712 in total data: 807\n",
      "Data processed: 3720 in total data: 807\n",
      "Data processed: 3728 in total data: 807\n",
      "Data processed: 3736 in total data: 807\n",
      "Data processed: 3744 in total data: 807\n",
      "Data processed: 3752 in total data: 807\n",
      "Data processed: 3760 in total data: 807\n",
      "Data processed: 3768 in total data: 807\n",
      "Data processed: 3776 in total data: 807\n",
      "Data processed: 3784 in total data: 807\n",
      "Data processed: 3792 in total data: 807\n",
      "Data processed: 3800 in total data: 807\n",
      "Data processed: 3808 in total data: 807\n",
      "Data processed: 3816 in total data: 807\n",
      "Data processed: 3824 in total data: 807\n",
      "Data processed: 3832 in total data: 807\n",
      "Data processed: 3840 in total data: 807\n",
      "Data processed: 3848 in total data: 807\n",
      "Data processed: 3856 in total data: 807\n",
      "Data processed: 3864 in total data: 807\n",
      "Data processed: 3872 in total data: 807\n",
      "Data processed: 3880 in total data: 807\n",
      "Data processed: 3888 in total data: 807\n",
      "Data processed: 3896 in total data: 807\n",
      "Data processed: 3904 in total data: 807\n",
      "Data processed: 3912 in total data: 807\n",
      "Data processed: 3920 in total data: 807\n",
      "Data processed: 3928 in total data: 807\n",
      "Data processed: 3936 in total data: 807\n",
      "Data processed: 3944 in total data: 807\n",
      "Data processed: 3952 in total data: 807\n",
      "Data processed: 3960 in total data: 807\n",
      "Data processed: 3968 in total data: 807\n",
      "Data processed: 3976 in total data: 807\n",
      "Data processed: 3984 in total data: 807\n",
      "Data processed: 3992 in total data: 807\n",
      "Data processed: 4000 in total data: 807\n",
      "Data processed: 4008 in total data: 807\n",
      "Data processed: 4016 in total data: 807\n",
      "Data processed: 4024 in total data: 807\n",
      "Data processed: 4032 in total data: 807\n",
      "Data processed: 4040 in total data: 807\n",
      "Data processed: 4048 in total data: 807\n",
      "Data processed: 4056 in total data: 807\n",
      "Data processed: 4064 in total data: 807\n",
      "Data processed: 4072 in total data: 807\n",
      "Data processed: 4080 in total data: 807\n",
      "Data processed: 4088 in total data: 807\n",
      "Data processed: 4096 in total data: 807\n",
      "Data processed: 4104 in total data: 807\n",
      "Data processed: 4112 in total data: 807\n",
      "Data processed: 4120 in total data: 807\n",
      "Data processed: 4128 in total data: 807\n",
      "Data processed: 4136 in total data: 807\n",
      "Data processed: 4144 in total data: 807\n",
      "Data processed: 4152 in total data: 807\n",
      "Data processed: 4160 in total data: 807\n",
      "Data processed: 4168 in total data: 807\n",
      "Data processed: 4176 in total data: 807\n",
      "Data processed: 4184 in total data: 807\n",
      "Data processed: 4192 in total data: 807\n",
      "Data processed: 4200 in total data: 807\n",
      "Data processed: 4208 in total data: 807\n",
      "Data processed: 4216 in total data: 807\n",
      "Data processed: 4224 in total data: 807\n",
      "Data processed: 4232 in total data: 807\n",
      "Data processed: 4240 in total data: 807\n",
      "Data processed: 4248 in total data: 807\n",
      "Data processed: 4256 in total data: 807\n",
      "Data processed: 4264 in total data: 807\n",
      "Data processed: 4272 in total data: 807\n",
      "Data processed: 4280 in total data: 807\n",
      "Data processed: 4288 in total data: 807\n",
      "Data processed: 4296 in total data: 807\n",
      "Data processed: 4304 in total data: 807\n",
      "Data processed: 4312 in total data: 807\n",
      "Data processed: 4320 in total data: 807\n",
      "Data processed: 4328 in total data: 807\n",
      "Data processed: 4336 in total data: 807\n",
      "Data processed: 4344 in total data: 807\n",
      "Data processed: 4352 in total data: 807\n",
      "Data processed: 4360 in total data: 807\n",
      "Data processed: 4368 in total data: 807\n",
      "Data processed: 4376 in total data: 807\n",
      "Data processed: 4384 in total data: 807\n",
      "Data processed: 4392 in total data: 807\n",
      "Data processed: 4400 in total data: 807\n",
      "Data processed: 4408 in total data: 807\n",
      "Data processed: 4416 in total data: 807\n",
      "Data processed: 4424 in total data: 807\n",
      "Data processed: 4432 in total data: 807\n",
      "Data processed: 4440 in total data: 807\n",
      "Data processed: 4448 in total data: 807\n",
      "Data processed: 4456 in total data: 807\n",
      "Data processed: 4464 in total data: 807\n",
      "Data processed: 4472 in total data: 807\n",
      "Data processed: 4480 in total data: 807\n",
      "Data processed: 4488 in total data: 807\n",
      "Data processed: 4496 in total data: 807\n",
      "Data processed: 4504 in total data: 807\n",
      "Data processed: 4512 in total data: 807\n",
      "Data processed: 4520 in total data: 807\n",
      "Data processed: 4528 in total data: 807\n",
      "Data processed: 4536 in total data: 807\n",
      "Data processed: 4544 in total data: 807\n",
      "Data processed: 4552 in total data: 807\n",
      "Data processed: 4560 in total data: 807\n",
      "Data processed: 4568 in total data: 807\n",
      "Data processed: 4576 in total data: 807\n",
      "Data processed: 4584 in total data: 807\n",
      "Data processed: 4592 in total data: 807\n",
      "Data processed: 4600 in total data: 807\n",
      "Data processed: 4608 in total data: 807\n",
      "Data processed: 4616 in total data: 807\n",
      "Data processed: 4624 in total data: 807\n",
      "Data processed: 4632 in total data: 807\n",
      "Data processed: 4640 in total data: 807\n",
      "Data processed: 4648 in total data: 807\n",
      "Data processed: 4656 in total data: 807\n",
      "Data processed: 4664 in total data: 807\n",
      "Data processed: 4672 in total data: 807\n",
      "Data processed: 4680 in total data: 807\n",
      "Data processed: 4688 in total data: 807\n",
      "Data processed: 4696 in total data: 807\n",
      "Data processed: 4704 in total data: 807\n",
      "Data processed: 4712 in total data: 807\n",
      "Data processed: 4720 in total data: 807\n",
      "Data processed: 4728 in total data: 807\n",
      "Data processed: 4736 in total data: 807\n",
      "Data processed: 4744 in total data: 807\n",
      "Data processed: 4752 in total data: 807\n",
      "Data processed: 4760 in total data: 807\n",
      "Data processed: 4768 in total data: 807\n",
      "Data processed: 4776 in total data: 807\n",
      "Data processed: 4784 in total data: 807\n",
      "Data processed: 4792 in total data: 807\n",
      "Data processed: 4800 in total data: 807\n",
      "Data processed: 4808 in total data: 807\n",
      "Data processed: 4816 in total data: 807\n",
      "Data processed: 4824 in total data: 807\n",
      "Data processed: 4832 in total data: 807\n",
      "Data processed: 4840 in total data: 807\n",
      "Data processed: 4848 in total data: 807\n",
      "Data processed: 4856 in total data: 807\n",
      "Data processed: 4864 in total data: 807\n",
      "Data processed: 4872 in total data: 807\n",
      "Data processed: 4880 in total data: 807\n",
      "Data processed: 4888 in total data: 807\n",
      "Data processed: 4896 in total data: 807\n",
      "Data processed: 4904 in total data: 807\n",
      "Data processed: 4912 in total data: 807\n",
      "Data processed: 4920 in total data: 807\n",
      "Data processed: 4928 in total data: 807\n",
      "Data processed: 4936 in total data: 807\n",
      "Data processed: 4944 in total data: 807\n",
      "Data processed: 4952 in total data: 807\n",
      "Data processed: 4960 in total data: 807\n",
      "Data processed: 4968 in total data: 807\n",
      "Data processed: 4976 in total data: 807\n",
      "Data processed: 4984 in total data: 807\n",
      "Data processed: 4992 in total data: 807\n",
      "Data processed: 5000 in total data: 807\n",
      "Data processed: 5008 in total data: 807\n",
      "Data processed: 5016 in total data: 807\n",
      "Data processed: 5024 in total data: 807\n",
      "Data processed: 5032 in total data: 807\n",
      "Data processed: 5040 in total data: 807\n",
      "Data processed: 5048 in total data: 807\n",
      "Data processed: 5056 in total data: 807\n",
      "Data processed: 5064 in total data: 807\n",
      "Data processed: 5072 in total data: 807\n",
      "Data processed: 5080 in total data: 807\n",
      "Data processed: 5088 in total data: 807\n",
      "Data processed: 5096 in total data: 807\n",
      "Data processed: 5104 in total data: 807\n",
      "Data processed: 5112 in total data: 807\n",
      "Data processed: 5120 in total data: 807\n",
      "Data processed: 5128 in total data: 807\n",
      "Data processed: 5136 in total data: 807\n",
      "Data processed: 5144 in total data: 807\n",
      "Data processed: 5152 in total data: 807\n",
      "Data processed: 5160 in total data: 807\n",
      "Data processed: 5168 in total data: 807\n",
      "Data processed: 5176 in total data: 807\n",
      "Data processed: 5184 in total data: 807\n",
      "Data processed: 5192 in total data: 807\n",
      "Data processed: 5200 in total data: 807\n",
      "Data processed: 5208 in total data: 807\n",
      "Data processed: 5216 in total data: 807\n",
      "Data processed: 5224 in total data: 807\n",
      "Data processed: 5232 in total data: 807\n",
      "Data processed: 5240 in total data: 807\n",
      "Data processed: 5248 in total data: 807\n",
      "Data processed: 5256 in total data: 807\n",
      "Data processed: 5264 in total data: 807\n",
      "Data processed: 5272 in total data: 807\n",
      "Data processed: 5280 in total data: 807\n",
      "Data processed: 5288 in total data: 807\n",
      "Data processed: 5296 in total data: 807\n",
      "Data processed: 5304 in total data: 807\n",
      "Data processed: 5312 in total data: 807\n",
      "Data processed: 5320 in total data: 807\n",
      "Data processed: 5328 in total data: 807\n",
      "Data processed: 5336 in total data: 807\n",
      "Data processed: 5344 in total data: 807\n",
      "Data processed: 5352 in total data: 807\n",
      "Data processed: 5360 in total data: 807\n",
      "Data processed: 5368 in total data: 807\n",
      "Data processed: 5376 in total data: 807\n",
      "Data processed: 5384 in total data: 807\n",
      "Data processed: 5392 in total data: 807\n",
      "Data processed: 5400 in total data: 807\n",
      "Data processed: 5408 in total data: 807\n",
      "Data processed: 5416 in total data: 807\n",
      "Data processed: 5424 in total data: 807\n",
      "Data processed: 5432 in total data: 807\n",
      "Data processed: 5440 in total data: 807\n",
      "Data processed: 5448 in total data: 807\n",
      "Data processed: 5456 in total data: 807\n",
      "Data processed: 5464 in total data: 807\n",
      "Data processed: 5472 in total data: 807\n",
      "Data processed: 5480 in total data: 807\n",
      "Data processed: 5488 in total data: 807\n",
      "Data processed: 5496 in total data: 807\n",
      "Data processed: 5504 in total data: 807\n",
      "Data processed: 5512 in total data: 807\n",
      "Data processed: 5520 in total data: 807\n",
      "Data processed: 5528 in total data: 807\n",
      "Data processed: 5536 in total data: 807\n",
      "Data processed: 5544 in total data: 807\n",
      "Data processed: 5552 in total data: 807\n",
      "Data processed: 5560 in total data: 807\n",
      "Data processed: 5568 in total data: 807\n",
      "Data processed: 5576 in total data: 807\n",
      "Data processed: 5584 in total data: 807\n",
      "Data processed: 5592 in total data: 807\n",
      "Data processed: 5600 in total data: 807\n",
      "Data processed: 5608 in total data: 807\n",
      "Data processed: 5616 in total data: 807\n",
      "Data processed: 5624 in total data: 807\n",
      "Data processed: 5632 in total data: 807\n",
      "Data processed: 5640 in total data: 807\n",
      "Data processed: 5648 in total data: 807\n",
      "Data processed: 5656 in total data: 807\n",
      "Data processed: 5664 in total data: 807\n",
      "Data processed: 5672 in total data: 807\n",
      "Data processed: 5680 in total data: 807\n",
      "Data processed: 5688 in total data: 807\n",
      "Data processed: 5696 in total data: 807\n",
      "Data processed: 5704 in total data: 807\n",
      "Data processed: 5712 in total data: 807\n",
      "Data processed: 5720 in total data: 807\n",
      "Data processed: 5728 in total data: 807\n",
      "Data processed: 5736 in total data: 807\n",
      "Data processed: 5744 in total data: 807\n",
      "Data processed: 5752 in total data: 807\n",
      "Data processed: 5760 in total data: 807\n",
      "Data processed: 5768 in total data: 807\n",
      "Data processed: 5776 in total data: 807\n",
      "Data processed: 5784 in total data: 807\n",
      "Data processed: 5792 in total data: 807\n",
      "Data processed: 5800 in total data: 807\n",
      "Data processed: 5808 in total data: 807\n",
      "Data processed: 5816 in total data: 807\n",
      "Data processed: 5824 in total data: 807\n",
      "Data processed: 5832 in total data: 807\n",
      "Data processed: 5840 in total data: 807\n",
      "Data processed: 5848 in total data: 807\n",
      "Data processed: 5856 in total data: 807\n",
      "Data processed: 5864 in total data: 807\n",
      "Data processed: 5872 in total data: 807\n",
      "Data processed: 5880 in total data: 807\n",
      "Data processed: 5888 in total data: 807\n",
      "Data processed: 5896 in total data: 807\n",
      "Data processed: 5904 in total data: 807\n",
      "Data processed: 5912 in total data: 807\n",
      "Data processed: 5920 in total data: 807\n",
      "Data processed: 5928 in total data: 807\n",
      "Data processed: 5936 in total data: 807\n",
      "Data processed: 5944 in total data: 807\n",
      "Data processed: 5952 in total data: 807\n",
      "Data processed: 5960 in total data: 807\n",
      "Data processed: 5968 in total data: 807\n",
      "Data processed: 5976 in total data: 807\n",
      "Data processed: 5984 in total data: 807\n",
      "Data processed: 5992 in total data: 807\n",
      "Data processed: 6000 in total data: 807\n",
      "Data processed: 6008 in total data: 807\n",
      "Data processed: 6016 in total data: 807\n",
      "Data processed: 6024 in total data: 807\n",
      "Data processed: 6032 in total data: 807\n",
      "Data processed: 6040 in total data: 807\n",
      "Data processed: 6048 in total data: 807\n",
      "Data processed: 6056 in total data: 807\n",
      "Data processed: 6064 in total data: 807\n",
      "Data processed: 6072 in total data: 807\n",
      "Data processed: 6080 in total data: 807\n",
      "Data processed: 6088 in total data: 807\n",
      "Data processed: 6096 in total data: 807\n",
      "Data processed: 6104 in total data: 807\n",
      "Data processed: 6112 in total data: 807\n",
      "Data processed: 6120 in total data: 807\n",
      "Data processed: 6128 in total data: 807\n",
      "Data processed: 6136 in total data: 807\n",
      "Data processed: 6144 in total data: 807\n",
      "Data processed: 6152 in total data: 807\n",
      "Data processed: 6160 in total data: 807\n",
      "Data processed: 6168 in total data: 807\n",
      "Data processed: 6176 in total data: 807\n",
      "Data processed: 6184 in total data: 807\n",
      "Data processed: 6192 in total data: 807\n",
      "Data processed: 6200 in total data: 807\n",
      "Data processed: 6208 in total data: 807\n",
      "Data processed: 6216 in total data: 807\n",
      "Data processed: 6224 in total data: 807\n",
      "Data processed: 6232 in total data: 807\n",
      "Data processed: 6240 in total data: 807\n",
      "Data processed: 6248 in total data: 807\n",
      "Data processed: 6256 in total data: 807\n",
      "Data processed: 6264 in total data: 807\n",
      "Data processed: 6272 in total data: 807\n",
      "Data processed: 6280 in total data: 807\n",
      "Data processed: 6288 in total data: 807\n",
      "Data processed: 6296 in total data: 807\n",
      "Data processed: 6304 in total data: 807\n",
      "Data processed: 6312 in total data: 807\n",
      "Data processed: 6320 in total data: 807\n",
      "Data processed: 6328 in total data: 807\n",
      "Data processed: 6336 in total data: 807\n",
      "Data processed: 6344 in total data: 807\n",
      "Data processed: 6352 in total data: 807\n",
      "Data processed: 6360 in total data: 807\n",
      "Data processed: 6368 in total data: 807\n",
      "Data processed: 6376 in total data: 807\n",
      "Data processed: 6384 in total data: 807\n",
      "Data processed: 6392 in total data: 807\n",
      "Data processed: 6400 in total data: 807\n",
      "Data processed: 6408 in total data: 807\n",
      "Data processed: 6416 in total data: 807\n",
      "Data processed: 6424 in total data: 807\n",
      "Data processed: 6432 in total data: 807\n",
      "Data processed: 6440 in total data: 807\n",
      "Data processed: 6448 in total data: 807\n",
      "Data processed: 6450 in total data: 807\n",
      "Epoch 1: Loss=0.0000, Train Acc=4.1247, Val Acc=4.0670\n",
      "Data processed: 8 in total data: 807\n",
      "Data processed: 16 in total data: 807\n",
      "Data processed: 24 in total data: 807\n",
      "Data processed: 32 in total data: 807\n",
      "Data processed: 40 in total data: 807\n",
      "Data processed: 48 in total data: 807\n",
      "Data processed: 56 in total data: 807\n",
      "Data processed: 64 in total data: 807\n",
      "Data processed: 72 in total data: 807\n",
      "Data processed: 80 in total data: 807\n",
      "Data processed: 88 in total data: 807\n",
      "Data processed: 96 in total data: 807\n",
      "Data processed: 104 in total data: 807\n",
      "Data processed: 112 in total data: 807\n",
      "Data processed: 120 in total data: 807\n",
      "Data processed: 128 in total data: 807\n",
      "Data processed: 136 in total data: 807\n",
      "Data processed: 144 in total data: 807\n",
      "Data processed: 152 in total data: 807\n",
      "Data processed: 160 in total data: 807\n",
      "Data processed: 168 in total data: 807\n",
      "Data processed: 176 in total data: 807\n",
      "Data processed: 184 in total data: 807\n",
      "Data processed: 192 in total data: 807\n",
      "Data processed: 200 in total data: 807\n",
      "Data processed: 208 in total data: 807\n",
      "Data processed: 216 in total data: 807\n",
      "Data processed: 224 in total data: 807\n",
      "Data processed: 232 in total data: 807\n",
      "Data processed: 240 in total data: 807\n",
      "Data processed: 248 in total data: 807\n",
      "Data processed: 256 in total data: 807\n",
      "Data processed: 264 in total data: 807\n",
      "Data processed: 272 in total data: 807\n",
      "Data processed: 280 in total data: 807\n",
      "Data processed: 288 in total data: 807\n",
      "Data processed: 296 in total data: 807\n",
      "Data processed: 304 in total data: 807\n",
      "Data processed: 312 in total data: 807\n",
      "Data processed: 320 in total data: 807\n",
      "Data processed: 328 in total data: 807\n",
      "Data processed: 336 in total data: 807\n",
      "Data processed: 344 in total data: 807\n",
      "Data processed: 352 in total data: 807\n",
      "Data processed: 360 in total data: 807\n",
      "Data processed: 368 in total data: 807\n",
      "Data processed: 376 in total data: 807\n",
      "Data processed: 384 in total data: 807\n",
      "Data processed: 392 in total data: 807\n",
      "Data processed: 400 in total data: 807\n",
      "Data processed: 408 in total data: 807\n",
      "Data processed: 416 in total data: 807\n",
      "Data processed: 424 in total data: 807\n",
      "Data processed: 432 in total data: 807\n",
      "Data processed: 440 in total data: 807\n",
      "Data processed: 448 in total data: 807\n",
      "Data processed: 456 in total data: 807\n",
      "Data processed: 464 in total data: 807\n",
      "Data processed: 472 in total data: 807\n",
      "Data processed: 480 in total data: 807\n",
      "Data processed: 488 in total data: 807\n",
      "Data processed: 496 in total data: 807\n",
      "Data processed: 504 in total data: 807\n",
      "Data processed: 512 in total data: 807\n",
      "Data processed: 520 in total data: 807\n",
      "Data processed: 528 in total data: 807\n",
      "Data processed: 536 in total data: 807\n",
      "Data processed: 544 in total data: 807\n",
      "Data processed: 552 in total data: 807\n",
      "Data processed: 560 in total data: 807\n",
      "Data processed: 568 in total data: 807\n",
      "Data processed: 576 in total data: 807\n",
      "Data processed: 584 in total data: 807\n",
      "Data processed: 592 in total data: 807\n",
      "Data processed: 600 in total data: 807\n",
      "Data processed: 608 in total data: 807\n",
      "Data processed: 616 in total data: 807\n",
      "Data processed: 624 in total data: 807\n",
      "Data processed: 632 in total data: 807\n",
      "Data processed: 640 in total data: 807\n",
      "Data processed: 648 in total data: 807\n",
      "Data processed: 656 in total data: 807\n",
      "Data processed: 664 in total data: 807\n",
      "Data processed: 672 in total data: 807\n",
      "Data processed: 680 in total data: 807\n",
      "Data processed: 688 in total data: 807\n",
      "Data processed: 696 in total data: 807\n",
      "Data processed: 704 in total data: 807\n",
      "Data processed: 712 in total data: 807\n",
      "Data processed: 720 in total data: 807\n",
      "Data processed: 728 in total data: 807\n",
      "Data processed: 736 in total data: 807\n",
      "Data processed: 744 in total data: 807\n",
      "Data processed: 752 in total data: 807\n",
      "Data processed: 760 in total data: 807\n",
      "Data processed: 768 in total data: 807\n",
      "Data processed: 776 in total data: 807\n",
      "Data processed: 784 in total data: 807\n",
      "Data processed: 792 in total data: 807\n",
      "Data processed: 800 in total data: 807\n",
      "Data processed: 808 in total data: 807\n",
      "Data processed: 816 in total data: 807\n",
      "Data processed: 824 in total data: 807\n",
      "Data processed: 832 in total data: 807\n",
      "Data processed: 840 in total data: 807\n",
      "Data processed: 848 in total data: 807\n",
      "Data processed: 856 in total data: 807\n",
      "Data processed: 864 in total data: 807\n",
      "Data processed: 872 in total data: 807\n",
      "Data processed: 880 in total data: 807\n",
      "Data processed: 888 in total data: 807\n",
      "Data processed: 896 in total data: 807\n",
      "Data processed: 904 in total data: 807\n",
      "Data processed: 912 in total data: 807\n",
      "Data processed: 920 in total data: 807\n",
      "Data processed: 928 in total data: 807\n",
      "Data processed: 936 in total data: 807\n",
      "Data processed: 944 in total data: 807\n",
      "Data processed: 952 in total data: 807\n",
      "Data processed: 960 in total data: 807\n",
      "Data processed: 968 in total data: 807\n",
      "Data processed: 976 in total data: 807\n",
      "Data processed: 984 in total data: 807\n",
      "Data processed: 992 in total data: 807\n",
      "Data processed: 1000 in total data: 807\n",
      "Data processed: 1008 in total data: 807\n",
      "Data processed: 1016 in total data: 807\n",
      "Data processed: 1024 in total data: 807\n",
      "Data processed: 1032 in total data: 807\n",
      "Data processed: 1040 in total data: 807\n",
      "Data processed: 1048 in total data: 807\n",
      "Data processed: 1056 in total data: 807\n",
      "Data processed: 1064 in total data: 807\n",
      "Data processed: 1072 in total data: 807\n",
      "Data processed: 1080 in total data: 807\n",
      "Data processed: 1088 in total data: 807\n",
      "Data processed: 1096 in total data: 807\n",
      "Data processed: 1104 in total data: 807\n",
      "Data processed: 1112 in total data: 807\n",
      "Data processed: 1120 in total data: 807\n",
      "Data processed: 1128 in total data: 807\n",
      "Data processed: 1136 in total data: 807\n",
      "Data processed: 1144 in total data: 807\n",
      "Data processed: 1152 in total data: 807\n",
      "Data processed: 1160 in total data: 807\n",
      "Data processed: 1168 in total data: 807\n",
      "Data processed: 1176 in total data: 807\n",
      "Data processed: 1184 in total data: 807\n",
      "Data processed: 1192 in total data: 807\n",
      "Data processed: 1200 in total data: 807\n",
      "Data processed: 1208 in total data: 807\n",
      "Data processed: 1216 in total data: 807\n",
      "Data processed: 1224 in total data: 807\n",
      "Data processed: 1232 in total data: 807\n",
      "Data processed: 1240 in total data: 807\n",
      "Data processed: 1248 in total data: 807\n",
      "Data processed: 1256 in total data: 807\n",
      "Data processed: 1264 in total data: 807\n",
      "Data processed: 1272 in total data: 807\n",
      "Data processed: 1280 in total data: 807\n",
      "Data processed: 1288 in total data: 807\n",
      "Data processed: 1296 in total data: 807\n",
      "Data processed: 1304 in total data: 807\n",
      "Data processed: 1312 in total data: 807\n",
      "Data processed: 1320 in total data: 807\n",
      "Data processed: 1328 in total data: 807\n",
      "Data processed: 1336 in total data: 807\n",
      "Data processed: 1344 in total data: 807\n",
      "Data processed: 1352 in total data: 807\n",
      "Data processed: 1360 in total data: 807\n",
      "Data processed: 1368 in total data: 807\n",
      "Data processed: 1376 in total data: 807\n",
      "Data processed: 1384 in total data: 807\n",
      "Data processed: 1392 in total data: 807\n",
      "Data processed: 1400 in total data: 807\n",
      "Data processed: 1408 in total data: 807\n",
      "Data processed: 1416 in total data: 807\n",
      "Data processed: 1424 in total data: 807\n",
      "Data processed: 1432 in total data: 807\n",
      "Data processed: 1440 in total data: 807\n",
      "Data processed: 1448 in total data: 807\n",
      "Data processed: 1456 in total data: 807\n",
      "Data processed: 1464 in total data: 807\n",
      "Data processed: 1472 in total data: 807\n",
      "Data processed: 1480 in total data: 807\n",
      "Data processed: 1488 in total data: 807\n",
      "Data processed: 1496 in total data: 807\n",
      "Data processed: 1504 in total data: 807\n",
      "Data processed: 1512 in total data: 807\n",
      "Data processed: 1520 in total data: 807\n",
      "Data processed: 1528 in total data: 807\n",
      "Data processed: 1536 in total data: 807\n",
      "Data processed: 1544 in total data: 807\n",
      "Data processed: 1552 in total data: 807\n",
      "Data processed: 1560 in total data: 807\n",
      "Data processed: 1568 in total data: 807\n",
      "Data processed: 1576 in total data: 807\n",
      "Data processed: 1584 in total data: 807\n",
      "Data processed: 1592 in total data: 807\n",
      "Data processed: 1600 in total data: 807\n",
      "Data processed: 1608 in total data: 807\n",
      "Data processed: 1616 in total data: 807\n",
      "Data processed: 1624 in total data: 807\n",
      "Data processed: 1632 in total data: 807\n",
      "Data processed: 1640 in total data: 807\n",
      "Data processed: 1648 in total data: 807\n",
      "Data processed: 1656 in total data: 807\n",
      "Data processed: 1664 in total data: 807\n",
      "Data processed: 1672 in total data: 807\n",
      "Data processed: 1680 in total data: 807\n",
      "Data processed: 1688 in total data: 807\n",
      "Data processed: 1696 in total data: 807\n",
      "Data processed: 1704 in total data: 807\n",
      "Data processed: 1712 in total data: 807\n",
      "Data processed: 1720 in total data: 807\n",
      "Data processed: 1728 in total data: 807\n",
      "Data processed: 1736 in total data: 807\n",
      "Data processed: 1744 in total data: 807\n",
      "Data processed: 1752 in total data: 807\n",
      "Data processed: 1760 in total data: 807\n",
      "Data processed: 1768 in total data: 807\n",
      "Data processed: 1776 in total data: 807\n",
      "Data processed: 1784 in total data: 807\n",
      "Data processed: 1792 in total data: 807\n",
      "Data processed: 1800 in total data: 807\n",
      "Data processed: 1808 in total data: 807\n",
      "Data processed: 1816 in total data: 807\n",
      "Data processed: 1824 in total data: 807\n",
      "Data processed: 1832 in total data: 807\n",
      "Data processed: 1840 in total data: 807\n",
      "Data processed: 1848 in total data: 807\n",
      "Data processed: 1856 in total data: 807\n",
      "Data processed: 1864 in total data: 807\n",
      "Data processed: 1872 in total data: 807\n",
      "Data processed: 1880 in total data: 807\n",
      "Data processed: 1888 in total data: 807\n",
      "Data processed: 1896 in total data: 807\n",
      "Data processed: 1904 in total data: 807\n",
      "Data processed: 1912 in total data: 807\n",
      "Data processed: 1920 in total data: 807\n",
      "Data processed: 1928 in total data: 807\n",
      "Data processed: 1936 in total data: 807\n",
      "Data processed: 1944 in total data: 807\n",
      "Data processed: 1952 in total data: 807\n",
      "Data processed: 1960 in total data: 807\n",
      "Data processed: 1968 in total data: 807\n",
      "Data processed: 1976 in total data: 807\n",
      "Data processed: 1984 in total data: 807\n",
      "Data processed: 1992 in total data: 807\n",
      "Data processed: 2000 in total data: 807\n",
      "Data processed: 2008 in total data: 807\n",
      "Data processed: 2016 in total data: 807\n",
      "Data processed: 2024 in total data: 807\n",
      "Data processed: 2032 in total data: 807\n",
      "Data processed: 2040 in total data: 807\n",
      "Data processed: 2048 in total data: 807\n",
      "Data processed: 2056 in total data: 807\n",
      "Data processed: 2064 in total data: 807\n",
      "Data processed: 2072 in total data: 807\n",
      "Data processed: 2080 in total data: 807\n",
      "Data processed: 2088 in total data: 807\n",
      "Data processed: 2096 in total data: 807\n",
      "Data processed: 2104 in total data: 807\n",
      "Data processed: 2112 in total data: 807\n",
      "Data processed: 2120 in total data: 807\n",
      "Data processed: 2128 in total data: 807\n",
      "Data processed: 2136 in total data: 807\n",
      "Data processed: 2144 in total data: 807\n",
      "Data processed: 2152 in total data: 807\n",
      "Data processed: 2160 in total data: 807\n",
      "Data processed: 2168 in total data: 807\n",
      "Data processed: 2176 in total data: 807\n",
      "Data processed: 2184 in total data: 807\n",
      "Data processed: 2192 in total data: 807\n",
      "Data processed: 2200 in total data: 807\n",
      "Data processed: 2208 in total data: 807\n",
      "Data processed: 2216 in total data: 807\n",
      "Data processed: 2224 in total data: 807\n",
      "Data processed: 2232 in total data: 807\n",
      "Data processed: 2240 in total data: 807\n",
      "Data processed: 2248 in total data: 807\n",
      "Data processed: 2256 in total data: 807\n",
      "Data processed: 2264 in total data: 807\n",
      "Data processed: 2272 in total data: 807\n",
      "Data processed: 2280 in total data: 807\n",
      "Data processed: 2288 in total data: 807\n",
      "Data processed: 2296 in total data: 807\n",
      "Data processed: 2304 in total data: 807\n",
      "Data processed: 2312 in total data: 807\n",
      "Data processed: 2320 in total data: 807\n",
      "Data processed: 2328 in total data: 807\n",
      "Data processed: 2336 in total data: 807\n",
      "Data processed: 2344 in total data: 807\n",
      "Data processed: 2352 in total data: 807\n",
      "Data processed: 2360 in total data: 807\n",
      "Data processed: 2368 in total data: 807\n",
      "Data processed: 2376 in total data: 807\n",
      "Data processed: 2384 in total data: 807\n",
      "Data processed: 2392 in total data: 807\n",
      "Data processed: 2400 in total data: 807\n",
      "Data processed: 2408 in total data: 807\n",
      "Data processed: 2416 in total data: 807\n",
      "Data processed: 2424 in total data: 807\n",
      "Data processed: 2432 in total data: 807\n",
      "Data processed: 2440 in total data: 807\n",
      "Data processed: 2448 in total data: 807\n",
      "Data processed: 2456 in total data: 807\n",
      "Data processed: 2464 in total data: 807\n",
      "Data processed: 2472 in total data: 807\n",
      "Data processed: 2480 in total data: 807\n",
      "Data processed: 2488 in total data: 807\n",
      "Data processed: 2496 in total data: 807\n",
      "Data processed: 2504 in total data: 807\n",
      "Data processed: 2512 in total data: 807\n",
      "Data processed: 2520 in total data: 807\n",
      "Data processed: 2528 in total data: 807\n",
      "Data processed: 2536 in total data: 807\n",
      "Data processed: 2544 in total data: 807\n",
      "Data processed: 2552 in total data: 807\n",
      "Data processed: 2560 in total data: 807\n",
      "Data processed: 2568 in total data: 807\n",
      "Data processed: 2576 in total data: 807\n",
      "Data processed: 2584 in total data: 807\n",
      "Data processed: 2592 in total data: 807\n",
      "Data processed: 2600 in total data: 807\n",
      "Data processed: 2608 in total data: 807\n",
      "Data processed: 2616 in total data: 807\n",
      "Data processed: 2624 in total data: 807\n",
      "Data processed: 2632 in total data: 807\n",
      "Data processed: 2640 in total data: 807\n",
      "Data processed: 2648 in total data: 807\n",
      "Data processed: 2656 in total data: 807\n",
      "Data processed: 2664 in total data: 807\n",
      "Data processed: 2672 in total data: 807\n",
      "Data processed: 2680 in total data: 807\n",
      "Data processed: 2688 in total data: 807\n",
      "Data processed: 2696 in total data: 807\n",
      "Data processed: 2704 in total data: 807\n",
      "Data processed: 2712 in total data: 807\n",
      "Data processed: 2720 in total data: 807\n",
      "Data processed: 2728 in total data: 807\n",
      "Data processed: 2736 in total data: 807\n",
      "Data processed: 2744 in total data: 807\n",
      "Data processed: 2752 in total data: 807\n",
      "Data processed: 2760 in total data: 807\n",
      "Data processed: 2768 in total data: 807\n",
      "Data processed: 2776 in total data: 807\n",
      "Data processed: 2784 in total data: 807\n",
      "Data processed: 2792 in total data: 807\n",
      "Data processed: 2800 in total data: 807\n",
      "Data processed: 2808 in total data: 807\n",
      "Data processed: 2816 in total data: 807\n",
      "Data processed: 2824 in total data: 807\n",
      "Data processed: 2832 in total data: 807\n",
      "Data processed: 2840 in total data: 807\n",
      "Data processed: 2848 in total data: 807\n",
      "Data processed: 2856 in total data: 807\n",
      "Data processed: 2864 in total data: 807\n",
      "Data processed: 2872 in total data: 807\n",
      "Data processed: 2880 in total data: 807\n",
      "Data processed: 2888 in total data: 807\n",
      "Data processed: 2896 in total data: 807\n",
      "Data processed: 2904 in total data: 807\n",
      "Data processed: 2912 in total data: 807\n",
      "Data processed: 2920 in total data: 807\n",
      "Data processed: 2928 in total data: 807\n",
      "Data processed: 2936 in total data: 807\n",
      "Data processed: 2944 in total data: 807\n",
      "Data processed: 2952 in total data: 807\n",
      "Data processed: 2960 in total data: 807\n",
      "Data processed: 2968 in total data: 807\n",
      "Data processed: 2976 in total data: 807\n",
      "Data processed: 2984 in total data: 807\n",
      "Data processed: 2992 in total data: 807\n",
      "Data processed: 3000 in total data: 807\n",
      "Data processed: 3008 in total data: 807\n",
      "Data processed: 3016 in total data: 807\n",
      "Data processed: 3024 in total data: 807\n",
      "Data processed: 3032 in total data: 807\n",
      "Data processed: 3040 in total data: 807\n",
      "Data processed: 3048 in total data: 807\n",
      "Data processed: 3056 in total data: 807\n",
      "Data processed: 3064 in total data: 807\n",
      "Data processed: 3072 in total data: 807\n",
      "Data processed: 3080 in total data: 807\n",
      "Data processed: 3088 in total data: 807\n",
      "Data processed: 3096 in total data: 807\n",
      "Data processed: 3104 in total data: 807\n",
      "Data processed: 3112 in total data: 807\n",
      "Data processed: 3120 in total data: 807\n",
      "Data processed: 3128 in total data: 807\n",
      "Data processed: 3136 in total data: 807\n",
      "Data processed: 3144 in total data: 807\n",
      "Data processed: 3152 in total data: 807\n",
      "Data processed: 3160 in total data: 807\n",
      "Data processed: 3168 in total data: 807\n",
      "Data processed: 3176 in total data: 807\n",
      "Data processed: 3184 in total data: 807\n",
      "Data processed: 3192 in total data: 807\n",
      "Data processed: 3200 in total data: 807\n",
      "Data processed: 3208 in total data: 807\n",
      "Data processed: 3216 in total data: 807\n",
      "Data processed: 3224 in total data: 807\n",
      "Data processed: 3232 in total data: 807\n",
      "Data processed: 3240 in total data: 807\n",
      "Data processed: 3248 in total data: 807\n",
      "Data processed: 3256 in total data: 807\n",
      "Data processed: 3264 in total data: 807\n",
      "Data processed: 3272 in total data: 807\n",
      "Data processed: 3280 in total data: 807\n",
      "Data processed: 3288 in total data: 807\n",
      "Data processed: 3296 in total data: 807\n",
      "Data processed: 3304 in total data: 807\n",
      "Data processed: 3312 in total data: 807\n",
      "Data processed: 3320 in total data: 807\n",
      "Data processed: 3328 in total data: 807\n",
      "Data processed: 3336 in total data: 807\n",
      "Data processed: 3344 in total data: 807\n",
      "Data processed: 3352 in total data: 807\n",
      "Data processed: 3360 in total data: 807\n",
      "Data processed: 3368 in total data: 807\n",
      "Data processed: 3376 in total data: 807\n",
      "Data processed: 3384 in total data: 807\n",
      "Data processed: 3392 in total data: 807\n",
      "Data processed: 3400 in total data: 807\n",
      "Data processed: 3408 in total data: 807\n",
      "Data processed: 3416 in total data: 807\n",
      "Data processed: 3424 in total data: 807\n",
      "Data processed: 3432 in total data: 807\n",
      "Data processed: 3440 in total data: 807\n",
      "Data processed: 3448 in total data: 807\n",
      "Data processed: 3456 in total data: 807\n",
      "Data processed: 3464 in total data: 807\n",
      "Data processed: 3472 in total data: 807\n",
      "Data processed: 3480 in total data: 807\n",
      "Data processed: 3488 in total data: 807\n",
      "Data processed: 3496 in total data: 807\n",
      "Data processed: 3504 in total data: 807\n",
      "Data processed: 3512 in total data: 807\n",
      "Data processed: 3520 in total data: 807\n",
      "Data processed: 3528 in total data: 807\n",
      "Data processed: 3536 in total data: 807\n",
      "Data processed: 3544 in total data: 807\n",
      "Data processed: 3552 in total data: 807\n",
      "Data processed: 3560 in total data: 807\n",
      "Data processed: 3568 in total data: 807\n",
      "Data processed: 3576 in total data: 807\n",
      "Data processed: 3584 in total data: 807\n",
      "Data processed: 3592 in total data: 807\n",
      "Data processed: 3600 in total data: 807\n",
      "Data processed: 3608 in total data: 807\n",
      "Data processed: 3616 in total data: 807\n",
      "Data processed: 3624 in total data: 807\n",
      "Data processed: 3632 in total data: 807\n",
      "Data processed: 3640 in total data: 807\n",
      "Data processed: 3648 in total data: 807\n",
      "Data processed: 3656 in total data: 807\n",
      "Data processed: 3664 in total data: 807\n",
      "Data processed: 3672 in total data: 807\n",
      "Data processed: 3680 in total data: 807\n",
      "Data processed: 3688 in total data: 807\n",
      "Data processed: 3696 in total data: 807\n",
      "Data processed: 3704 in total data: 807\n",
      "Data processed: 3712 in total data: 807\n",
      "Data processed: 3720 in total data: 807\n",
      "Data processed: 3728 in total data: 807\n",
      "Data processed: 3736 in total data: 807\n",
      "Data processed: 3744 in total data: 807\n",
      "Data processed: 3752 in total data: 807\n",
      "Data processed: 3760 in total data: 807\n",
      "Data processed: 3768 in total data: 807\n",
      "Data processed: 3776 in total data: 807\n",
      "Data processed: 3784 in total data: 807\n",
      "Data processed: 3792 in total data: 807\n",
      "Data processed: 3800 in total data: 807\n",
      "Data processed: 3808 in total data: 807\n",
      "Data processed: 3816 in total data: 807\n",
      "Data processed: 3824 in total data: 807\n",
      "Data processed: 3832 in total data: 807\n",
      "Data processed: 3840 in total data: 807\n",
      "Data processed: 3848 in total data: 807\n",
      "Data processed: 3856 in total data: 807\n",
      "Data processed: 3864 in total data: 807\n",
      "Data processed: 3872 in total data: 807\n",
      "Data processed: 3880 in total data: 807\n",
      "Data processed: 3888 in total data: 807\n",
      "Data processed: 3896 in total data: 807\n",
      "Data processed: 3904 in total data: 807\n",
      "Data processed: 3912 in total data: 807\n",
      "Data processed: 3920 in total data: 807\n",
      "Data processed: 3928 in total data: 807\n",
      "Data processed: 3936 in total data: 807\n",
      "Data processed: 3944 in total data: 807\n",
      "Data processed: 3952 in total data: 807\n",
      "Data processed: 3960 in total data: 807\n",
      "Data processed: 3968 in total data: 807\n",
      "Data processed: 3976 in total data: 807\n",
      "Data processed: 3984 in total data: 807\n",
      "Data processed: 3992 in total data: 807\n",
      "Data processed: 4000 in total data: 807\n",
      "Data processed: 4008 in total data: 807\n",
      "Data processed: 4016 in total data: 807\n",
      "Data processed: 4024 in total data: 807\n",
      "Data processed: 4032 in total data: 807\n",
      "Data processed: 4040 in total data: 807\n",
      "Data processed: 4048 in total data: 807\n",
      "Data processed: 4056 in total data: 807\n",
      "Data processed: 4064 in total data: 807\n",
      "Data processed: 4072 in total data: 807\n",
      "Data processed: 4080 in total data: 807\n",
      "Data processed: 4088 in total data: 807\n",
      "Data processed: 4096 in total data: 807\n",
      "Data processed: 4104 in total data: 807\n",
      "Data processed: 4112 in total data: 807\n",
      "Data processed: 4120 in total data: 807\n",
      "Data processed: 4128 in total data: 807\n",
      "Data processed: 4136 in total data: 807\n",
      "Data processed: 4144 in total data: 807\n",
      "Data processed: 4152 in total data: 807\n",
      "Data processed: 4160 in total data: 807\n",
      "Data processed: 4168 in total data: 807\n",
      "Data processed: 4176 in total data: 807\n",
      "Data processed: 4184 in total data: 807\n",
      "Data processed: 4192 in total data: 807\n",
      "Data processed: 4200 in total data: 807\n",
      "Data processed: 4208 in total data: 807\n",
      "Data processed: 4216 in total data: 807\n",
      "Data processed: 4224 in total data: 807\n",
      "Data processed: 4232 in total data: 807\n",
      "Data processed: 4240 in total data: 807\n",
      "Data processed: 4248 in total data: 807\n",
      "Data processed: 4256 in total data: 807\n",
      "Data processed: 4264 in total data: 807\n",
      "Data processed: 4272 in total data: 807\n",
      "Data processed: 4280 in total data: 807\n",
      "Data processed: 4288 in total data: 807\n",
      "Data processed: 4296 in total data: 807\n",
      "Data processed: 4304 in total data: 807\n",
      "Data processed: 4312 in total data: 807\n",
      "Data processed: 4320 in total data: 807\n",
      "Data processed: 4328 in total data: 807\n",
      "Data processed: 4336 in total data: 807\n",
      "Data processed: 4344 in total data: 807\n",
      "Data processed: 4352 in total data: 807\n",
      "Data processed: 4360 in total data: 807\n",
      "Data processed: 4368 in total data: 807\n",
      "Data processed: 4376 in total data: 807\n",
      "Data processed: 4384 in total data: 807\n",
      "Data processed: 4392 in total data: 807\n",
      "Data processed: 4400 in total data: 807\n",
      "Data processed: 4408 in total data: 807\n",
      "Data processed: 4416 in total data: 807\n",
      "Data processed: 4424 in total data: 807\n",
      "Data processed: 4432 in total data: 807\n",
      "Data processed: 4440 in total data: 807\n",
      "Data processed: 4448 in total data: 807\n",
      "Data processed: 4456 in total data: 807\n",
      "Data processed: 4464 in total data: 807\n",
      "Data processed: 4472 in total data: 807\n",
      "Data processed: 4480 in total data: 807\n",
      "Data processed: 4488 in total data: 807\n",
      "Data processed: 4496 in total data: 807\n",
      "Data processed: 4504 in total data: 807\n",
      "Data processed: 4512 in total data: 807\n",
      "Data processed: 4520 in total data: 807\n",
      "Data processed: 4528 in total data: 807\n",
      "Data processed: 4536 in total data: 807\n",
      "Data processed: 4544 in total data: 807\n",
      "Data processed: 4552 in total data: 807\n",
      "Data processed: 4560 in total data: 807\n",
      "Data processed: 4568 in total data: 807\n",
      "Data processed: 4576 in total data: 807\n",
      "Data processed: 4584 in total data: 807\n",
      "Data processed: 4592 in total data: 807\n",
      "Data processed: 4600 in total data: 807\n",
      "Data processed: 4608 in total data: 807\n",
      "Data processed: 4616 in total data: 807\n",
      "Data processed: 4624 in total data: 807\n",
      "Data processed: 4632 in total data: 807\n",
      "Data processed: 4640 in total data: 807\n",
      "Data processed: 4648 in total data: 807\n",
      "Data processed: 4656 in total data: 807\n",
      "Data processed: 4664 in total data: 807\n",
      "Data processed: 4672 in total data: 807\n",
      "Data processed: 4680 in total data: 807\n",
      "Data processed: 4688 in total data: 807\n",
      "Data processed: 4696 in total data: 807\n",
      "Data processed: 4704 in total data: 807\n",
      "Data processed: 4712 in total data: 807\n",
      "Data processed: 4720 in total data: 807\n",
      "Data processed: 4728 in total data: 807\n",
      "Data processed: 4736 in total data: 807\n",
      "Data processed: 4744 in total data: 807\n",
      "Data processed: 4752 in total data: 807\n",
      "Data processed: 4760 in total data: 807\n",
      "Data processed: 4768 in total data: 807\n",
      "Data processed: 4776 in total data: 807\n",
      "Data processed: 4784 in total data: 807\n",
      "Data processed: 4792 in total data: 807\n",
      "Data processed: 4800 in total data: 807\n",
      "Data processed: 4808 in total data: 807\n",
      "Data processed: 4816 in total data: 807\n",
      "Data processed: 4824 in total data: 807\n",
      "Data processed: 4832 in total data: 807\n",
      "Data processed: 4840 in total data: 807\n",
      "Data processed: 4848 in total data: 807\n",
      "Data processed: 4856 in total data: 807\n",
      "Data processed: 4864 in total data: 807\n",
      "Data processed: 4872 in total data: 807\n",
      "Data processed: 4880 in total data: 807\n",
      "Data processed: 4888 in total data: 807\n",
      "Data processed: 4896 in total data: 807\n",
      "Data processed: 4904 in total data: 807\n",
      "Data processed: 4912 in total data: 807\n",
      "Data processed: 4920 in total data: 807\n",
      "Data processed: 4928 in total data: 807\n",
      "Data processed: 4936 in total data: 807\n",
      "Data processed: 4944 in total data: 807\n",
      "Data processed: 4952 in total data: 807\n",
      "Data processed: 4960 in total data: 807\n",
      "Data processed: 4968 in total data: 807\n",
      "Data processed: 4976 in total data: 807\n",
      "Data processed: 4984 in total data: 807\n",
      "Data processed: 4992 in total data: 807\n",
      "Data processed: 5000 in total data: 807\n",
      "Data processed: 5008 in total data: 807\n",
      "Data processed: 5016 in total data: 807\n",
      "Data processed: 5024 in total data: 807\n",
      "Data processed: 5032 in total data: 807\n",
      "Data processed: 5040 in total data: 807\n",
      "Data processed: 5048 in total data: 807\n",
      "Data processed: 5056 in total data: 807\n",
      "Data processed: 5064 in total data: 807\n",
      "Data processed: 5072 in total data: 807\n",
      "Data processed: 5080 in total data: 807\n",
      "Data processed: 5088 in total data: 807\n",
      "Data processed: 5096 in total data: 807\n",
      "Data processed: 5104 in total data: 807\n",
      "Data processed: 5112 in total data: 807\n",
      "Data processed: 5120 in total data: 807\n",
      "Data processed: 5128 in total data: 807\n",
      "Data processed: 5136 in total data: 807\n",
      "Data processed: 5144 in total data: 807\n",
      "Data processed: 5152 in total data: 807\n",
      "Data processed: 5160 in total data: 807\n",
      "Data processed: 5168 in total data: 807\n",
      "Data processed: 5176 in total data: 807\n",
      "Data processed: 5184 in total data: 807\n",
      "Data processed: 5192 in total data: 807\n",
      "Data processed: 5200 in total data: 807\n",
      "Data processed: 5208 in total data: 807\n",
      "Data processed: 5216 in total data: 807\n",
      "Data processed: 5224 in total data: 807\n",
      "Data processed: 5232 in total data: 807\n",
      "Data processed: 5240 in total data: 807\n",
      "Data processed: 5248 in total data: 807\n",
      "Data processed: 5256 in total data: 807\n",
      "Data processed: 5264 in total data: 807\n",
      "Data processed: 5272 in total data: 807\n",
      "Data processed: 5280 in total data: 807\n",
      "Data processed: 5288 in total data: 807\n",
      "Data processed: 5296 in total data: 807\n",
      "Data processed: 5304 in total data: 807\n",
      "Data processed: 5312 in total data: 807\n",
      "Data processed: 5320 in total data: 807\n",
      "Data processed: 5328 in total data: 807\n",
      "Data processed: 5336 in total data: 807\n",
      "Data processed: 5344 in total data: 807\n",
      "Data processed: 5352 in total data: 807\n",
      "Data processed: 5360 in total data: 807\n",
      "Data processed: 5368 in total data: 807\n",
      "Data processed: 5376 in total data: 807\n",
      "Data processed: 5384 in total data: 807\n",
      "Data processed: 5392 in total data: 807\n",
      "Data processed: 5400 in total data: 807\n",
      "Data processed: 5408 in total data: 807\n",
      "Data processed: 5416 in total data: 807\n",
      "Data processed: 5424 in total data: 807\n",
      "Data processed: 5432 in total data: 807\n",
      "Data processed: 5440 in total data: 807\n",
      "Data processed: 5448 in total data: 807\n",
      "Data processed: 5456 in total data: 807\n",
      "Data processed: 5464 in total data: 807\n",
      "Data processed: 5472 in total data: 807\n",
      "Data processed: 5480 in total data: 807\n",
      "Data processed: 5488 in total data: 807\n",
      "Data processed: 5496 in total data: 807\n",
      "Data processed: 5504 in total data: 807\n",
      "Data processed: 5512 in total data: 807\n",
      "Data processed: 5520 in total data: 807\n",
      "Data processed: 5528 in total data: 807\n",
      "Data processed: 5536 in total data: 807\n",
      "Data processed: 5544 in total data: 807\n",
      "Data processed: 5552 in total data: 807\n",
      "Data processed: 5560 in total data: 807\n",
      "Data processed: 5568 in total data: 807\n",
      "Data processed: 5576 in total data: 807\n",
      "Data processed: 5584 in total data: 807\n",
      "Data processed: 5592 in total data: 807\n",
      "Data processed: 5600 in total data: 807\n",
      "Data processed: 5608 in total data: 807\n",
      "Data processed: 5616 in total data: 807\n",
      "Data processed: 5624 in total data: 807\n",
      "Data processed: 5632 in total data: 807\n",
      "Data processed: 5640 in total data: 807\n",
      "Data processed: 5648 in total data: 807\n",
      "Data processed: 5656 in total data: 807\n",
      "Data processed: 5664 in total data: 807\n",
      "Data processed: 5672 in total data: 807\n",
      "Data processed: 5680 in total data: 807\n",
      "Data processed: 5688 in total data: 807\n",
      "Data processed: 5696 in total data: 807\n",
      "Data processed: 5704 in total data: 807\n",
      "Data processed: 5712 in total data: 807\n",
      "Data processed: 5720 in total data: 807\n",
      "Data processed: 5728 in total data: 807\n",
      "Data processed: 5736 in total data: 807\n",
      "Data processed: 5744 in total data: 807\n",
      "Data processed: 5752 in total data: 807\n",
      "Data processed: 5760 in total data: 807\n",
      "Data processed: 5768 in total data: 807\n",
      "Data processed: 5776 in total data: 807\n",
      "Data processed: 5784 in total data: 807\n",
      "Data processed: 5792 in total data: 807\n",
      "Data processed: 5800 in total data: 807\n",
      "Data processed: 5808 in total data: 807\n",
      "Data processed: 5816 in total data: 807\n",
      "Data processed: 5824 in total data: 807\n",
      "Data processed: 5832 in total data: 807\n",
      "Data processed: 5840 in total data: 807\n",
      "Data processed: 5848 in total data: 807\n",
      "Data processed: 5856 in total data: 807\n",
      "Data processed: 5864 in total data: 807\n",
      "Data processed: 5872 in total data: 807\n",
      "Data processed: 5880 in total data: 807\n",
      "Data processed: 5888 in total data: 807\n",
      "Data processed: 5896 in total data: 807\n",
      "Data processed: 5904 in total data: 807\n",
      "Data processed: 5912 in total data: 807\n",
      "Data processed: 5920 in total data: 807\n",
      "Data processed: 5928 in total data: 807\n",
      "Data processed: 5936 in total data: 807\n",
      "Data processed: 5944 in total data: 807\n",
      "Data processed: 5952 in total data: 807\n",
      "Data processed: 5960 in total data: 807\n",
      "Data processed: 5968 in total data: 807\n",
      "Data processed: 5976 in total data: 807\n",
      "Data processed: 5984 in total data: 807\n",
      "Data processed: 5992 in total data: 807\n",
      "Data processed: 6000 in total data: 807\n",
      "Data processed: 6008 in total data: 807\n",
      "Data processed: 6016 in total data: 807\n",
      "Data processed: 6024 in total data: 807\n",
      "Data processed: 6032 in total data: 807\n",
      "Data processed: 6040 in total data: 807\n",
      "Data processed: 6048 in total data: 807\n",
      "Data processed: 6056 in total data: 807\n",
      "Data processed: 6064 in total data: 807\n",
      "Data processed: 6072 in total data: 807\n",
      "Data processed: 6080 in total data: 807\n",
      "Data processed: 6088 in total data: 807\n",
      "Data processed: 6096 in total data: 807\n",
      "Data processed: 6104 in total data: 807\n",
      "Data processed: 6112 in total data: 807\n",
      "Data processed: 6120 in total data: 807\n",
      "Data processed: 6128 in total data: 807\n",
      "Data processed: 6136 in total data: 807\n",
      "Data processed: 6144 in total data: 807\n",
      "Data processed: 6152 in total data: 807\n",
      "Data processed: 6160 in total data: 807\n",
      "Data processed: 6168 in total data: 807\n",
      "Data processed: 6176 in total data: 807\n",
      "Data processed: 6184 in total data: 807\n",
      "Data processed: 6192 in total data: 807\n",
      "Data processed: 6200 in total data: 807\n",
      "Data processed: 6208 in total data: 807\n",
      "Data processed: 6216 in total data: 807\n",
      "Data processed: 6224 in total data: 807\n",
      "Data processed: 6232 in total data: 807\n",
      "Data processed: 6240 in total data: 807\n",
      "Data processed: 6248 in total data: 807\n",
      "Data processed: 6256 in total data: 807\n",
      "Data processed: 6264 in total data: 807\n",
      "Data processed: 6272 in total data: 807\n",
      "Data processed: 6280 in total data: 807\n",
      "Data processed: 6288 in total data: 807\n",
      "Data processed: 6296 in total data: 807\n",
      "Data processed: 6304 in total data: 807\n",
      "Data processed: 6312 in total data: 807\n",
      "Data processed: 6320 in total data: 807\n",
      "Data processed: 6328 in total data: 807\n",
      "Data processed: 6336 in total data: 807\n",
      "Data processed: 6344 in total data: 807\n",
      "Data processed: 6352 in total data: 807\n",
      "Data processed: 6360 in total data: 807\n",
      "Data processed: 6368 in total data: 807\n",
      "Data processed: 6376 in total data: 807\n",
      "Data processed: 6384 in total data: 807\n",
      "Data processed: 6392 in total data: 807\n",
      "Data processed: 6400 in total data: 807\n",
      "Data processed: 6408 in total data: 807\n",
      "Data processed: 6416 in total data: 807\n",
      "Data processed: 6424 in total data: 807\n",
      "Data processed: 6432 in total data: 807\n",
      "Data processed: 6440 in total data: 807\n",
      "Data processed: 6448 in total data: 807\n",
      "Data processed: 6450 in total data: 807\n",
      "Epoch 2: Loss=0.0000, Train Acc=4.1247, Val Acc=4.0688\n",
      "Data processed: 8 in total data: 807\n",
      "Data processed: 16 in total data: 807\n",
      "Data processed: 24 in total data: 807\n",
      "Data processed: 32 in total data: 807\n",
      "Data processed: 40 in total data: 807\n",
      "Data processed: 48 in total data: 807\n",
      "Data processed: 56 in total data: 807\n",
      "Data processed: 64 in total data: 807\n",
      "Data processed: 72 in total data: 807\n",
      "Data processed: 80 in total data: 807\n",
      "Data processed: 88 in total data: 807\n",
      "Data processed: 96 in total data: 807\n",
      "Data processed: 104 in total data: 807\n",
      "Data processed: 112 in total data: 807\n",
      "Data processed: 120 in total data: 807\n",
      "Data processed: 128 in total data: 807\n",
      "Data processed: 136 in total data: 807\n",
      "Data processed: 144 in total data: 807\n",
      "Data processed: 152 in total data: 807\n",
      "Data processed: 160 in total data: 807\n",
      "Data processed: 168 in total data: 807\n",
      "Data processed: 176 in total data: 807\n",
      "Data processed: 184 in total data: 807\n",
      "Data processed: 192 in total data: 807\n",
      "Data processed: 200 in total data: 807\n",
      "Data processed: 208 in total data: 807\n",
      "Data processed: 216 in total data: 807\n",
      "Data processed: 224 in total data: 807\n",
      "Data processed: 232 in total data: 807\n",
      "Data processed: 240 in total data: 807\n",
      "Data processed: 248 in total data: 807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m train_model(model, train_dataloader, val_dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[65], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m input_ids, attention_mask, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_mask\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[52], line 10\u001b[0m, in \u001b[0;36mBertGenderClassifier.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m     11\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output  \u001b[38;5;66;03m# [batch_size, hidden_size]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1143\u001b[0m     embedding_output,\n\u001b[0;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1145\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1146\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1147\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1148\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1149\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1150\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1151\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1153\u001b[0m )\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    696\u001b[0m         hidden_states,\n\u001b[0;32m    697\u001b[0m         attention_mask,\n\u001b[0;32m    698\u001b[0m         layer_head_mask,\n\u001b[0;32m    699\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    700\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    701\u001b[0m         past_key_value,\n\u001b[0;32m    702\u001b[0m         output_attentions,\n\u001b[0;32m    703\u001b[0m     )\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    586\u001b[0m         hidden_states,\n\u001b[0;32m    587\u001b[0m         attention_mask,\n\u001b[0;32m    588\u001b[0m         head_mask,\n\u001b[0;32m    589\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    590\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    516\u001b[0m         hidden_states,\n\u001b[0;32m    517\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         output_attentions,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:467\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m--> 467\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    468\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_correct, processed_data = 0, 0, 0\n",
    "        \n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            processed_data += len(input_ids)\n",
    "            print(f'Data processed: {processed_data} in total data: {len(train_loader)}')\n",
    "\n",
    "        train_acc = total_correct / len(train_loader.dataset)\n",
    "        val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in loader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, val_dataloader, epochs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', num_labels=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification, Trainer, TFTrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=7,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=1e-5,               \n",
    "    logging_dir='./logs',            \n",
    "    eval_steps=100                   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                 \n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \n\u001b[0;32m      4\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,         \n\u001b[0;32m      5\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,            \n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\trainer.py:459\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_accelerator_and_postprocess()\n\u001b[0;32m    461\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[1;32mc:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\transformers\\trainer.py:5032\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5029\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m=\u001b[39m grad_acc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m-> 5032\u001b[0m accelerator_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maccelerator_config\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m   5034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.28.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   5035\u001b[0m     dataloader_config \u001b[38;5;241m=\u001b[39m DataLoaderConfiguration(\n\u001b[0;32m   5036\u001b[0m         split_batches\u001b[38;5;241m=\u001b[39maccelerator_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   5037\u001b[0m         dispatch_batches\u001b[38;5;241m=\u001b[39maccelerator_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdispatch_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   5038\u001b[0m         even_batches\u001b[38;5;241m=\u001b[39maccelerator_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meven_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   5039\u001b[0m         use_seedable_sampler\u001b[38;5;241m=\u001b[39maccelerator_config\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_seedable_sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   5040\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_dict'"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                 \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset,            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.0",
   "language": "python",
   "name": "v3.11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
