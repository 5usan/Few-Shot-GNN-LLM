{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\susan\\anaconda3\\envs\\v3.11.0\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import contractions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from io import StringIO\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from  sentence_transformers import SentenceTransformer\n",
    "from torch_geometric.nn import GCNConv, GATConv,GraphSAGE\n",
    "from torch_geometric.transforms import RandomNodeSplit, RandomLinkSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_regions = {\n",
    "    \"Northeast\": [\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Rhode Island\", \"Connecticut\", \"New York\", \"New Jersey\", \"Pennsylvania\"],\n",
    "    \"Midwest\": [\"Ohio\", \"Indiana\", \"Illinois\", \"Michigan\", \"Wisconsin\", \"Missouri\", \"North Dakota\", \"South Dakota\", \"Nebraska\", \"Kansas\"],\n",
    "    \"South\": [\"Delaware\", \"Maryland\", \"Virginia\", \"West Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\", \"Kentucky\", \"Tennessee\", \"Alabama\", \"Mississippi\", \"Arkansas\", \"Louisiana\", \"Oklahoma\", \"Texas\"],\n",
    "    \"West\": [\"Montana\", \"Wyoming\", \"Colorado\", \"New Mexico\", \"Arizona\", \"Utah\", \"Idaho\", \"Nevada\", \"Washington\", \"Oregon\", \"California\", \"Alaska\", \"Hawaii\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: d:\\Projects\\Machine Learning\\Few-Shot-GNN-LLM\\data-related\\GeoText_raw.csv\n",
      "Processed data path: d:\\Projects\\Machine Learning\\Few-Shot-GNN-LLM\\data-related\\GeoText.csv\n"
     ]
    }
   ],
   "source": [
    "GEOTEXTDATA_PATH = os.path.join(os.getcwd(), 'GeoText.csv')\n",
    "GEOTEXT_RAW_DATA_PATH = os.path.join(os.getcwd(), 'GeoText_raw.csv')\n",
    "print(f'Raw data path: {GEOTEXT_RAW_DATA_PATH}')\n",
    "print(f'Processed data path: {GEOTEXTDATA_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_regularization(text):\n",
    "    try:\n",
    "        text = str(text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Expand contractions, can't => cannot\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "        # Remove punctuations\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "        # Normalize accented characters \"café\" → \"cafe\"\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "        # Remove extra white spaces\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('text_regularization', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTextDataExtractor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            print(\"Instance Created\")\n",
    "            us_states = gpd.read_file(os.path.join('ne_110m_admin_1_states_provinces', 'ne_110m_admin_1_states_provinces.shp'))\n",
    "            self.us_states = us_states[us_states['admin'] == \"United States of America\"]\n",
    "            self.df = ''\n",
    "            self.region_bounds_range = {\n",
    "                \"Northeast\": self.get_region_lat_long_range(\"Northeast\"),\n",
    "                \"Midwest\": self.get_region_lat_long_range(\"Midwest\"),\n",
    "                \"South\": self.get_region_lat_long_range(\"South\"),\n",
    "                \"West\": self.get_region_lat_long_range(\"West\"),\n",
    "            }\n",
    "            self.feature = []\n",
    "            self.label = []\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "    \n",
    "    def get_region_lat_long_range(self, region_name):\n",
    "        try:\n",
    "            if region_name not in US_regions:\n",
    "                print(f\"Region '{region_name}' not found\")\n",
    "                return\n",
    "\n",
    "            region_states = US_regions[region_name]\n",
    "            region_bounds = {\"lat_min\": float('inf'), \"lat_max\": float('-inf'), \"lon_min\": float('inf'), \"lon_max\": float('-inf')}\n",
    "\n",
    "            for state_name in region_states:\n",
    "                state = self.us_states[self.us_states['name'] == state_name]\n",
    "\n",
    "                if state.empty:\n",
    "                    return\n",
    "\n",
    "                minx, miny, maxx, maxy = state.geometry.bounds.iloc[0]\n",
    "\n",
    "                # Update the region bounding box\n",
    "                region_bounds[\"lat_min\"] = min(region_bounds[\"lat_min\"], miny)\n",
    "                region_bounds[\"lat_max\"] = max(region_bounds[\"lat_max\"], maxy)\n",
    "                region_bounds[\"lon_min\"] = min(region_bounds[\"lon_min\"], minx)\n",
    "                region_bounds[\"lon_max\"] = max(region_bounds[\"lon_max\"], maxx)\n",
    "\n",
    "            return region_bounds\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "\n",
    "    def compute_region(self, coord):\n",
    "        try:\n",
    "            for region_name, bounding_range in self.region_bounds_range.items():\n",
    "                latitude, longitude = coord\n",
    "                lat_min = bounding_range['lat_min']\n",
    "                lat_max = bounding_range['lat_max']\n",
    "                lon_min = bounding_range['lon_min']\n",
    "                lon_max = bounding_range['lon_max']\n",
    "\n",
    "                if lat_min<=latitude<=lat_max and lon_min<=longitude<=lon_max:\n",
    "                    return region_name\n",
    "            return \"Unknown\"\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "    \n",
    "    def geo_text_data_extractor(self):\n",
    "        try:\n",
    "            # with open(os.path.join(os.getcwd(), 'GeoText_raw.csv'), \"r\", encoding=\"ISO-8859-1\") as data_source:\n",
    "            #     first_raw_data = data_source.readline()\n",
    "            #     self.df = pd.read_csv(StringIO(first_raw_data), sep=\"\\t\", header=None, names=[\"User ID\", \"Timestamp\",\"Location\", \"Latitude\", \"Longitude\", \"Tweet Content\"])\n",
    "            #     self.compute_region((self.df[\"Latitude\"][0], self.df[\"Longitude\"][0]))\n",
    "            #     raw_data = data_source.read()\n",
    "            #     self.df = pd.read_csv(StringIO(raw_data), sep=\"\\t\", header=None, names=[\"User ID\", \"Timestamp\",\"Location\", \"Latitude\", \"Longitude\", \"Tweet Content\"])\n",
    "            #     self.df[\"Location\"] = self.df.apply(lambda row: self.compute_region((row[\"Latitude\"], row[\"Longitude\"])), axis = 1)\n",
    "            #     self.df.to_csv(GEOTEXTDATA_PATH, index=False)\n",
    "            geotext_data = pd.read_csv(GEOTEXT_RAW_DATA_PATH)\n",
    "            geotext_data['location'] = [self.compute_region(each) for each in zip(geotext_data[\"latitude\"], geotext_data[\"longitude\"])]\n",
    "            geotext_data.to_csv(GEOTEXTDATA_PATH, index=False)\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        geotext_data = pd.read_csv(GEOTEXTDATA_PATH)\n",
    "        geotext_data = geotext_data.dropna()\n",
    "        geotext_data = geotext_data[geotext_data['location'] != \"Unknown\"]\n",
    "        self.feature = [text_regularization(each) for each in geotext_data[\"tweet\"]]\n",
    "        self.label = geotext_data['location']\n",
    "        return {\n",
    "            \"feature\": self.feature,\n",
    "            \"label\": self.label.tolist()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance Created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GeoTextDataExtractorInstance = GeoTextDataExtractor()\n",
    "# data = GeoTextDataExtractorInstance.geo_text_data_extractor()\n",
    "data = GeoTextDataExtractorInstance.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = data['feature']\n",
    "label = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_embeddings = sbert_model.encode(feature, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 0),\n",
       " (1, 1),\n",
       " (1, 1),\n",
       " (2, 2),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (8, 8),\n",
       " (9, 9),\n",
       " (9, 9),\n",
       " (10, 10),\n",
       " (10, 10),\n",
       " (11, 11),\n",
       " (11, 11),\n",
       " (12, 12),\n",
       " (12, 12),\n",
       " (13, 13),\n",
       " (13, 13),\n",
       " (14, 14),\n",
       " (14, 14),\n",
       " (15, 15),\n",
       " (15, 15),\n",
       " (16, 16),\n",
       " (16, 16),\n",
       " (17, 17),\n",
       " (17, 17),\n",
       " (18, 18),\n",
       " (18, 18),\n",
       " (19, 19),\n",
       " (19, 19),\n",
       " (20, 20),\n",
       " (20, 20),\n",
       " (21, 21),\n",
       " (21, 21),\n",
       " (22, 22),\n",
       " (22, 22),\n",
       " (23, 23),\n",
       " (23, 23),\n",
       " (24, 24),\n",
       " (24, 24),\n",
       " (25, 25),\n",
       " (25, 25),\n",
       " (26, 26),\n",
       " (26, 26),\n",
       " (27, 27),\n",
       " (27, 27),\n",
       " (28, 28),\n",
       " (28, 28),\n",
       " (29, 29),\n",
       " (29, 29),\n",
       " (30, 30),\n",
       " (30, 30),\n",
       " (31, 31),\n",
       " (31, 31),\n",
       " (32, 32),\n",
       " (32, 32),\n",
       " (33, 33),\n",
       " (33, 33),\n",
       " (34, 34),\n",
       " (34, 34),\n",
       " (35, 35),\n",
       " (35, 35),\n",
       " (36, 36),\n",
       " (36, 36),\n",
       " (37, 37),\n",
       " (37, 37),\n",
       " (38, 38),\n",
       " (38, 38),\n",
       " (39, 39),\n",
       " (39, 39),\n",
       " (40, 40),\n",
       " (40, 40),\n",
       " (41, 41),\n",
       " (41, 41),\n",
       " (42, 42),\n",
       " (42, 42),\n",
       " (43, 43),\n",
       " (43, 43),\n",
       " (44, 44),\n",
       " (44, 44),\n",
       " (45, 45),\n",
       " (45, 45),\n",
       " (46, 46),\n",
       " (46, 46),\n",
       " (47, 47),\n",
       " (47, 47),\n",
       " (48, 48),\n",
       " (48, 48),\n",
       " (49, 49),\n",
       " (49, 49),\n",
       " (50, 50),\n",
       " (50, 50),\n",
       " (51, 51),\n",
       " (51, 51),\n",
       " (52, 52),\n",
       " (52, 52),\n",
       " (53, 53),\n",
       " (53, 53),\n",
       " (54, 54),\n",
       " (54, 54),\n",
       " (55, 55),\n",
       " (55, 55),\n",
       " (56, 56),\n",
       " (56, 56),\n",
       " (57, 57),\n",
       " (57, 57),\n",
       " (58, 58),\n",
       " (58, 58),\n",
       " (59, 59),\n",
       " (59, 59),\n",
       " (60, 60),\n",
       " (60, 60),\n",
       " (61, 61),\n",
       " (61, 61),\n",
       " (62, 62),\n",
       " (62, 62),\n",
       " (63, 63),\n",
       " (63, 63),\n",
       " (64, 64),\n",
       " (64, 64),\n",
       " (65, 65),\n",
       " (65, 65),\n",
       " (66, 66),\n",
       " (66, 66),\n",
       " (67, 67),\n",
       " (67, 67),\n",
       " (68, 68),\n",
       " (68, 68),\n",
       " (69, 69),\n",
       " (69, 69),\n",
       " (70, 70),\n",
       " (70, 70),\n",
       " (71, 71),\n",
       " (71, 71),\n",
       " (72, 72),\n",
       " (72, 72),\n",
       " (73, 73),\n",
       " (73, 73),\n",
       " (74, 74),\n",
       " (74, 74),\n",
       " (75, 75),\n",
       " (75, 75),\n",
       " (76, 76),\n",
       " (76, 76),\n",
       " (77, 77),\n",
       " (77, 77),\n",
       " (78, 78),\n",
       " (78, 78),\n",
       " (79, 79),\n",
       " (79, 79),\n",
       " (80, 80),\n",
       " (80, 80),\n",
       " (81, 81),\n",
       " (81, 81),\n",
       " (82, 82),\n",
       " (82, 82),\n",
       " (83, 83),\n",
       " (83, 83),\n",
       " (84, 84),\n",
       " (84, 84),\n",
       " (85, 85),\n",
       " (85, 85),\n",
       " (86, 86),\n",
       " (86, 86),\n",
       " (87, 87),\n",
       " (87, 87),\n",
       " (88, 88),\n",
       " (88, 88),\n",
       " (89, 89),\n",
       " (89, 89),\n",
       " (90, 90),\n",
       " (90, 90),\n",
       " (91, 91),\n",
       " (91, 91),\n",
       " (92, 92),\n",
       " (92, 92),\n",
       " (93, 93),\n",
       " (93, 93),\n",
       " (94, 94),\n",
       " (94, 94),\n",
       " (95, 95),\n",
       " (95, 95),\n",
       " (96, 96),\n",
       " (96, 96),\n",
       " (97, 97),\n",
       " (97, 97),\n",
       " (98, 98),\n",
       " (98, 98),\n",
       " (99, 99),\n",
       " (99, 99),\n",
       " (100, 100),\n",
       " (100, 100),\n",
       " (101, 101),\n",
       " (101, 101),\n",
       " (102, 102),\n",
       " (102, 102),\n",
       " (103, 103),\n",
       " (103, 103),\n",
       " (104, 104),\n",
       " (104, 104),\n",
       " (105, 105),\n",
       " (105, 105),\n",
       " (106, 106),\n",
       " (106, 106),\n",
       " (107, 107),\n",
       " (107, 107),\n",
       " (108, 108),\n",
       " (108, 108),\n",
       " (109, 109),\n",
       " (109, 109),\n",
       " (110, 110),\n",
       " (110, 110),\n",
       " (111, 111),\n",
       " (111, 111),\n",
       " (112, 112),\n",
       " (112, 112),\n",
       " (113, 113),\n",
       " (113, 113),\n",
       " (114, 114),\n",
       " (114, 114),\n",
       " (115, 115),\n",
       " (115, 115),\n",
       " (116, 116),\n",
       " (116, 116),\n",
       " (117, 117),\n",
       " (117, 117),\n",
       " (118, 118),\n",
       " (118, 118),\n",
       " (119, 119),\n",
       " (119, 119),\n",
       " (120, 120),\n",
       " (120, 120),\n",
       " (121, 121),\n",
       " (121, 121),\n",
       " (122, 122),\n",
       " (122, 122),\n",
       " (123, 123),\n",
       " (123, 123),\n",
       " (124, 124),\n",
       " (124, 124),\n",
       " (125, 125),\n",
       " (125, 125),\n",
       " (126, 126),\n",
       " (126, 126),\n",
       " (127, 127),\n",
       " (127, 127),\n",
       " (128, 128),\n",
       " (128, 128),\n",
       " (129, 129),\n",
       " (129, 129),\n",
       " (130, 130),\n",
       " (130, 130),\n",
       " (131, 131),\n",
       " (131, 131),\n",
       " (132, 132),\n",
       " (132, 132),\n",
       " (133, 133),\n",
       " (133, 133),\n",
       " (134, 134),\n",
       " (134, 134),\n",
       " (135, 135),\n",
       " (135, 135),\n",
       " (136, 136),\n",
       " (136, 136),\n",
       " (137, 137),\n",
       " (137, 137),\n",
       " (138, 138),\n",
       " (138, 138),\n",
       " (139, 139),\n",
       " (139, 139),\n",
       " (140, 140),\n",
       " (140, 140),\n",
       " (141, 141),\n",
       " (141, 141),\n",
       " (142, 142),\n",
       " (142, 142),\n",
       " (143, 143),\n",
       " (143, 143),\n",
       " (144, 144),\n",
       " (144, 144),\n",
       " (145, 145),\n",
       " (145, 145),\n",
       " (146, 146),\n",
       " (146, 146),\n",
       " (147, 147),\n",
       " (147, 147),\n",
       " (148, 148),\n",
       " (148, 148),\n",
       " (149, 149),\n",
       " (149, 149),\n",
       " (150, 150),\n",
       " (150, 150),\n",
       " (151, 151),\n",
       " (151, 151),\n",
       " (152, 152),\n",
       " (152, 152),\n",
       " (153, 153),\n",
       " (153, 153),\n",
       " (154, 154),\n",
       " (154, 154),\n",
       " (155, 155),\n",
       " (155, 155),\n",
       " (156, 156),\n",
       " (156, 156),\n",
       " (157, 157),\n",
       " (157, 157),\n",
       " (158, 158),\n",
       " (158, 158),\n",
       " (159, 159),\n",
       " (159, 159),\n",
       " (160, 160),\n",
       " (160, 160),\n",
       " (161, 161),\n",
       " (161, 161),\n",
       " (162, 162),\n",
       " (162, 162),\n",
       " (163, 163),\n",
       " (163, 163),\n",
       " (164, 164),\n",
       " (164, 164),\n",
       " (165, 165),\n",
       " (165, 165),\n",
       " (166, 166),\n",
       " (166, 166),\n",
       " (167, 167),\n",
       " (167, 167),\n",
       " (168, 168),\n",
       " (168, 168),\n",
       " (169, 169),\n",
       " (169, 169),\n",
       " (170, 170),\n",
       " (170, 170),\n",
       " (171, 171),\n",
       " (171, 171),\n",
       " (172, 172),\n",
       " (172, 172),\n",
       " (173, 173),\n",
       " (173, 173),\n",
       " (174, 174),\n",
       " (174, 174),\n",
       " (175, 175),\n",
       " (175, 175),\n",
       " (176, 176),\n",
       " (176, 176),\n",
       " (177, 177),\n",
       " (177, 177),\n",
       " (178, 178),\n",
       " (178, 178),\n",
       " (179, 179),\n",
       " (179, 179),\n",
       " (179, 8862),\n",
       " (8862, 179),\n",
       " (180, 180),\n",
       " (180, 180),\n",
       " (181, 181),\n",
       " (181, 181),\n",
       " (182, 182),\n",
       " (182, 182),\n",
       " (183, 183),\n",
       " (183, 183),\n",
       " (184, 184),\n",
       " (184, 184),\n",
       " (185, 185),\n",
       " (185, 185),\n",
       " (186, 186),\n",
       " (186, 186),\n",
       " (187, 187),\n",
       " (187, 187),\n",
       " (188, 188),\n",
       " (188, 188),\n",
       " (189, 189),\n",
       " (189, 189),\n",
       " (190, 190),\n",
       " (190, 190),\n",
       " (191, 191),\n",
       " (191, 191),\n",
       " (192, 192),\n",
       " (192, 192),\n",
       " (193, 193),\n",
       " (193, 193),\n",
       " (194, 194),\n",
       " (194, 194),\n",
       " (195, 195),\n",
       " (195, 195),\n",
       " (196, 196),\n",
       " (196, 196),\n",
       " (197, 197),\n",
       " (197, 197),\n",
       " (198, 198),\n",
       " (198, 198),\n",
       " (199, 199),\n",
       " (199, 199),\n",
       " (200, 200),\n",
       " (200, 200),\n",
       " (201, 201),\n",
       " (201, 201),\n",
       " (202, 202),\n",
       " (202, 202),\n",
       " (203, 203),\n",
       " (203, 203),\n",
       " (204, 204),\n",
       " (204, 204),\n",
       " (205, 205),\n",
       " (205, 205),\n",
       " (206, 206),\n",
       " (206, 206),\n",
       " (207, 207),\n",
       " (207, 207),\n",
       " (208, 208),\n",
       " (208, 208),\n",
       " (209, 209),\n",
       " (209, 209),\n",
       " (210, 210),\n",
       " (210, 210),\n",
       " (211, 211),\n",
       " (211, 211),\n",
       " (212, 212),\n",
       " (212, 212),\n",
       " (213, 213),\n",
       " (213, 213),\n",
       " (214, 214),\n",
       " (214, 214),\n",
       " (215, 215),\n",
       " (215, 215),\n",
       " (216, 216),\n",
       " (216, 216),\n",
       " (217, 217),\n",
       " (217, 217),\n",
       " (218, 218),\n",
       " (218, 218),\n",
       " (219, 219),\n",
       " (219, 219),\n",
       " (220, 220),\n",
       " (220, 220),\n",
       " (221, 221),\n",
       " (221, 221),\n",
       " (222, 222),\n",
       " (222, 222),\n",
       " (223, 223),\n",
       " (223, 223),\n",
       " (224, 224),\n",
       " (224, 224),\n",
       " (225, 225),\n",
       " (225, 225),\n",
       " (226, 226),\n",
       " (226, 226),\n",
       " (227, 227),\n",
       " (227, 227),\n",
       " (228, 228),\n",
       " (228, 228),\n",
       " (229, 229),\n",
       " (229, 229),\n",
       " (230, 230),\n",
       " (230, 230),\n",
       " (231, 231),\n",
       " (231, 231),\n",
       " (232, 232),\n",
       " (232, 232),\n",
       " (233, 233),\n",
       " (233, 233),\n",
       " (234, 234),\n",
       " (234, 234),\n",
       " (235, 235),\n",
       " (235, 235),\n",
       " (236, 236),\n",
       " (236, 236),\n",
       " (237, 237),\n",
       " (237, 237),\n",
       " (238, 238),\n",
       " (238, 238),\n",
       " (239, 239),\n",
       " (239, 239),\n",
       " (240, 240),\n",
       " (240, 240),\n",
       " (241, 241),\n",
       " (241, 241),\n",
       " (242, 242),\n",
       " (242, 242),\n",
       " (243, 243),\n",
       " (243, 243),\n",
       " (244, 244),\n",
       " (244, 244),\n",
       " (245, 245),\n",
       " (245, 245),\n",
       " (246, 246),\n",
       " (246, 246),\n",
       " (247, 247),\n",
       " (247, 247),\n",
       " (248, 248),\n",
       " (248, 248),\n",
       " (249, 249),\n",
       " (249, 249),\n",
       " (250, 250),\n",
       " (250, 250),\n",
       " (251, 251),\n",
       " (251, 251),\n",
       " (252, 252),\n",
       " (252, 252),\n",
       " (253, 253),\n",
       " (253, 253),\n",
       " (254, 254),\n",
       " (254, 254),\n",
       " (255, 255),\n",
       " (255, 255),\n",
       " (256, 256),\n",
       " (256, 256),\n",
       " (257, 257),\n",
       " (257, 257),\n",
       " (258, 258),\n",
       " (258, 258),\n",
       " (259, 259),\n",
       " (259, 259),\n",
       " (260, 260),\n",
       " (260, 260),\n",
       " (261, 261),\n",
       " (261, 261),\n",
       " (262, 262),\n",
       " (262, 262),\n",
       " (263, 263),\n",
       " (263, 263),\n",
       " (264, 264),\n",
       " (264, 264),\n",
       " (265, 265),\n",
       " (265, 265),\n",
       " (266, 266),\n",
       " (266, 266),\n",
       " (267, 267),\n",
       " (267, 267),\n",
       " (268, 268),\n",
       " (268, 268),\n",
       " (269, 269),\n",
       " (269, 269),\n",
       " (270, 270),\n",
       " (270, 270),\n",
       " (271, 271),\n",
       " (271, 271),\n",
       " (272, 272),\n",
       " (272, 272),\n",
       " (273, 273),\n",
       " (273, 273),\n",
       " (274, 274),\n",
       " (274, 274),\n",
       " (275, 275),\n",
       " (275, 275),\n",
       " (276, 276),\n",
       " (276, 276),\n",
       " (277, 277),\n",
       " (277, 277),\n",
       " (278, 278),\n",
       " (278, 278),\n",
       " (279, 279),\n",
       " (279, 279),\n",
       " (280, 280),\n",
       " (280, 280),\n",
       " (281, 281),\n",
       " (281, 281),\n",
       " (282, 282),\n",
       " (282, 282),\n",
       " (283, 283),\n",
       " (283, 283),\n",
       " (284, 284),\n",
       " (284, 284),\n",
       " (285, 285),\n",
       " (285, 285),\n",
       " (286, 286),\n",
       " (286, 286),\n",
       " (287, 287),\n",
       " (287, 287),\n",
       " (288, 288),\n",
       " (288, 288),\n",
       " (289, 289),\n",
       " (289, 289),\n",
       " (290, 290),\n",
       " (290, 290),\n",
       " (291, 291),\n",
       " (291, 291),\n",
       " (292, 292),\n",
       " (292, 292),\n",
       " (293, 293),\n",
       " (293, 293),\n",
       " (294, 294),\n",
       " (294, 294),\n",
       " (295, 295),\n",
       " (295, 295),\n",
       " (296, 296),\n",
       " (296, 296),\n",
       " (297, 297),\n",
       " (297, 297),\n",
       " (298, 298),\n",
       " (298, 298),\n",
       " (299, 299),\n",
       " (299, 299),\n",
       " (300, 300),\n",
       " (300, 300),\n",
       " (301, 301),\n",
       " (301, 301),\n",
       " (302, 302),\n",
       " (302, 302),\n",
       " (303, 303),\n",
       " (303, 303),\n",
       " (304, 304),\n",
       " (304, 304),\n",
       " (305, 305),\n",
       " (305, 305),\n",
       " (306, 306),\n",
       " (306, 306),\n",
       " (307, 307),\n",
       " (307, 307),\n",
       " (308, 308),\n",
       " (308, 308),\n",
       " (309, 309),\n",
       " (309, 309),\n",
       " (310, 310),\n",
       " (310, 310),\n",
       " (311, 311),\n",
       " (311, 311),\n",
       " (312, 312),\n",
       " (312, 312),\n",
       " (313, 313),\n",
       " (313, 313),\n",
       " (314, 314),\n",
       " (314, 314),\n",
       " (315, 315),\n",
       " (315, 315),\n",
       " (316, 316),\n",
       " (316, 316),\n",
       " (317, 317),\n",
       " (317, 317),\n",
       " (318, 318),\n",
       " (318, 318),\n",
       " (319, 319),\n",
       " (319, 319),\n",
       " (320, 320),\n",
       " (320, 320),\n",
       " (321, 321),\n",
       " (321, 321),\n",
       " (322, 322),\n",
       " (322, 322),\n",
       " (323, 323),\n",
       " (323, 323),\n",
       " (324, 324),\n",
       " (324, 324),\n",
       " (325, 325),\n",
       " (325, 325),\n",
       " (326, 326),\n",
       " (326, 326),\n",
       " (327, 327),\n",
       " (327, 327),\n",
       " (328, 328),\n",
       " (328, 328),\n",
       " (329, 329),\n",
       " (329, 329),\n",
       " (330, 330),\n",
       " (330, 330),\n",
       " (331, 331),\n",
       " (331, 331),\n",
       " (332, 332),\n",
       " (332, 332),\n",
       " (333, 333),\n",
       " (333, 333),\n",
       " (334, 334),\n",
       " (334, 334),\n",
       " (335, 335),\n",
       " (335, 335),\n",
       " (336, 336),\n",
       " (336, 336),\n",
       " (337, 337),\n",
       " (337, 337),\n",
       " (338, 338),\n",
       " (338, 338),\n",
       " (339, 339),\n",
       " (339, 339),\n",
       " (339, 512),\n",
       " (512, 339),\n",
       " (340, 340),\n",
       " (340, 340),\n",
       " (341, 341),\n",
       " (341, 341),\n",
       " (342, 342),\n",
       " (342, 342),\n",
       " (343, 343),\n",
       " (343, 343),\n",
       " (344, 344),\n",
       " (344, 344),\n",
       " (345, 345),\n",
       " (345, 345),\n",
       " (346, 346),\n",
       " (346, 346),\n",
       " (347, 347),\n",
       " (347, 347),\n",
       " (348, 348),\n",
       " (348, 348),\n",
       " (349, 349),\n",
       " (349, 349),\n",
       " (350, 350),\n",
       " (350, 350),\n",
       " (351, 351),\n",
       " (351, 351),\n",
       " (352, 352),\n",
       " (352, 352),\n",
       " (353, 353),\n",
       " (353, 353),\n",
       " (354, 354),\n",
       " (354, 354),\n",
       " (355, 355),\n",
       " (355, 355),\n",
       " (356, 356),\n",
       " (356, 356),\n",
       " (357, 357),\n",
       " (357, 357),\n",
       " (358, 358),\n",
       " (358, 358),\n",
       " (359, 359),\n",
       " (359, 359),\n",
       " (360, 360),\n",
       " (360, 360),\n",
       " (361, 361),\n",
       " (361, 361),\n",
       " (362, 362),\n",
       " (362, 362),\n",
       " (363, 363),\n",
       " (363, 363),\n",
       " (364, 364),\n",
       " (364, 364),\n",
       " (365, 365),\n",
       " (365, 365),\n",
       " (366, 366),\n",
       " (366, 366),\n",
       " (367, 367),\n",
       " (367, 367),\n",
       " (368, 368),\n",
       " (368, 368),\n",
       " (369, 369),\n",
       " (369, 369),\n",
       " (370, 370),\n",
       " (370, 370),\n",
       " (371, 371),\n",
       " (371, 371),\n",
       " (372, 372),\n",
       " (372, 372),\n",
       " (373, 373),\n",
       " (373, 373),\n",
       " (374, 374),\n",
       " (374, 374),\n",
       " (375, 375),\n",
       " (375, 375),\n",
       " (376, 376),\n",
       " (376, 376),\n",
       " (377, 377),\n",
       " (377, 377),\n",
       " (378, 378),\n",
       " (378, 378),\n",
       " (379, 379),\n",
       " (379, 379),\n",
       " (380, 380),\n",
       " (380, 380),\n",
       " (381, 381),\n",
       " (381, 381),\n",
       " (382, 382),\n",
       " (382, 382),\n",
       " (382, 6344),\n",
       " (6344, 382),\n",
       " (383, 383),\n",
       " (383, 383),\n",
       " (384, 384),\n",
       " (384, 384),\n",
       " (385, 385),\n",
       " (385, 385),\n",
       " (386, 386),\n",
       " (386, 386),\n",
       " (387, 387),\n",
       " (387, 387),\n",
       " (388, 388),\n",
       " (388, 388),\n",
       " (389, 389),\n",
       " (389, 389),\n",
       " (390, 390),\n",
       " (390, 390),\n",
       " (391, 391),\n",
       " (391, 391),\n",
       " (392, 392),\n",
       " (392, 392),\n",
       " (393, 393),\n",
       " (393, 393),\n",
       " (394, 394),\n",
       " (394, 394),\n",
       " (395, 395),\n",
       " (395, 395),\n",
       " (396, 396),\n",
       " (396, 396),\n",
       " (397, 397),\n",
       " (397, 397),\n",
       " (398, 398),\n",
       " (398, 398),\n",
       " (399, 399),\n",
       " (399, 399),\n",
       " (400, 400),\n",
       " (400, 400),\n",
       " (401, 401),\n",
       " (401, 401),\n",
       " (402, 402),\n",
       " (402, 402),\n",
       " (403, 403),\n",
       " (403, 403),\n",
       " (404, 404),\n",
       " (404, 404),\n",
       " (405, 405),\n",
       " (405, 405),\n",
       " (406, 406),\n",
       " (406, 406),\n",
       " (407, 407),\n",
       " (407, 407),\n",
       " (408, 408),\n",
       " (408, 408),\n",
       " (409, 409),\n",
       " (409, 409),\n",
       " (410, 410),\n",
       " (410, 410),\n",
       " (411, 411),\n",
       " (411, 411),\n",
       " (412, 412),\n",
       " (412, 412),\n",
       " (413, 413),\n",
       " (413, 413),\n",
       " (414, 414),\n",
       " (414, 414),\n",
       " (415, 415),\n",
       " (415, 415),\n",
       " (416, 416),\n",
       " (416, 416),\n",
       " (417, 417),\n",
       " (417, 417),\n",
       " (418, 418),\n",
       " (418, 418),\n",
       " (419, 419),\n",
       " (419, 419),\n",
       " (420, 420),\n",
       " (420, 420),\n",
       " (421, 421),\n",
       " (421, 421),\n",
       " (422, 422),\n",
       " (422, 422),\n",
       " (423, 423),\n",
       " (423, 423),\n",
       " (424, 424),\n",
       " (424, 424),\n",
       " (425, 425),\n",
       " (425, 425),\n",
       " (426, 426),\n",
       " (426, 426),\n",
       " (427, 427),\n",
       " (427, 427),\n",
       " (428, 428),\n",
       " (428, 428),\n",
       " (429, 429),\n",
       " (429, 429),\n",
       " (430, 430),\n",
       " (430, 430),\n",
       " (431, 431),\n",
       " (431, 431),\n",
       " (432, 432),\n",
       " (432, 432),\n",
       " (433, 433),\n",
       " (433, 433),\n",
       " (434, 434),\n",
       " (434, 434),\n",
       " (435, 435),\n",
       " (435, 435),\n",
       " (436, 436),\n",
       " (436, 436),\n",
       " (437, 437),\n",
       " (437, 437),\n",
       " (438, 438),\n",
       " (438, 438),\n",
       " (439, 439),\n",
       " (439, 439),\n",
       " (440, 440),\n",
       " (440, 440),\n",
       " (441, 441),\n",
       " (441, 441),\n",
       " (442, 442),\n",
       " (442, 442),\n",
       " (443, 443),\n",
       " (443, 443),\n",
       " (444, 444),\n",
       " (444, 444),\n",
       " (445, 445),\n",
       " (445, 445),\n",
       " (446, 446),\n",
       " (446, 446),\n",
       " (447, 447),\n",
       " (447, 447),\n",
       " (448, 448),\n",
       " (448, 448),\n",
       " (449, 449),\n",
       " (449, 449),\n",
       " (450, 450),\n",
       " (450, 450),\n",
       " (451, 451),\n",
       " (451, 451),\n",
       " (452, 452),\n",
       " (452, 452),\n",
       " (453, 453),\n",
       " (453, 453),\n",
       " (454, 454),\n",
       " (454, 454),\n",
       " (455, 455),\n",
       " (455, 455),\n",
       " (456, 456),\n",
       " (456, 456),\n",
       " (457, 457),\n",
       " (457, 457),\n",
       " (458, 458),\n",
       " (458, 458),\n",
       " (459, 459),\n",
       " (459, 459),\n",
       " (460, 460),\n",
       " (460, 460),\n",
       " (461, 461),\n",
       " (461, 461),\n",
       " (462, 462),\n",
       " (462, 462),\n",
       " (463, 463),\n",
       " (463, 463),\n",
       " (464, 464),\n",
       " (464, 464),\n",
       " (465, 465),\n",
       " (465, 465),\n",
       " (466, 466),\n",
       " (466, 466),\n",
       " (467, 467),\n",
       " (467, 467),\n",
       " (468, 468),\n",
       " (468, 468),\n",
       " (469, 469),\n",
       " (469, 469),\n",
       " (470, 470),\n",
       " (470, 470),\n",
       " (471, 471),\n",
       " (471, 471),\n",
       " (472, 472),\n",
       " (472, 472),\n",
       " (473, 473),\n",
       " (473, 473),\n",
       " (474, 474),\n",
       " (474, 474),\n",
       " (475, 475),\n",
       " (475, 475),\n",
       " (476, 476),\n",
       " (476, 476),\n",
       " (477, 477),\n",
       " (477, 477),\n",
       " (478, 478),\n",
       " (478, 478),\n",
       " (479, 479),\n",
       " (479, 479),\n",
       " (480, 480),\n",
       " (480, 480),\n",
       " (481, 481),\n",
       " (481, 481),\n",
       " (482, 482),\n",
       " (482, 482),\n",
       " (483, 483),\n",
       " (483, 483),\n",
       " (484, 484),\n",
       " (484, 484),\n",
       " (485, 485),\n",
       " (485, 485),\n",
       " (486, 486),\n",
       " (486, 486),\n",
       " (487, 487),\n",
       " (487, 487),\n",
       " (488, 488),\n",
       " (488, 488),\n",
       " (489, 489),\n",
       " (489, 489),\n",
       " (490, 490),\n",
       " (490, 490),\n",
       " (491, 491),\n",
       " (491, 491),\n",
       " (492, 492),\n",
       " (492, 492),\n",
       " (493, 493),\n",
       " (493, 493),\n",
       " (494, 494),\n",
       " (494, 494),\n",
       " (495, 495),\n",
       " (495, 495),\n",
       " (496, 496),\n",
       " (496, 496),\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_edges(features, threshold=0.5):\n",
    "    similarity_matrix = cosine_similarity(features)\n",
    "    edges = []\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i, len(features)):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                edges.append((i, j))\n",
    "                edges.append((j, i))\n",
    "    return edges\n",
    "\n",
    "edges = build_edges(feature_embeddings.to(\"cpu\"), threshold=0.8)\n",
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [2],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoded = label_encoder.fit_transform(label)\n",
    "label_tensor = torch.tensor(label_encoded, dtype=torch.long).unsqueeze(1)\n",
    "label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Midwest\n",
      "1: Northeast\n",
      "2: South\n",
      "3: West\n"
     ]
    }
   ],
   "source": [
    "for index, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{index}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0571, -0.0400,  0.0816,  ..., -0.0082,  0.0355, -0.0031],\n",
       "        [-0.0653, -0.0836, -0.0041,  ..., -0.0310,  0.0244, -0.0153],\n",
       "        [-0.0383, -0.0308, -0.0211,  ...,  0.0403, -0.0469,  0.0323],\n",
       "        ...,\n",
       "        [-0.0338, -0.0603, -0.0540,  ..., -0.0078,  0.0151, -0.0263],\n",
       "        [-0.0274, -0.0523,  0.0061,  ...,  0.0044,  0.0096, -0.0644],\n",
       "        [-0.0379, -0.0054, -0.0246,  ...,  0.0471, -0.0181,  0.0179]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = feature_embeddings\n",
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0],\n",
       "        [   0,    0],\n",
       "        [   1,    1],\n",
       "        ...,\n",
       "        [9453, 9453],\n",
       "        [9454, 9454],\n",
       "        [9454, 9454]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_label = label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = edge_index.t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[9455, 384], edge_index=[2, 18960], y=[9455, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data = Data(x=node_features, edge_index=edge_index, y=node_label)\n",
    "graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "splitter = RandomNodeSplit(split=\"train_rest\", num_val=0.2, num_test=0.2)\n",
    "graph_data = splitter(graph_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access masks for each set\n",
    "train_mask = graph_data.train_mask\n",
    "val_mask = graph_data.val_mask\n",
    "test_mask = graph_data.test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    1,  ..., 9453, 9454, 9454],\n",
       "        [   0,    0,    1,  ..., 9453, 9454, 9454]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5673]) torch.Size([1891]) torch.Size([1891])\n"
     ]
    }
   ],
   "source": [
    "train_nodes = train_mask.nonzero().flatten()\n",
    "val_nodes = val_mask.nonzero().flatten()\n",
    "test_nodes = test_mask.nonzero().flatten()\n",
    "print(train_nodes.shape, val_nodes.shape, test_nodes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_splitter = RandomLinkSplit(num_val=0.2, num_test=0.2)\n",
    "train_data, val_data, test_data = edge_splitter(graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0571, -0.0400,  0.0816,  ..., -0.0082,  0.0355, -0.0031],\n",
      "        [-0.0653, -0.0836, -0.0041,  ..., -0.0310,  0.0244, -0.0153],\n",
      "        [-0.0383, -0.0308, -0.0211,  ...,  0.0403, -0.0469,  0.0323],\n",
      "        ...,\n",
      "        [-0.0338, -0.0603, -0.0540,  ..., -0.0078,  0.0151, -0.0263],\n",
      "        [-0.0274, -0.0523,  0.0061,  ...,  0.0044,  0.0096, -0.0644],\n",
      "        [-0.0379, -0.0054, -0.0246,  ...,  0.0471, -0.0181,  0.0179]],\n",
      "       device='cuda:0')\n",
      "Data(x=[9455, 384], edge_index=[2, 11376], y=[9455, 1], train_mask=[9455], val_mask=[9455], test_mask=[9455], edge_label=[7584], edge_label_index=[2, 7584])\n",
      "Data(x=[9455, 384], edge_index=[2, 15168], y=[9455, 1], train_mask=[9455], val_mask=[9455], test_mask=[9455], edge_label=[7584], edge_label_index=[2, 7584])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.x)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = graph_data.num_node_features\n",
    "hidden_dim = 16\n",
    "output_dim = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Graph Neural Network model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "val_data = val_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "model = GCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Multi Class classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(train_data.x, train_data.edge_index)\n",
    "    loss = loss_fn(out[train_data.train_mask], train_data.y[train_data.train_mask].squeeze())\n",
    "    # loss = F.nll_loss(out[train_data.train_mask], train_data.y[train_data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        \n",
    "        # Convert logits to probabilities using sigmoid\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Convert probabilities to binary predictions (0 or 1)\n",
    "        preds = probs.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        # Mask the predictions and true labels to only include test data\n",
    "        preds = preds[data.test_mask.cpu().numpy()]\n",
    "        y_true = data.y[data.test_mask].cpu().numpy()\n",
    "        # Compute accuracy\n",
    "        acc = accuracy_score(y_true, preds)\n",
    "        precision = precision_score(y_true, preds, average=\"weighted\", zero_division=1)\n",
    "        recall = recall_score(y_true, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_true, preds, average=\"macro\")\n",
    "        return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.3871, Test Accuracy: 0.4120\n",
      "Epoch 20, Loss: 1.2616, Test Accuracy: 0.4384\n",
      "Early stopping at epoch 39. Best validation accuracy: 0.4432\n",
      "Final Test Accuracy: 0.4384\n",
      "Final Test Precision: 0.5649\n",
      "Final Test Recall: 0.4384\n",
      "Final Test F1: 0.2185\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_val_acc = 0\n",
    "patience = 10  # Stop if validation accuracy does not improve for 10 epochs\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train(model)\n",
    "    train_acc, _, _, _ = evaluate(model, train_data)\n",
    "    val_acc, _, _, _ = evaluate(model, val_data)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate(model, test_data)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate(model, test_data)\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GAT Model for Binary Classification\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
    "        super(GAT, self).__init__()\n",
    "        # First GAT layer (multi-head attention)\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
    "        # Second GAT layer (single-head for binary output)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)  # Use ELU activation function\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_data.num_node_features\n",
    "hidden_dim = 64\n",
    "output_dim = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GAT model\n",
    "gat_model = GAT(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(gat_model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()  # Multi Class classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4144, Test Accuracy: 0.4252\n",
      "Early stopping at epoch 20. Best validation accuracy: 0.4400\n",
      "Final Test Accuracy: 0.4363\n",
      "Final Test Precision: 0.3824\n",
      "Final Test Recall: 0.4363\n",
      "Final Test F1: 0.2146\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "best_val_acc = 0\n",
    "patience = 10  # Stop if validation accuracy does not improve for 10 epochs\n",
    "wait = 0\n",
    "best_model_state = None\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    loss = train(gat_model)\n",
    "    train_acc, _, _, _ = evaluate(gat_model, train_data)\n",
    "    val_acc, _, _, _ = evaluate(gat_model, val_data)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}. Best validation accuracy: {best_val_acc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        result = evaluate(gat_model, test_data)\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}, Test Accuracy: {result[0]:.4f}')\n",
    "\n",
    "# Final evaluation\n",
    "acc, precision, recall, f1 = evaluate(gat_model, test_data)\n",
    "print(f'Final Test Accuracy: {acc:.4f}')\n",
    "print(f'Final Test Precision: {precision:.4f}')\n",
    "print(f'Final Test Recall: {recall:.4f}')\n",
    "print(f'Final Test F1: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel, BertTokenizer, Trainer, TrainingArguments, DistilBertModel, DistilBertTokenizer, RobertaModel, RobertaTokenizer, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 5673\n",
      "Validation data size: 1891\n",
      "Test data size: 1891\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and temp datasets with a ratio of 6:4\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    feature, label_encoded, test_size=0.4, random_state=seed, stratify=label_encoded\n",
    ")\n",
    "\n",
    "# Split the temp dataset into validation and test datasets with a ratio of 2:2\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data, temp_labels, test_size=0.5, random_state=seed, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(f'Train data size: {len(train_data)}')\n",
    "print(f'Validation data size: {len(val_data)}')\n",
    "print(f'Test data size: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_function(texts, tokenizer):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length = 128,\n",
    "        return_tensors='pt'  \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer_function(train_data, tokenizer)\n",
    "val_encodings = tokenizer_function(val_data, tokenizer)\n",
    "test_encodings = tokenizer_function(test_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GeoTextDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = GeoTextDataset(val_encodings, val_labels.tolist())\n",
    "test_dataset = GeoTextDataset(test_encodings, test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, pre_trained_model):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = pre_trained_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, len(label_encoder.classes_))  \n",
    "        self.loss_fn = nn.CrossEntropyLoss()  # Multi Class classification loss\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use [CLS] token representation\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.long()\n",
    "            loss = self.loss_fn(logits, labels)\n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "pre_trained_bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model = CustomBertModel(pre_trained_bert_model).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, zero_division=1, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./geotext-bertResults\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    "    # warmup_ratio=0.06,  # Prevent aggressive weight updates early\n",
    "    # gradient_accumulation_steps=2,  # Simulate larger batch without increasing memory\n",
    "    # fp16=True,  # Use mixed precision training if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1775' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1775/3550 12:48 < 12:49, 2.31 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.235400</td>\n",
       "      <td>1.235615</td>\n",
       "      <td>0.430460</td>\n",
       "      <td>0.706942</td>\n",
       "      <td>0.267365</td>\n",
       "      <td>0.200930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.199100</td>\n",
       "      <td>1.227285</td>\n",
       "      <td>0.443152</td>\n",
       "      <td>0.467011</td>\n",
       "      <td>0.293244</td>\n",
       "      <td>0.249402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.123900</td>\n",
       "      <td>1.269137</td>\n",
       "      <td>0.420412</td>\n",
       "      <td>0.319195</td>\n",
       "      <td>0.312894</td>\n",
       "      <td>0.304615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.991200</td>\n",
       "      <td>1.308670</td>\n",
       "      <td>0.421470</td>\n",
       "      <td>0.314730</td>\n",
       "      <td>0.298766</td>\n",
       "      <td>0.285754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.836400</td>\n",
       "      <td>1.461734</td>\n",
       "      <td>0.424114</td>\n",
       "      <td>0.323451</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>0.302107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1775, training_loss=1.0805637284399758, metrics={'train_runtime': 768.7777, 'train_samples_per_second': 73.792, 'train_steps_per_second': 4.618, 'total_flos': 0.0, 'train_loss': 1.0805637284399758, 'epoch': 5.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.2189521789550781, 'eval_accuracy': 0.4537281861448969, 'eval_precision': 0.721756313861577, 'eval_recall': 0.30086623937096346, 'eval_f1': 0.2569393835760324, 'eval_runtime': 16.1602, 'eval_samples_per_second': 117.016, 'eval_steps_per_second': 7.364, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f'Test Results: {test_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model):\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)['logits']\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        predicted_class = probs.argmax(dim=1).item()\n",
    "        predicted_class = label_encoder.inverse_transform([predicted_class])[0]\n",
    "        return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gender Probabilities: Northeast\n"
     ]
    }
   ],
   "source": [
    "example_text = \"I love watching football and playing video games.\"\n",
    "prediction = predict(example_text, model)\n",
    "print(f\"Predicted Gender Probabilities: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings_distilbert = tokenizer_function(train_data, distilbert_tokenizer)\n",
    "val_encodings_distilbert = tokenizer_function(val_data, distilbert_tokenizer)\n",
    "test_encodings_distilbert = tokenizer_function(test_data, distilbert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_trained_distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = CustomBertModel(pre_trained_bert_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_training_args = TrainingArguments(\n",
    "    output_dir=\"./geotext-distilbertResults\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer = Trainer(\n",
    "    model=distilbert_model,\n",
    "    args=distilbert_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/3550 10:35 < 15:54, 2.23 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.124800</td>\n",
       "      <td>1.255912</td>\n",
       "      <td>0.453199</td>\n",
       "      <td>0.368976</td>\n",
       "      <td>0.315506</td>\n",
       "      <td>0.291192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.793900</td>\n",
       "      <td>1.647996</td>\n",
       "      <td>0.407721</td>\n",
       "      <td>0.339147</td>\n",
       "      <td>0.307153</td>\n",
       "      <td>0.292397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.680500</td>\n",
       "      <td>1.937828</td>\n",
       "      <td>0.399260</td>\n",
       "      <td>0.308587</td>\n",
       "      <td>0.297982</td>\n",
       "      <td>0.277604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400500</td>\n",
       "      <td>2.310466</td>\n",
       "      <td>0.395029</td>\n",
       "      <td>0.317908</td>\n",
       "      <td>0.311917</td>\n",
       "      <td>0.308353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1420, training_loss=0.7571277534458, metrics={'train_runtime': 635.9689, 'train_samples_per_second': 89.202, 'train_steps_per_second': 5.582, 'total_flos': 0.0, 'train_loss': 0.7571277534458, 'epoch': 4.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.2512311935424805, 'eval_accuracy': 0.45690111052353255, 'eval_precision': 0.4079763558844771, 'eval_recall': 0.3224358971655647, 'eval_f1': 0.30369412156452913, 'eval_runtime': 16.34, 'eval_samples_per_second': 115.728, 'eval_steps_per_second': 7.283, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "distilbert_test_results = distilbert_trainer.evaluate(test_dataset)\n",
    "print(f'Test Results: {distilbert_test_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings_roberta = tokenizer_function(train_data, roberta_tokenizer)\n",
    "val_encodings_roberta = tokenizer_function(val_data, roberta_tokenizer)\n",
    "test_encodings_roberta = tokenizer_function(test_data, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pre_trained_roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "roberta_model = CustomBertModel(pre_trained_roberta_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_training_args = TrainingArguments(\n",
    "    output_dir=\"./geotext-robertaResults\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=roberta_training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='3550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/3550 10:39 < 15:59, 2.22 it/s, Epoch 4/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.245000</td>\n",
       "      <td>1.262962</td>\n",
       "      <td>0.414595</td>\n",
       "      <td>0.853649</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.262000</td>\n",
       "      <td>1.263888</td>\n",
       "      <td>0.414595</td>\n",
       "      <td>0.853649</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.291400</td>\n",
       "      <td>1.261969</td>\n",
       "      <td>0.414595</td>\n",
       "      <td>0.853649</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.280100</td>\n",
       "      <td>1.261093</td>\n",
       "      <td>0.414595</td>\n",
       "      <td>0.853649</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.146542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1420, training_loss=1.271571818875595, metrics={'train_runtime': 639.6061, 'train_samples_per_second': 88.695, 'train_steps_per_second': 5.55, 'total_flos': 0.0, 'train_loss': 1.271571818875595, 'epoch': 4.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 1.2629201412200928, 'eval_accuracy': 0.41459545214172394, 'eval_precision': 0.853648863035431, 'eval_recall': 0.25, 'eval_f1': 0.14654205607476636, 'eval_runtime': 16.6177, 'eval_samples_per_second': 113.794, 'eval_steps_per_second': 7.161, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "roberta_test_results = roberta_trainer.evaluate(test_dataset)\n",
    "print(f'Test Results: {roberta_test_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gender Probabilities: West\n"
     ]
    }
   ],
   "source": [
    "example_text = \"I love watching football and playing video games.\"\n",
    "prediction = predict(example_text, model)\n",
    "print(f\"Predicted Gender Probabilities: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v3.11.0",
   "language": "python",
   "name": "v3.11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
